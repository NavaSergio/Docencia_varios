@article{Elsayed2021,
   abstract = {Time series forecasting is a crucial task in machine learning, as it has a wide range of applications including but not limited to forecasting electricity consumption, traffic, and air quality. Traditional forecasting models rely on rolling averages, vector auto-regression and auto-regressive integrated moving averages. On the other hand, deep learning and matrix factorization models have been recently proposed to tackle the same problem with more competitive performance. However, one major drawback of such models is that they tend to be overly complex in comparison to traditional techniques. In this paper, we report the results of prominent deep learning models with respect to a well-known machine learning baseline, a Gradient Boosting Regression Tree (GBRT) model. Similar to the deep neural network (DNN) models, we transform the time series forecasting task into a window-based regression problem. Furthermore, we feature-engineered the input and output structure of the GBRT model, such that, for each training window, the target values are concatenated with external features, and then flattened to form one input instance for a multi-output GBRT model. We conducted a comparative study on nine datasets for eight state-of-the-art deep-learning models that were presented at top-level conferences in the last years. The results demonstrate that the window-based input transformation boosts the performance of a simple GBRT model to levels that outperform all state-of-the-art DNN models evaluated in this paper.},
   author = {Shereen Elsayed and Daniela Thyssens and Ahmed Rashed and Hadi Samer Jomaa and Lars Schmidt-Thieme},
   keywords = {Boosting Regres-sion Trees,Deep Learning,Time Series Forecasting},
   month = {1},
   title = {Do We Really Need Deep Learning Models for Time Series Forecasting?},
   url = {https://arxiv.org/pdf/2101.02118},
   year = {2021}
}
@article{Shah2021,
   abstract = {Deep-learning techniques have been successfully used for time-series forecasting and have often shown superior performance on many standard benchmark datasets as compared to traditional techniques. Here we present a comprehensive and comparative study of performance of deep-learning techniques for forecasting prices in financial markets. We benchmark state-of-the-art deep-learning baselines, such as NBeats, etc., on data from currency as well as stock markets. We also generate synthetic data using a fuzzy-logic based model of demand driven by technical rules such as moving averages, which are often used by traders. We benchmark the baseline techniques on this synthetic data as well as use it for data augmentation. We also apply gradient-based meta-learning to account for non-stationarity of financial time-series. Our extensive experiments notwithstanding, the surprising result is that the standard ARIMA models outperforms deep-learning even using data augmentation or meta-learning. We conclude by speculating as to why this might be the case.},
   author = {Vedant Shah and Appcair Bits Pilani and K K Birla and Goa Campus and Gautam Shroff},
   month = {10},
   title = {Forecasting Market Prices using DL with Data Augmentation and Meta-learning: ARIMA still wins!},
   url = {https://arxiv.org/pdf/2110.10233},
   year = {2021}
}
@article{Li2013,
   abstract = {This paper studies the optimal multiple headways determination for a single bus route with a stochastic expected value model. The stochastic events from the model consist of random passenger arrivals, random boarding/alighting as well as random bus travel time. The expected value model employs two objective functions to maximize the bus company profit and to minimize the passengers waiting time cost respectively. The model is solved by a hybrid intelligent algorithm in which stochastic simulation and genetic algorithm are used to treat with the uncertain function. The algorithm efficiency is tested by a numerical experiment and concerned conclusions are drawn. ?? 2012 Elsevier Inc. All rights reserved.},
   author = {Yanhong Li and Wangtu Xu and Shiwei He},
   doi = {10.1016/j.amc.2012.11.098},
   isbn = {0096-3003},
   issn = {00963003},
   issue = {11},
   journal = {Applied Mathematics and Computation},
   keywords = {Bus headway,Expected value model,Genetic algorithm,Stochastic simulation,Uncertain programming},
   pages = {5849-5861},
   title = {Expected value model for optimizing the multiple bus headways},
   volume = {219},
   url = {http://dx.doi.org/10.1016/j.amc.2012.11.098},
   year = {2013}
}
@article{Verbas2013,
   abstract = {This study proposes a formulation for the transit network frequency setting problem. The formulation provides an optimal allocation of resources over space and time while recognizing the existence of multiple service patterns along each bus route. Transit agencies must allocate their limited resources optimally to maximize user benefits, operator benefits, or a combination of the two. The coupling of the routes with the service patterns provided along all or portions of the routes is effectively captured, and the user perspective and the operator perspective are merged into one formulation. The service patterns may be scheduled with different subsets of stops for a given route. Users see the resulting combined route headways at the stops. The number of riders varies with the prevailing number of bus trips at a given stop, which is the combination of different pattern dispatch frequencies. Two main formulations are introduced. The first extends work of Furth and Wilson and seeks to maximize the number of riders and the total wait time savings under budget, fleet, policy headway, and bus loading constraints. The second minimizes the net cost under fleet, policy headway, bus loading, minimum ridership, and minimum wait time savings constraints. In both formulations, pattern headways in different time-of-day intervals are the decision variables. This paper provides the mathematical formulation underlying the proposed methodology, describes the solution method and implementation, and demonstrates, by example, important properties of the frequency setting problem in this context, including some that may at first appear counterintuitive.},
   author = {I O Verbas and H S Mahmassani},
   doi = {10.3141/2334-06},
   isbn = {0361-1981},
   issn = {0361-1981},
   issue = {2334},
   journal = {Transportation Research Record},
   keywords = {DESIGN,Engineering, Civil,OPTIMIZATION,Transportation,Transportation Science & Technology},
   pages = {50-59},
   title = {Optimal Allocation of Service Frequencies over Transit Network Routes and Time Periods Formulation, Solution, and Implementation Using Bus Route Patterns},
   url = {<Go to ISI>://WOS:000329379700007},
   year = {2013}
}
@article{Martnez2014,
   abstract = {We study the transit frequency optimization problem, which aims to determine the time interval between subsequent buses for a set of public transportation lines given by their itineraries, i.e., sequences of stops and street sections. The solution should satisfy a given origin–destination demand and a constraint on the available fleet of buses. We propose a new mixed integer linear programming (MILP) formulation for an already existing model, originally formulated as a nonlinear bilevel one. The proposed formulation is able to solve to optimality real small-sized instances of the problem using MILP techniques. For solving larger instances we propose a metaheuristic which accuracy is estimated by comparing against exact results (when possible). Both exact and approximated approaches are tested by using existing cases, including a real one related to a small-city which public transportation system comprises 13 lines. The magnitude of the improvement of that system obtained by applying the proposed methodologies, is comparable with the improvements reported in the literature, related to other real systems. Also, we investigate the applicability of the metaheuristic to a larger-sized real case, comprising more than 130 lines.},
   author = {Héctor Martínez and Antonio Mauttone and María E. Urquhart},
   doi = {10.1016/j.ejor.2013.11.007},
   isbn = {0377-2217},
   issn = {03772217},
   issue = {1},
   journal = {European Journal of Operational Research},
   keywords = {Bus frequency optimization,Mixed integer linear programming,Tabu Search,Transportation},
   pages = {27-36},
   title = {Frequency optimization in public transportation systems: Formulation and metaheuristic approach},
   volume = {236},
   url = {http://www.sciencedirect.com/science/article/pii/S0377221713009065},
   year = {2014}
}
@article{ShariatMohaymany2009,
   author = {Afshin Shariat Mohaymany and Mahdi Amiripour},
   issue = {3},
   journal = {International Journal of Industrial Engineering \& Production Research},
   keywords = {timetable,urban bus network},
   pages = {83-91},
   title = {Creating bus timetables under stochastic demand},
   volume = {20},
   year = {2009}
}
@article{Verbas2015,
   author = {İ. Ömer Verbas and Hani S Mahmassani},
   doi = {10.3141/2498-05},
   issn = {0361-1981},
   journal = {Transportation Research Record: Journal of the Transportation Research Board},
   month = {6},
   pages = {37-45},
   title = {Integrated Frequency Allocation and User Assignment in Multimodal Transit Networks},
   volume = {2498},
   url = {http://trrjournalonline.trb.org/doi/10.3141/2498-05},
   year = {2015}
}
@article{Chen2005,
   abstract = {This study develops a dynamic bus arrival time prediction model using the data collected by the automatic vehicle location and automatic passenger counter systems. It is based on the Kalman filter algorithm with a two-dimensional state variable in which the prediction error in the most recent observation is used to optimize the arrival time estimate for each downstream stop. The impact of schedule recovery is considered as a control factor in the model to reflect the driver's schedule recovery behavior. The algorithm performs well when tested with a set of automatic vehicle location-automatic passenger counter data collected from a real-world bus route. The algorithm does not require intensive computation or an excessive data preprocessing effort. It is a promising approach for real-time bus arrival time prediction in practice.},
   author = {Mei Chen and Xiaobo Liu and Jingxin Xia},
   doi = {10.3141/1923-22},
   issn = {0361-1981},
   journal = {Transportation Research Record: Journal of the Transportation Research Board},
   month = {1},
   pages = {208-217},
   publisher = {Transportation Research Board of the National Academies},
   title = {Dynamic Prediction Method with Schedule Recovery Impact for Bus Arrival Time},
   volume = {1923},
   url = {http://trrjournalonline.trb.org/doi/abs/10.3141/1923-22},
   year = {2005}
}
@article{Wang2014,
   abstract = {This paper proposes an approach combining historical data and real-time situation information to forecast the bus arrival time. The approach includes two phases. Firstly, Radial Basis Function Neural Networks (RBFNN) model is used to learn and approximate the nonlinear relationship in historical data in the first phase. Then, in the second phase, an online oriented method is introduced to adjust to the actual situation, which means to use the practical information to modify the predicted result of RBFNN in the first phase. Afterwards, the system designing outline is given to summarize the structure and components of the system. We did an experimental study on bus route No.21 in Dalian by deploying this system to demonstrate the validity and effectiveness of this approach. In addition, Multiple Linear Regression model, BP Neural Networks and RBFNN without online adjustment are used in contrast. Results show that the approach with RBFNN and online adjustment has a better predicting performance.},
   author = {Lei Wang and Zhongyi Zuo and Junhao Fu},
   doi = {10.1016/j.sbspro.2014.07.182},
   issn = {18770428},
   issue = {0},
   journal = {Procedia - Social and Behavioral Sciences},
   keywords = {Arrival Time Prediction,Intelligent Transportation,Public Transport,RBFNN,System Design},
   pages = {67-75},
   title = {Bus Arrival Time Prediction Using RBF Neural Networks Adjusted by Online Data},
   volume = {138},
   url = {http://www.sciencedirect.com/science/article/pii/S1877042814041019},
   year = {2014}
}
@article{Watkins2011,
   author = {Kari Edison Watkins and Brian Ferris and Alan Borning and G. Scott Rutherford and David Layton},
   doi = {10.1016/j.tra.2011.06.010},
   issn = {09658564},
   issue = {8},
   journal = {Transportation Research Part A: Policy and Practice},
   month = {10},
   pages = {839-848},
   title = {Where Is My Bus? Impact of mobile real-time information on the perceived and actual wait time of transit riders},
   volume = {45},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0965856411001030},
   year = {2011}
}
@article{Cathey2003,
   author = {F.W. Cathey and D.J. Dailey},
   doi = {10.1016/S0968-090X(03)00023-8},
   issn = {0968090X},
   issue = {3-4},
   journal = {Transportation Research Part C: Emerging Technologies},
   keywords = {avl,bus,kalman filter,prediction,tcip,transit},
   pages = {241-264},
   title = {A prescription for transit arrival/departure prediction using automatic vehicle location data},
   volume = {11},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0968090X03000238},
   year = {2003}
}
@article{Wagale2013,
   abstract = {Timetabling and vehicle scheduling is the basis of security and efficiency for various bus enterprises. It is necessary to take into account the passenger travel demand to meet both the social and economic benefits for these bus enterprises. A Demand- and Travel time Responsive (DTR) model has been applied to actualize a timetable for each bus stop on the basis of optimal bus frequency. This paper presents a model to optimize the bus scheduling by taking into consideration of both bus stop and route segments of the city in an integrated manner. In this study different real time data event parameters, such as bus stop departures and arrivals for buses operating on a line-based time-table and bus traffic costs have been applied to optimize the bus scheduling process. A bus headway time-table is also being developed. The proposed model has been verified by taking a case study of Jaipur city. The relevant data have been collected from Jaipur City Transport Services Limited (JCTSL). The sensitivity analysis for various parameters and assumption used in proposed model has been applied to assess the reliability of the optimal solution.The DTR model for an optimal bus scheduling developed herein is based on a holistic, integrated, systems-oriented approach, which clearly demonstrates the overall usage of the model. The concept of DTR model is to determine the optimal cost at which total cumulative cost and total cumulative profit become equal on the basis of optimal service frequency. The optimal total traffic cost line demarcates the boundary representing the stops with excessive investment and stops with profits. It will essentially enable decision makers and policy makers to evaluate appropriate strategies for best practices to be adopted for serving passengers so that efficient and optimum transportation facilities can be implemented.},
   author = {Makrand Wagale and Ajit Pratap Singh and Ashoke K. Sarkar and Srinivas Arkatkar},
   doi = {10.1016/j.sbspro.2013.11.179},
   isbn = {1877-0428},
   issn = {18770428},
   journal = {Procedia - Social and Behavioral Sciences},
   keywords = {1877-0428,2013 the authors,91-1596-244183,91-1596-515235,apsbits,bus scheduling,com,corresponding author,dtr model,e-mail address,fax,gmail,headway,published by elsevier ltd,tel,time-table,wagalemakrand91},
   pages = {845-854},
   title = {Real-time Optimal Bus Scheduling for a City Using A DTR Model},
   volume = {104},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S1877042813045709},
   year = {2013}
}
@article{Zhang2013,
   author = {Cen Zhang and Jing Teng},
   doi = {10.1016/j.sbspro.2013.08.151},
   isbn = {0860216958300},
   issn = {18770428},
   issue = {Cictp},
   journal = {Procedia - Social and Behavioral Sciences},
   pages = {1329-1340},
   title = {Bus Dwell Time Estimation and Prediction: A Study Case in Shanghai-China},
   volume = {96},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S1877042813022775},
   year = {2013}
}
@article{Zhou2015,
   author = {Chunjie Zhou and Pengfei Dai and Fusheng Wang and Zhenxing Zhang},
   doi = {10.1016/j.pmcj.2015.10.003},
   isbn = {8653566732},
   issn = {15741192},
   journal = {Pervasive and Mobile Computing},
   month = {10},
   title = {Predicting the passenger demand on bus services for mobile users},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S157411921500187X},
   year = {2015}
}
@article{Xinghao2013,
   author = {Song Xinghao and Teng Jing and Chen Guojun and Shu Qichong},
   doi = {10.1016/j.sbspro.2013.08.258},
   issn = {18770428},
   issue = {Cictp},
   journal = {Procedia - Social and Behavioral Sciences},
   pages = {2287-2299},
   title = {Predicting Bus Real-time Travel Time Basing on both GPS and RFID Data},
   volume = {96},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S1877042813023847},
   year = {2013}
}
@article{Chen2015,
   author = {Jingxu Chen and Zhiyuan Liu and Senlai Zhu and Wei Wang},
   doi = {10.1016/j.tre.2015.08.007},
   issn = {13665545},
   journal = {Transportation Research Part E: Logistics and Transportation Review},
   pages = {1-15},
   title = {Design of limited-stop bus service with capacity constraint and stochastic travel time},
   volume = {83},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S1366554515001635},
   year = {2015}
}
@article{Mouwen2015,
   abstract = {This paper aims to improve the understanding of the drivers of customer satisfaction with public transport (PT). The methodology provides a relevant contribution to the previous studies since it highlights the complex interaction between the level and composition of satisfaction, negative social safety experiences (NSSEs), urban settings, and the PT mode used. Overall, PT users see the service attributes on-time performance, travel speed, and service frequency as the most important, followed by personnel/driver behaviour and vehicle tidiness. A generic policy aimed at achieving these attributes may yield favourable results with respect to satisfaction. Further, we demonstrate the influence of differences in customer characteristics on satisfaction. A policy aimed at increasing the service frequency and putting new vehicles into operation will probably lead specifically to more satisfied older people (>65), passengers travelling by regional train, and people living in dense urban areas. These findings may be of help to PTAs intending to exert an influence on the actions of PT operators, for instance by using them as a measuring rod in incentive contracts.},
   author = {Arnoud Mouwen},
   doi = {10.1016/j.tra.2015.05.005},
   issn = {09658564},
   journal = {Transportation Research Part A: Policy and Practice},
   keywords = {Public transport,Satisfaction,User segments,Weights of service attributes},
   pages = {1-20},
   title = {Drivers of customer satisfaction with public transport services},
   volume = {78},
   url = {http://www.sciencedirect.com/science/article/pii/S0965856415001251},
   year = {2015}
}
@article{He2015,
   abstract = {Bus bunching could seriously damage the stability of transit system. This resultant instability always causes a dissatisfying performance of transit system. The strategies of resisting bus bunching aim at improving schedule and headway reliability so as to improve the level of service of transit system. From a perspective of effectively using accurate and reliable information, a generalized framework is proposed to deal with bus bunching and explain the potential efficiencies of various existing anti-bunching strategies. In particular, a strategy that adaptively determines the actual holding time and/or adjusts the bus cruising speed when a bus arrives at a bus stop is studied in depth. The information required by this new strategy includes only the arrival time of the current bus at the current bus stop and the arrival times of the preceding bus at this bus stop and the next. The nonlinearity of boarding process is also taken into account in this new strategy. Numerical analysis and simulation experiment show that the new strategy can not only alleviate bus bunching and keep high schedule and headway reliability with less slack added in schedule, but also generate a relatively high commercial speed for the cruising buses.},
   author = {Sheng-Xue He},
   doi = {10.1016/j.cie.2015.03.004},
   issn = {03608352},
   journal = {Computers \& Industrial Engineering},
   keywords = {Adaptive control,Bus bunching,Schedule reliability,Transit operations},
   month = {7},
   pages = {17-32},
   title = {An anti-bunching strategy to improve bus schedule and headway reliability by making use of the available accurate information},
   volume = {85},
   url = {http://www.sciencedirect.com/science/article/pii/S0360835215001138 http://linkinghub.elsevier.com/retrieve/pii/S0360835215001138},
   year = {2015}
}
@article{Berrebi2015,
   abstract = {One of the greatest problems facing transit agencies that operate high-frequency routes is maintaining stable headways and avoiding bus bunching. In this work, a real-time holding mechanism is proposed to dispatch buses on a loop-shaped route using real-time information. Holds are applied at one or several control points to minimize passenger waiting time while maintaining the highest possible frequency, i.e. using no buffer time. The bus dispatching problem is formulated as a stochastic decision process. The optimality equations are derived and the optimal holding policy is found by backward induction. A control method that requires much less information and that closely approximates the optimal dispatching policy is found. A simulation assuming stochastic operating conditions and unstable headway dynamics is performed to assess the expected average waiting time of passengers at stations. The proposed control strategy is found to provide lower passenger waiting time and better resiliency than methods used in practice and recommended in the literature.},
   author = {Simon J. Berrebi and Kari E. Watkins and Jorge a. Laval},
   doi = {10.1016/j.trb.2015.05.012},
   issn = {01912615},
   journal = {Transportation Research Part B: Methodological},
   keywords = {Bus dispatching,Public transportation,Real-time information},
   month = {11},
   pages = {377-389},
   title = {A real-time bus dispatching policy to minimize passenger wait on a high frequency route},
   volume = {81},
   url = {http://www.sciencedirect.com/science/article/pii/S0191261515001149 http://linkinghub.elsevier.com/retrieve/pii/S0191261515001149},
   year = {2015}
}
@article{Corts2010,
   abstract = {A hybrid predictive control formulation based on evolutionary multi-objective optimization to optimize real-time operations of public transport systems is presented. The state space model includes bus position, expected load and arrival time at stops. The system is based on discrete events, and the possible operator control actions are: holding vehicles at stations and skipping some stations. The controller (operator) pursues the minimization of a dynamic objective function to generate better operational decisions under uncertain demand at bus stops. In this work, a multi-objective approach is conducted to include different goals in the optimization process that could be opposite. In this case, the optimization was defined in terms of two objectives: waiting time minimization on one side, and impact of the strategies on the other. A genetic algorithm method is proposed to solve the multi-objective dynamic problem. From the conducted experiments considering a single bus line corridor, we found that the two objectives are opposite but with a certain degree of overlapping, in the sense that in all cases both objectives significantly improve the level of service with respect to the open-loop scenario by regularizing the headways. On average, the observed trade-off validates the proposed multi-objective methodology for the studied system, allowing dynamically finding the pseudo-optimal Pareto front and making real-time decisions based on different optimization criteria reflected in the proposed objective function compounds. © 2009 Elsevier Ltd.},
   author = {Cristián E. Cortés and Doris Sáez and Freddy Milla and Alfredo Núñez and Marcela Riquelme},
   doi = {10.1016/j.trc.2009.05.016},
   isbn = {0968-090X},
   issn = {0968090X},
   issue = {5},
   journal = {Transportation Research Part C: Emerging Technologies},
   keywords = {Genetic algorithms,Hybrid predictive control,Multi-objective optimization,Public transport system},
   month = {10},
   pages = {757-769},
   title = {Hybrid predictive control for real-time optimization of public transport systems’ operations based on evolutionary multi-objective optimization},
   volume = {18},
   url = {http://dx.doi.org/10.1016/j.trc.2009.05.016 http://linkinghub.elsevier.com/retrieve/pii/S0968090X09000825},
   year = {2010}
}
@article{Ibarra-Rojas2015,
   abstract = {The efficiency of a transport system depends on several elements, such as available technology, governmental policies, the planning process, and control strategies. Indeed, the interaction between these elements is quite complex, leading to intractable decision making problems. The planning process and real-time control strategies have been widely studied in recent years, and there are several practical implementations with promising results. In this paper, we review the literature on Transit Network Planning problems and real-time control strategies suitable to bus transport systems. Our goal is to present a comprehensive review, emphasizing recent studies as well as works not addressed in previous reviews.},
   author = {O.J. Ibarra-Rojas and F. Delgado and R. Giesen and J.C. Muñoz},
   doi = {10.1016/j.trb.2015.03.002},
   issn = {01912615},
   journal = {Transportation Research Part B: Methodological},
   month = {7},
   pages = {38-75},
   title = {Planning, operation, and control of bus transport systems: A literature review},
   volume = {77},
   url = {http://www.sciencedirect.com/science/article/pii/S0191261515000454\nhttp://www.sciencedirect.com.ezproxy.ulb.ac.be/science/article/pii/S0191261515000454\nhttp://www.sciencedirect.com.ezproxy.ulb.ac.be/science/article/pii/S0191261515000454/pdfft?md5=77a2af},
   year = {2015}
}
@article{Hernndez2015,
   abstract = {a b s t r a c t Control strategies have been widely used in the literature to counteract the effects of bus bunching in passenger's waiting times and its variability. These strategies have only been studied for the case of a single bus line in a corridor. However, in many real cases this assumption does not hold. Indeed, there are many transit corridors with multiple bus lines interacting, and this interaction affects the efficiency of the implemented control mechanism. This work develops an optimization model capable of executing a control scheme based on holding strategy for a corridor with multiple bus lines. We analyzed the benefits in the level of service of the public transport system when con-sidering a central operator who wants to maximize the level of service for users of all the bus lines, versus scenarios where each bus line operates independently. A simulation was carried out considering two medium frequency bus lines that serve a set of stops and where these two bus lines coexist in a given subset of stops. In the simulation we compared the existence of a central operator, using the optimization model we developed, against the independent operation of each line. In the simulations the central operator showed a greater reduction in the overall waiting time of the passengers of 55% compared to a no control scenario. It also provided a bal-anced load of the buses along the corridor, and a lower variability of the bus headways in the subset of stops where the lines coexist, thus obtaining better reliability for all types of passengers present in the public transport system.},
   author = {Daniel Hernández and Juan Carlos Muñoz and Ricardo Giesen and Felipe Delgado},
   doi = {10.1016/j.trb.2015.04.011},
   issn = {01912615},
   journal = {Transportation Research Part B: Methodological},
   month = {8},
   pages = {83-105},
   title = {Analysis of real-time control strategies in a corridor with multiple bus services},
   volume = {78},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0191261515000934},
   year = {2015}
}
@article{Rojo2015,
   author = {Marta Rojo and Luigi Dell’Olio and Hernán Gonzalo-Orden and Ángel Ibeas},
   doi = {10.1016/j.tranpol.2015.05.002},
   issn = {0967070X},
   journal = {Transport Policy},
   keywords = {Metropolitan bus service,Public transport economics,Transport planning,Travel demand modelling,public transport economics},
   month = {8},
   pages = {52-63},
   title = {Inclusion of quality criteria in public bus service contracts in metropolitan areas},
   volume = {42},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0967070X1530010X},
   year = {2015}
}
@article{Cancela2015,
   abstract = {In this work, we study the transit network design problem from the perspective of mathematical programming. More precisely, we consider the problem of defining the number and itinerary of bus routes and their frequencies, for a public transportation system. In this problem, the routes should be defined in terms of a given infrastructure of streets and stops and should cover a given origin–destination demand. The solution (routes and frequencies) should be convenient for the users and the operators. We review existing mathematical programming formulations and propose a new one, paying attention to the following aspects of public transportation systems, that are identified as key elements in order to have a realistic model: (a) the interest of the users, (b) the interest of the operators, (c) the behavior of the users, and (d) constraints regarding transfer, infrastructure and bus capacity. First, we discuss the formulations existing on the literature, in terms of the aspects mentioned above. Second, we propose a mixed integer linear programming (MILP) formulation, that incorporates the waiting time and the existence of multiple lines in the behavior of the users. We validate the proposed formulation using several cases, including a real one. Also, we compare the obtained results against results from the existing literature. In order to include transfer, infrastructure and bus capacity constraints, we propose an extension to the formulation and we discuss its impact in the structure of the model, based on concepts of bi-level mathematical programming. The mathematical formulations developed contribute towards a more realistic modeling effort, taking into account important aspects of the real system which were not included in previous proposals in the literature.},
   author = {Héctor Cancela and Antonio Mauttone and María E. Urquhart},
   doi = {10.1016/j.trb.2015.03.006},
   issn = {01912615},
   journal = {Transportation Research Part B: Methodological},
   keywords = {Bilevel programming,Mathematical programming,Optimal strategies,Public transportation,Transit network design},
   month = {7},
   pages = {17-37},
   title = {Mathematical programming formulations for transit network design},
   volume = {77},
   url = {http://www.sciencedirect.com/science/article/pii/S0191261515000491 http://linkinghub.elsevier.com/retrieve/pii/S0191261515000491},
   year = {2015}
}
@article{Guihaire2008,
   author = {Valérie Guihaire and Jin-Kao Hao},
   doi = {10.1016/j.tra.2008.03.011},
   issn = {09658564},
   issue = {10},
   journal = {Transportation Research Part A: Policy and Practice},
   keywords = {public transportation},
   month = {12},
   pages = {1251-1273},
   title = {Transit network design and scheduling: A global review},
   volume = {42},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0965856408000888},
   year = {2008}
}
@article{Barker2003,
   author = {J Barry Barker and Danny Alvarez and Ronald L Barnes and Constance Garber and Fred M Gilliam and Sharon Greene and Robert H Irwin and David A Lee and Dmjm Harris and Jeffrey M Rosenberg and TRB},
   isbn = {0-309-08776-7},
   journal = {TCRB Report 100},
   title = {Transit Capacity and Quality of Service Manual},
   volume = {2nd},
   url = {www.national-academies.org/trb/bookstore\nPrinted},
   year = {2003}
}
@book{Grimmett2001,
   abstract = {This book gives an introduction to probability and its many practical application by providing a thorough, entertaining account of basic probability and important random processes, covering a range of important topics. Emphasis is on modelling rather than abstraction and there are new sections on sampling and Markov chain Monte Carlo, renewal-reward, queueing networks, stochastic calculus, and option pricing in the Black-Scholes model for financial markets. In addition, there are almost 400 exercises and problems relevant to the material. Solutions can be found in One Thousand Exercises in Probability.},
   author = {Geoffrey R Grimmett and David R Stirzaker},
   doi = {10.2307/2288525},
   isbn = {0198572220},
   issn = {01621459},
   journal = {Journal of the American Statistical Association},
   pages = {608},
   title = {Probability and Random Processes},
   volume = {80},
   url = {http://www.jstor.org/stable/2288525?origin=crossref},
   year = {2001}
}
@book{Devore2012,
   abstract = {This comprehensive introduction to probability and statistics will give you the solid grounding you need no matter what your engineering specialty. Through the use of lively and realistic examples, the author helps you go beyond simply learning about statistics-you'll also learn how to put the statistical methods to use. In addition, rather than focusing on rigorous mathematical development and potentially overwhelming derivations, PROBABILITY AND STATISTICS FOR ENGINEERING AND THE SCIENCES emphasizes concepts, models, methodology, and applications that facilitate your understanding.},
   author = {J. L. Devore},
   isbn = {9780495382232},
   journal = {book},
   pages = {A12-A13},
   title = {Probability and Statistics for Engineering and the Sciences},
   url = {http://books.google.com/books?hl=en&amp;lr=&amp;id=3qoP7dlO4BUC&amp;oi=fnd&amp;pg=PR7&amp;dq=probability+and+statistics+for+engineering+and+the+sciences&amp;ots=m6FhahP0Jm&amp;sig=yPjmSEleDZVRvgZci0-1TiFrRMQ},
   year = {2012}
}
@book{Hogg1995,
   abstract = {This classic book retains its outstanding ongoing features and continues to provide readers with excellent background material necessary for a successful understanding of mathematical statistics. Chapter topics cover classical statistical inference procedures in estimation and testing, and an in-depth treatment of sufficiency and testing theoryincluding uniformly most powerful tests and likelihood ratios. Many illustrative examples and exercises enhance the presentation of material throughout the book. For a more complete understanding of mathematical statistics.},
   author = {R V Hogg and A T Craig},
   doi = {10.2307/1267313},
   isbn = {0130085073},
   issn = {00401706},
   issue = {3},
   journal = {Technometrics},
   pages = {807},
   title = {Introduction to Mathematical Statistics},
   volume = {14},
   url = {http://www.jstor.org/stable/1267313?origin=crossref},
   year = {1995}
}
@article{Mickelson2014,
   abstract = {Although much is known about broad societal attitudes toward poverty, less is known about how women perceive their own poverty. We sought to examine the types of self attributions low-income women make about their poverty, as well as the association of self poverty attributions to women’s mental health and upward mobility beliefs. Using close-ended questions in a community sample of 66 low-income mothers from the Midwestern United States, we found these women were most likely to attribute their poverty to issues related to having children, their romantic relationships, and structural/government blame. The least endorsed attributions for poverty were fatalistic and individualistic reasons. Attributing one’s poverty to children and structural reasons was related to greater depression, and attributing one’s poverty to romantic relationships and structural reasons was related to greater anxiety. Moreover, attributing one’s poverty to children and romantic relationships was positively related to upward mobility beliefs, whereas individualistic attributions were negatively related to upward mobility beliefs. Understanding how women view their poverty and upward mobility can help to improve interventions and policies aimed at low-income women.},
   author = {Kristin D. Mickelson and Emily Hazlett},
   doi = {10.1007/s11199-014-0414-4},
   isbn = {0360-0025},
   issn = {15732762},
   issue = {9-10},
   journal = {Sex Roles},
   keywords = {Attributions,Mental Health,Poverty,Social Class Perceptions},
   month = {11},
   pages = {319-332},
   publisher = {Springer US},
   title = {“Why me?”: Low-Income Women’s Poverty Attributions, Mental Health, and Social Class Perceptions},
   volume = {71},
   url = {http://link.springer.com/10.1007/s11199-014-0414-4},
   year = {2014}
}
@article{Adhikari2016,
   abstract = {BACKGROUND Women's role has been a priority area not only for sustainable development, but also in reproductive health since ICPD 1994. However, very little empirical evidence is available about women's role on maternal health service utilization in Nepal. This paper explores dimensions of women's autonomy and their relationship to utilization of maternal health services. METHODS The analysis uses data from the Nepal Demographic and Health Survey, 2011. The analysis is confined to women who had given birth in the 5 years preceding the survey (n = 4,148). Women's autonomy related variables are taken from the standard DHS questionnaire and measured based on decision in household about obtaining health care, large household purchases and visit to family or relative. The net effect of women's autonomy on utilization of maternal health services after controlling for the effect of other predictors has been measured through multivariate logistic regression analysis. RESULTS The findings indicate only about a half of the women who had given birth in the past 5 years preceding the survey had 4 or more ANC check up for their last birth. Similarly, 40 % of the women had delivered their last child in the health facilities. Furthermore, slightly higher than two-fifth women (43 %) had postnatal check up for their last child. Only slightly higher than a fourth woman (27 %) had utilized all the services (adequate ANC visit, delivered at health institution and post natal check up) for their last child. This study found that many socio-demographic variables such as age of women, number of children born, level of education, ethnicity, place of residence and wealth index are predicators of utilizing the maternal health services of recent child. Notably, higher level autonomy was associated with higher use of maternal health services [adjusted odds ratio (aOR) =1.40; CI 1.18-1.65]. CONCLUSIONS Utilization of maternal health services for the recent child among women is very low. The study results suggest that policy actions that increase women's autonomy at home could be effective in helping assure good maternal health.},
   author = {Ramesh Adhikari},
   doi = {10.1186/s12905-016-0305-7},
   isbn = {1297801601},
   issn = {1472-6874},
   issue = {1},
   journal = {BMC women's health},
   keywords = {Cross sectional,Maternal health,Nepal,Women’s autonomy},
   month = {3},
   pages = {26},
   pmid = {27177683},
   publisher = {BioMed Central},
   title = {Effect of Women's autonomy on maternal health service utilization in Nepal: a cross sectional study.},
   volume = {16},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0167629603000869 http://www.ncbi.nlm.nih.gov/pubmed/27177683%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4867085},
   year = {2016}
}
@article{Kiers1991,
   author = {Henk A. L. Kiers},
   doi = {10.1007/BF02294485},
   issn = {0033-3123},
   issue = {3},
   journal = {Psychometrika},
   month = {9},
   pages = {449-470},
   publisher = {Springer-Verlag},
   title = {Hierarchical relations among three-way methods},
   volume = {56},
   url = {http://link.springer.com/10.1007/BF02294485},
   year = {1991}
}
@article{Victor2013,
   author = {Bravo Rodriguez Victor and Alma Perez Y Arechiga and Mary Carmen and H E R Miguel},
   isbn = {0060290013},
   pages = {3-4},
   title = {- A otros bancos - Cuenta CLABE},
   year = {2013}
}
@article{Filho2015,
   abstract = {Industrial batch processes are commonly used in the production of a variety of items. Data emerging from such processes present a peculiar structure, and a number of customized multivariate control charts (CCs) have been proposed for their monitoring. In this paper, we propose CCs based on STATIS method, an exploratory technique for measuring similarities between data matrices. Data are arranged in such a way that the monitoring along time is prioritized. The methodology easily allows a nonparametric online monitoring of complex batch processes in time, in situations where a large number of variables are present. Besides its presentation, the new approach is illustrated using simulated data.},
   author = {Danilo Marcondes Filho and Luiz Paulo Luna Oliveira and Luiz Paulo Luna de Oliveira},
   doi = {10.1007/s00170-015-7428-0},
   issn = {1433-3015},
   issue = {5},
   journal = {The International Journal of Advanced Manufacturing Technology},
   keywords = {batch processes,control charts,fault detection,statis method,statistical process control},
   pages = {867-875},
   title = {Multivariate quality control of batch processes using STATIS},
   volume = {82},
   url = {http://link.springer.com/10.1007/s00170-015-7428-0%5Cnhttp://dx.doi.org/10.1007/s00170-015-7428-0},
   year = {2015}
}
@article{Liu2009,
   abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
   author = {Baoding Liu},
   doi = {10.1007/978-3-540-89484-1_7},
   isbn = {9783540894834},
   issn = {14349922},
   journal = {Studies in Fuzziness and Soft Computing},
   pages = {111-128},
   pmid = {25246403},
   title = {Uncertain programming},
   volume = {239},
   year = {2009}
}
@article{Lavit1994,
   abstract = {ACT (STATIS method) is a data analysis technique which computes Euclidean distances between configurations of the same observations obtained in K different circumstances, and thus handles three-way data as a set of K matrices. In this article, the recent developments of the ACT technique are fully described - concepts and theorems related to Euclidean scaling being discussed in the Appendix - and the software manipulation is illustrated on real data. ?? 1994.},
   author = {Christine Lavit and Yves Escoufier and Robert Sabatier and Pierre Traissac},
   doi = {10.1016/0167-9473(94)90134-1},
   issn = {01679473},
   issue = {1},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Euclidean vector spaces,RV coefficient,Spectral decomposition},
   pages = {97-119},
   pmid = {1937},
   title = {The ACT (STATIS method)},
   volume = {18},
   year = {1994}
}
@article{Fernndez2004,
   author = {Vicenç Fernández},
   journal = {Universitat Politécnica de Catalunya},
   pages = {287},
   title = {Relación entre la Capacidad de Absorción y las Estructuras Organizativas},
   year = {2004}
}
@article{Banko2011,
   abstract = {Principal Component Analysis (PCA) based, time-series analysis methods have become basic tools of every process engineer in the past few years thanks to their efficiency and solid statistical basis. However, there are two drawbacks of these methods which have to be taken into account. First, linear relationships are assumed between the process variables, and second, process dynamics are not considered. The authors presented a PCA based multivariate time-series segmentation method which addressed the first problem. The nonlinear processes were split into locally linear segments by using T 2 and Q statistics as cost functions. Based on this solution, we demonstrate how the homogeneous operation ranges and changes in process dynamics can also be detected in dynamic processes. Our approach is examined in detail on simple, theoretical processes and on the well-known pH process.},
   author = {Zoltan Banko and Laszlo Dobos and Janos Abonyi},
   issue = {1},
   journal = {Conservation, Information, Evolution},
   keywords = {dynamic principal component analysis,multivariate dynamic,time series segmentation},
   pages = {11-24},
   title = {Dynamic Principal Component Analysis in Multivariate Time-Series Segmentation},
   volume = {1},
   year = {2011}
}
@book{VilaEspeso2010,
   author = {Miguel Angel Vila Espeso and Roberto Escuder Valles and Rosalía Romero Rodriguez},
   isbn = {9788479784195},
   pages = {109},
   title = {Auditorías internas de la calidad},
   year = {2010}
}
@article{Sociales2002,
   abstract = {Guía para la aplicacion de análisis multivariado a encuestas.},
   author = {Indicadores Sociales},
   pages = {66},
   title = {Aplicacion de análisis multivariado a encuestas},
   year = {2002}
}
@article{SchmidWLazos2004,
   author = {R. Schmid, W, .Lazos},
   journal = {(CENAM), Centro Nacional De Metrología},
   pages = {1-27},
   title = {Guía para estimar la incertidumbre de la medición.},
   year = {2004}
}
@book{Nmero2011,
   abstract = {Obra: El hombre controlador del universo, o El hombre en la máquina del tiempo. México, 1934. Autor: Diego Rivera. México.},
   author = {Volumen Número},
   isbn = {9786079616205},
   issue = {3},
   keywords = {en trámite,n},
   pages = {1-149},
   title = {Issn : En Trámite},
   volume = {II},
   year = {2011}
}
@article{Mecnica2001,
   author = {Área D E Metrología Mecánica and E L Sistema and D E Unidades Si},
   title = {Internacional},
   year = {2001}
}
@article{Calibrators2000,
   author = {Block Calibrators},
   isbn = {9783942992145},
   issue = {July},
   pages = {1-14},
   title = {Guidelines on the Calibration of},
   year = {2000}
}
@article{Rske2012,
   author = {D Röske},
   isbn = {9781627481908},
   keywords = {2003,calibration,hand torque tools,iso 6789,torque wrenches,uncertainty},
   title = {Iso 6789 : 2003 Calibration Results of Hand Torque Tools With Measurement Uncertainty – Some Proposals},
   year = {2012}
}
@article{SchmidWLazos2004,
   author = {R. Schmid, W, .Lazos},
   journal = {(CENAM), Centro Nacional De Metrología},
   pages = {1-27},
   title = {Guía para estimar la incertidumbre de la medición.},
   year = {2004}
}
@article{Mosquera2007,
   author = {Cristian Mosquera},
   keywords = {Repetibilidad,incertidumbre,reproducibilidad},
   title = {Comparación entre los métodos de evaluación de incertidumbre y estudios de repetibilidad y reproducibilidad para la evaluación de las mediciones},
   year = {2007}
}
@article{VilarBarrio2005,
   author = {José Francisco Vilar Barrio},
   isbn = {84-96169-59-6},
   title = {Control estadístico de los procesos (SPC},
   year = {2005}
}
@article{VanDeun2009,
   author = {Katrijn Van Deun and Age K Smilde and Mariët J van der Werf and Henk AL Kiers and Iven Van Mechelen},
   doi = {10.1186/1471-2105-10-246},
   issn = {1471-2105},
   issue = {1},
   journal = {BMC Bioinformatics},
   pages = {246},
   publisher = {BioMed Central},
   title = {A structured overview of simultaneous component based data integration},
   volume = {10},
   url = {http://www.biomedcentral.com/1471-2105/10/246},
   year = {2009}
}
@article{,
   abstract = {In this paper we present two methods to analyze three-way data ar-rays with double neighbourhood relations. The first procedure use Kronecker product between graph matrices to construct a neighbourhood operator. Some of the most significant eigenvectors of this operator allows modelization of the underlying phenomena. The second methods make Kronecker product between neighbourhood operators of each graph matrices and is equivalent to a particular STATIS. A comparison between these two procedures on ecological data set is then performed.},
   author = {Pierre-Andre Comillon and Pietro Amenta and Robert Sabatiert},
   keywords = {Geary coefficient,Graph,Kronecker product,Neighbourhood operator,STA-TIS method,Three-ways data},
   title = {Three-Way Data Arrays with Double Neighbourhood Relations as a Tool to Analyze a Contiguity Structure},
   url = {http://download.springer.com.svproxy01.cimat.mx/static/pdf/774/chp%253A10.1007%252F978-3-642-60126-2_33.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-60126-2_33&token2=exp=1491851905~acl=%2Fstatic%2Fpdf%2F774%2Fchp%25253A10.}
}
@article{Licandro2000,
   abstract = {Year-to-year variations in abundance and composition of zooplankton were studied in the Ligurian Sea at a station sampled two times a month between 1985 and 1995. As a break of 2 years (April 1989-December 1990) occurred in the time series, the STATIS method was chosen instead of time series analysis. Each of the nine sampled years was a single table of monthly or seasonal average densities of 26 plankton taxa. STATIS allowed (i) estimation of similarity between each yearly table, (ii) visualization of the trajectories of both species and observations (seasons) from one year to another, and (iii) associations of particular species, which showed similar temporal variations, to be determined. A strong seasonal variation was evident for most species, and years 1987, 1992 and 1994 were different from the others. Trajectories indicated which species were stable and which were characterized by small or large fluctuations during the nine years. Five different taxa associations were detected. For each association, the most representative period was identified, where each period was a group of months obtained by clustering on species abundances. Taking into account hydro-climatic factors in the representative periods, a contingency discriminant analysis allowed us to identify and characterize the most discriminant environmental parameters associated with each group of species. Environmental factors that best discriminated the different representative periods were atmospheric pressure, current speed and direction, and water temperature.},
   author = {Priscilla Licandro},
   doi = {10.1093/plankt/22.12.2225},
   isbn = {0142-7873},
   issn = {14643774},
   issue = {12},
   journal = {Journal of Plankton Research},
   month = {12},
   pages = {2225-2253},
   title = {Changes of zooplankton communities in the Gulf of Tigullio (Ligurian Sea, Western Mediterranean) from 1985 to 1995. Influence of hydroclimatic factors},
   volume = {22},
   url = {http://plankt.oxfordjournals.org/cgi/content/abstract/22/12/2225 https://academic.oup.com/plankt/article-lookup/doi/10.1093/plankt/22.12.2225},
   year = {2000}
}
@article{Enachescu2003,
   abstract = {The paper discusses techniques to emphasize patterns in citation data\nand to study their dynamics. In this context, the STATIS and the STATIS\ndual methods are presented. The methods are a generalization of the\nprincipal component analysis from a dynamical point of view. STATIS and\nits dual are applied in order to illustrate the dynamic of `citing-cited\npatterns' by using citation data of sixteen major journals from the\nstatistics field.},
   author = {Cornelia Enachescu and Tiberiu Postelnicu},
   doi = {10.1023/A:1021946506987},
   isbn = {0138-9130},
   issn = {01389130},
   issue = {1},
   journal = {Scientometrics},
   pages = {43-59},
   title = {Patterns in journal citation data revealed by exploratory multivariate analysis},
   volume = {56},
   url = {http://link.springer.com/10.1023/A:1021946506987},
   year = {2003}
}
@article{,
   abstract = {In this paper the air pollution data related to SO2 emissions are studied for 14 stations in the city of Montreal for 1975. Some descriptive statistics are computed in order to characterize the overall pollution situation: monthly means of hourly measures, overall monthly means, relative frequencies of daily means and of daily maxima. The seasonal effects can easily be seen. Then some comparative analyses are made in order to study the proximities between the stations: Chernoff's faces, Andrews' graphs, STATIS and arrowhead charts. It is seen that the stations can be separated in three main groups in which different pollution sources can be identified. Finally correlation coefficients are computed for every pair of stations and some interesting comments can be made with respect to the sources of pollution.},
   author = {R Cli~roux and R Roy and N Fortin},
   title = {AIR POLLUTION IN MONTREAL: A STATISTICAL ANALYSIS OF SULPHUR DIOXIDE DATA},
   url = {http://download.springer.com.svproxy01.cimat.mx/static/pdf/357/art%253A10.1007%252FBF02279542.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF02279542&token2=exp=1491852477~acl=%2Fstatic%2Fpdf%2F357%2Fart%25253A10.1007%25252FBF0227954}
}
@book{Rubin2004,
   abstract = {Statistical background -- Underlying Bayesian theory -- Randomization-based evaluations -- Procedures with ignorable nonresponse -- Procedures with nonignorable nonresponse.},
   author = {Donald B. Rubin},
   isbn = {9780471655749},
   pages = {287},
   publisher = {Wiley-Interscience},
   title = {Multiple imputation for nonresponse in surveys},
   year = {2004}
}
@book{Rubin1987,
   abstract = {Introduction -- Statistical Background. Underlying Bayesian Theory -- Randomization-Based Evaluations -- Procedures with Ignorable Nonresponse -- Procedures with Nonignorable Nonresponse -- Problems. References -- Index.},
   author = {Donald B. Rubin},
   isbn = {9780471087052},
   pages = {258},
   publisher = {Wiley},
   title = {Multiple imputation for nonresponse in surveys},
   url = {https://www.abebooks.com/9780471087052/Multiple-Imputation-Nonresponse-Surveys-Wiley-047108705X/plp},
   year = {1987}
}
@book{Little2002,
   abstract = {2nd ed. The Problem of Missing Data -- Missing-Data Patterns -- Mechanisms That Lead to Missing Data -- A Taxonomy of Missing-Data Methods -- Missing Data in Experiments -- The Exact Least Squares Solution with Complete Data -- The Correct Least Squares Analysis with Missing Data -- Filling in Least Squares Estimates -- Bartlett's ANCOVA Method -- Least Squares Estimates of Missing Values by ANCOVA Using Only Complete-Data Methods -- Correct Least Squares Estimates of Standard Errors and One Degree of Freedom Sums of Squares -- Correct Least Squares Sums of Squares with More Than One Degree of Freedom -- Complete-Case and Available-Case Analysis, Including Weighting Methods -- Complete-Case Analysis -- Weighted Complete-Case Analysis -- Available-Case Analysis -- Single Imputation Methods -- Imputing Means from a Predictive Distribution -- Imputing Draws from a Predictive Distribution -- Estimation of Imputation Uncertainty -- Imputation Methods that Provide Valid Standard Errors from a Single Filled-in Data Set -- Standard Errors for Imputed Data by Resampling -- Introduction to Multiple Imputation -- Comparison of Resampling Methods and Multiple Imputation -- Likelihood-Based Approaches to the Analysis of Missing Data -- Theory of Inference Based on the Likelihood Function -- Review of Likelihood-Based Estimation for Complete Data -- Likelihood-Based Inference with Incomplete Data -- A Generally Flawed Alternative to Maximum Likelihood: Maximizing Over the Parameters and the Missing Data -- Likelihood Theory for Coarsened Data. Factored likelihood methods, ignoring the missing-data mechanism -- Bivariate normal data with one variable subject to nonresponse : ML estimation -- Bivariate normal monotone data : small-sample inference -- monotone data with more than two variables -- Factorizations for special nonmonotone patterns -- Maximum likelihood for general patterns of missing data : introduction and theory with ignorable nonresponse -- Alternative computational strategies -- Introduction to the EM algorithm -- The E and M steps of EM -- Theory of the EM algorithm -- extensions of EM -- Hybrid maximization methods -- Large-sample inference based on maximum likelihood estimates -- Standard errors based on the information matrix -- Standard errors via methods that do not require computing and inverting an estimate of the observed information matrix -- Bayes and multiple imputation -- Bayesian iterative simulation methods -- Multiple imputation -- Multivariate normal examples, ignoring the missing-data mechanism -- Inference for a mean vector and covariance matrix with missing data under normality -- Estimation with a restricted covariance matrix -- Multiple linear regression -- A general repeated-measures model with missing data -- Time series models -- Robust estimation -- Robust estimation for a univariate sample -- Robust estimation of the mean and covariance matrix -- Further extensions of the t model. Models for partially classified contingency tables, ignoring the missing-data mechanism -- Factored likelihoods for monotone multinomial data -- ML and Bayes estimation for multinomial samples with general patterns of missing data -- Loglinear models for partially classified contingency tables -- mixed normal and non-normal data with missing values, ignoring the missing-data mechanism -- The general location model -- The general location model with parameter constraints -- Regression problems involving mixtures of continuous and categorical variables -- Futher extensions of the general location model -- Nonignorable missing-data models -- Likelihood theory for nonignorable models -- Models with known nonignorable missing-data mechanisms : grouped and rounded data -- Normal selection models -- Normal pattern-mixture models -- Nonignorable models for normal repeated-measures data -- Nonignorable models for categorical data.},
   author = {Roderick J. A. Little and Donald B. Rubin},
   isbn = {9780471183860},
   pages = {381},
   publisher = {Wiley},
   title = {Statistical analysis with missing data},
   year = {2002}
}
@book{Schafer1997,
   author = {J. L. (Joseph L.) Schafer},
   isbn = {9780412040610},
   pages = {430},
   publisher = {Chapman \& Hall},
   title = {Analysis of incomplete multivariate data},
   url = {https://www.crcpress.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610},
   year = {1997}
}
@article{Gautam2015,
   author = {Chandan Gautam and Vadlamani Ravi},
   doi = {10.1016/j.neucom.2014.12.073},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {5},
   pages = {134-142},
   title = {Data imputation via evolutionary computation, clustering and a neural network},
   volume = {156},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231214017378},
   year = {2015}
}
@article{Nishanth2016,
   author = {Kancherla Jonah Nishanth and Vadlamani Ravi},
   doi = {10.1016/j.neucom.2016.08.044},
   issn = {09252312},
   journal = {Neurocomputing},
   month = {12},
   pages = {17-25},
   title = {Probabilistic neural network based categorical data imputation},
   volume = {218},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231216309407},
   year = {2016}
}
@article{FigueroaGarca2011,
   abstract = {This paper presents a proposal based on an evolutionary algorithm to impute missing observations in multivariate data. A genetic algorithm based on the minimization of an error function derived from their covariance matrix and vector of means is presented. All methodological aspects of the genetic structure are presented. An extended explanation of the design of the fitness function is provided. An application example is solved by the proposed method. © 2011 Elsevier Ltd. All rights reserved.},
   author = {Juan C. Figueroa García and Dusko Kalenatic and Cesar Amilcar Lopez Bello},
   doi = {10.1016/j.chb.2010.06.026},
   isbn = {0747-5632},
   issn = {07475632},
   issue = {5},
   journal = {Computers in Human Behavior},
   keywords = {Evolutionary optimization,Missing data,Multiple data imputation,Multivariate analysis},
   pages = {1468-1474},
   title = {Missing data imputation in multivariate data by evolutionary algorithms},
   volume = {27},
   year = {2011}
}
@article{Lobato2015,
   author = {Fabio Lobato and Claudomiro Sales and Igor Araujo and Vincent Tadaiesky and Lilian Dias and Leonardo Ramos and Adamo Santana},
   doi = {10.1016/j.patrec.2015.08.023},
   issn = {01678655},
   journal = {Pattern Recognition Letters},
   month = {12},
   pages = {126-131},
   title = {Multi-objective genetic algorithm for missing data imputation},
   volume = {68},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515002883},
   year = {2015}
}
@book{Marwala2009,
   abstract = {"This book is for those who use data analysis to build decision support systems, particularly engineers, scientists and statisticians"--Provided by publisher.},
   author = {Tshilidzi Marwala},
   doi = {10.4018/978-1-60566-336-4},
   isbn = {9781605663364 (hardcover)\r9781605663371 (ebook)},
   journal = {Computational Intelligence},
   pages = {1-327},
   pmid = {15460030},
   title = {Computational Intelligence for Missing Data Imputation, Estimation and Management},
   year = {2009}
}
@article{Silva2016,
   author = {Hiroshi De Silva and A Shehan Perera},
   isbn = {9781509060788},
   pages = {141-146},
   title = {Missing Data Imputation using Evolutionary k- Nearest Neighbor Algorithm for Gene Expression Data},
   year = {2016}
}
@article{OrdezGaln2017,
   abstract = {This article proposes a new missing data imputation method based on genetic algorithms. The algorithm presented in this paper is a useful tool for the completion of missing data in knowledge and skills tests. This algorithm uses both Bayesian and Akaike's information criterions as fitness functions and applies them to the classical item response theory models of one, two and three parameters. The results obtained by this new algorithm have been compared with those achieved by means of the Multivariate Imputation by Chained Equations (MICE) algorithm. For all the missing data ratios checked, the average incorrect imputation percentages obtained with the GA algorithm were, statistically, significantly lower than the results obtained with the MICE method. The most favorable frameworks for the use of the algorithm developed in the present research are those questionnaires in which missing answers would be considered as missing completely at random (MCAR). In other words, those questionnaires in which the same questions are present for all the examinees, but not necessarily in the same order.},
   author = {Celestino Ordóñez Galán and Fernando Sánchez Lasheras and Francisco Javier de Cos Juez and Antonio Bernardo Sánchez},
   doi = {10.1016/j.cam.2016.08.012},
   isbn = {03770427},
   issn = {03770427},
   journal = {Journal of Computational and Applied Mathematics},
   keywords = {Genetic algorithms,Imputation method,Item response theory,Missing data,Multivariate imputation by chained equations (MICE},
   pages = {704-717},
   publisher = {Elsevier B.V.},
   title = {Missing data imputation of questionnaires by means of genetic algorithms with different fitness functions},
   volume = {311},
   url = {http://dx.doi.org/10.1016/j.cam.2016.08.012},
   year = {2017}
}
@book{,
   isbn = {9783790813265},
   title = {No Title}
}
@article{Novella2013,
   author = {Pedro Revilla Novella},
   title = {Métodos estadísticos de depuración e imputación de datos},
   year = {2013}
}
@article{Efron1994,
   abstract = {Missing data refers to a class of problems made difficult by the absence of some portions of a familiar data structure. For example, a regression problem might have some missing values in the predictor vectors. This article concerns nonparametric approaches to assessing the accuracy of an estimator in a missing data situation. Three main topics are discussed: bootstrap methods for missing data, these methods' relationship to the theory of multiple imputation, and computationally efficient ways of executing them. The simplest form of nonparametric bootstrap confidence interval turns out to give convenient and accurate answers. There are interesting practical and theoretical differences between bootstrap methods and the multiple imputation approach, as well as some useful similarities.},
   author = {Bradley Efron},
   doi = {10.1080/01621459.1994.10476768},
   isbn = {0162-1459},
   issn = {0162-1459},
   issue = {426},
   journal = {Journal of the American Statistical Association},
   keywords = {bayesian bootstrap,bootstrap confidence intervals,data augmentation,ignorable nonresponse,nonparametric},
   pages = {463-475},
   title = {Missing Data, Imputation, and the Bootstrap},
   volume = {89},
   url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476768},
   year = {1994}
}
@article{Efron1994,
   author = {Bradley Efron},
   issue = {426},
   pages = {478-479},
   title = {Missing Data , Imputation , and the Bootstrap : Rejoinder Author ( s ): Bradley Efron Source : Journal of the American Statistical Association , Vol . 89 , No . 426 ( Jun ., 1994 ), pp . Published by : Taylor \& Francis , Ltd . on behalf of the American St},
   volume = {89},
   year = {1994}
}
@article{Baraldi2010,
   author = {Amanda N. Baraldi and Craig K. Enders},
   doi = {10.1016/j.jsp.2009.10.001},
   issn = {00224405},
   issue = {1},
   journal = {Journal of School Psychology},
   month = {2},
   pages = {5-37},
   title = {An introduction to modern missing data analyses},
   volume = {48},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0022440509000661},
   year = {2010}
}
@article{Donders2006,
   author = {A. Rogier T. Donders and Geert J.M.G. van der Heijden and Theo Stijnen and Karel G.M. Moons},
   doi = {10.1016/j.jclinepi.2006.01.014},
   issn = {08954356},
   issue = {10},
   journal = {Journal of Clinical Epidemiology},
   month = {10},
   pages = {1087-1091},
   title = {Review: A gentle introduction to imputation of missing values},
   volume = {59},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435606001971},
   year = {2006}
}
@article{Molenberghs2008,
   author = {Geert Molenberghs and Caroline Beunckens and Cristina Sotto and Michael G. Kenward},
   doi = {10.1111/j.1467-9868.2007.00640.x},
   issn = {1369-7412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   keywords = {Contingency table,Ignorability,Missing completely at random,Pattern,Selection model,Shared parameter model,mixture model},
   month = {4},
   pages = {371-388},
   publisher = {Blackwell Publishing Ltd},
   title = {Every missingness not at random model has a missingness at random counterpart with equal fit},
   volume = {70},
   url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00640.x},
   year = {2008}
}
@article{Garca-Laencina2010,
   author = {Pedro J. García-Laencina and José-Luis Sancho-Gómez and Aníbal R. Figueiras-Vidal},
   doi = {10.1007/s00521-009-0295-6},
   issn = {0941-0643},
   issue = {2},
   journal = {Neural Computing and Applications},
   month = {3},
   pages = {263-282},
   publisher = {Springer-Verlag},
   title = {Pattern classification with missing data: a review},
   volume = {19},
   url = {http://link.springer.com/10.1007/s00521-009-0295-6},
   year = {2010}
}
@article{Little2017,
   author = {Roderick J. Little and Donald B. Rubin and Sahar Z. Zangeneh},
   doi = {10.1080/01621459.2015.1136826},
   issn = {0162-1459},
   issue = {517},
   journal = {Journal of the American Statistical Association},
   month = {1},
   pages = {314-320},
   title = {Conditions for Ignoring the Missing-Data Mechanism in Likelihood Inferences for Parameter Subsets},
   volume = {112},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2015.1136826},
   year = {2017}
}
@article{Roth1999,
   abstract = {Researchers in many fields use multiple item scales to measure important variables such as attitudes and personality traits, but find that some respondents failed to complete certain items. Past missing data research focuses on missing entire instruments, and is of limited help because there are few variables to help impute missing scores and the variables are often not highly related to each other. Multiple item scales offer the unique opportunity to impute missing values from other correlated items designed to measure the same construct. A Monte Carlo analysis was conducted to compare several missing data techniques. The techniques included listwise deletion, regression imputation, hot-deck imputation, and two forms of mean substitution. Results suggest that regression imputation and substituting the mean response of a person to other items on a scale are very promising approaches. Furthermore, the imputation techniques often outperformed listwise deletion.},
   author = {Philip L. Roth and Fred S. Switzer and Deborah M. Switzer},
   doi = {10.1177/109442819923001},
   issn = {1094-4281},
   issue = {3},
   journal = {Organizational Research Methods},
   month = {7},
   pages = {211-232},
   publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
   title = {Missing Data in Multiple Item Scales: A Monte Carlo Analysis of Missing Data Techniques},
   volume = {2},
   url = {http://journals.sagepub.com/doi/10.1177/109442819923001},
   year = {1999}
}
@article{Song2007,
   author = {Qinbao Song and Martin Shepperd},
   doi = {10.1016/j.jss.2006.05.003},
   issn = {01641212},
   issue = {1},
   journal = {Journal of Systems and Software},
   month = {1},
   pages = {51-62},
   title = {A new imputation method for small software project data sets},
   volume = {80},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121206001464},
   year = {2007}
}
@article{Merlin2010,
   author = {Paul Merlin and Antti Sorjamaa and Bertrand Maillet and Amaury Lendasse},
   doi = {10.1016/j.neucom.2009.11.019},
   issn = {09252312},
   issue = {7-9},
   journal = {Neurocomputing},
   month = {3},
   pages = {1103-1108},
   title = {X-SOM and L-SOM: A double classification approach for missing value imputation},
   volume = {73},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231210000196},
   year = {2010}
}
@article{Gupta1996,
   author = {Amit Gupta and Monica S. Lam},
   doi = {10.1057/jors.1996.21},
   issn = {0160-5682},
   issue = {2},
   journal = {Journal of the Operational Research Society},
   month = {2},
   pages = {229-238},
   publisher = {Palgrave Macmillan UK},
   title = {Estimating Missing Values Using Neural Networks},
   volume = {47},
   url = {http://link.springer.com/10.1057/jors.1996.21},
   year = {1996}
}
@article{Batista2002,
   abstract = {Data quality is a major concern in Machine Learning and other correlated areas such as Knowledge Discovery from Databases (KDD). As most Machine Learning algorithms induce knowledge strictly from data, the quality of the knowledge extracted is largely determined by the quality of the underlying data. One relevant problem in data quality is the presence of missing data. Despite the frequent occurrence of missing data, many Machine Learning algorithms handle missing data in a rather naive way. Missing data treatment should be carefully thought, otherwise bias might be introduced into the knowledge induced. In this work, we analyse the use of the k-nearest neighbour as an imputation method. Imputation is a term that denotes a procedure that replaces the missing values in a data set by some plausible values. Our analysis indicates that missing data imputation based on the k-nearest neighbour algorithm can outperform the internal methods used by C4.5 and CN2 to treat missing data.},
   author = {Gustavo E. A. P. A. Batista and Maria C. Monard},
   issue = {October 2002},
   journal = {Frontiers in Artificial Intelligence and Applications},
   pages = {251-260},
   title = {A study of k-nearest neighbour as an imputation method},
   volume = {87},
   year = {2002}
}
@article{Gabrys2002,
   author = {Bogdan Gabrys},
   doi = {10.1016/S0888-613X(02)00070-1},
   issn = {0888613X},
   issue = {3},
   journal = {International Journal of Approximate Reasoning},
   month = {9},
   pages = {149-179},
   title = {Neuro-fuzzy approach to processing inputs with missing values in pattern recognition problems},
   volume = {30},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S0888613X02000701},
   year = {2002}
}
@article{Liu2017,
   abstract = {Many clinical research datasets have a large percentage of missing values that directly impacts their usefulness in yielding high accuracy classifiers when used for training in supervised machine learning. While missing value imputation methods have been shown to work well with smaller percentages of missing values, their ability to impute sparse clinical research data can be problem specific. We previously attempted to learn quantitative guidelines for ordering cardiac magnetic resonance imaging during the evaluation for pediatric cardiomyopathy, but missing data significantly reduced our usable sample size. In this work, we sought to determine if increasing the usable sample size through imputation would allow us to learn better guidelines. We first review several machine learning methods for estimating missing data. Then, we apply four popular methods (mean imputation, decision tree, k-nearest neighbors, and self-organizing maps) to a clinical research dataset of pediatric patients undergoing evaluation for cardiomyopathy. Using Bayesian Rule Learning (BRL) to learn ruleset models, we compared the performance of imputation-augmented models versus unaugmented models. We found that all four imputation-augmented models performed similarly to unaugmented models. While imputation did not improve performance, it did provide evidence for the robustness of our learned models.},
   author = {Yuzhe Liu and Vanathi Gopalakrishnan},
   doi = {10.3390/data2010008},
   issn = {2306-5729},
   issue = {1},
   journal = {Data},
   keywords = {decision tree imputation,k-nearest,k-nearest neighbors imputation,machine learning,missing value imputation,neighbors imputation,self-organizing map imputation},
   pages = {8},
   title = {An Overview and Evaluation of Recent Machine Learning Imputation Methods Using Cardiac Imaging Data},
   volume = {2},
   url = {http://www.mdpi.com/2306-5729/2/1/8},
   year = {2017}
}
@article{Haeri2017,
   author = {Maryam Amir Haeri and Mohammad Mehdi Ebadzadeh and Gianluigi Folino},
   doi = {10.1016/j.asoc.2017.06.050},
   issn = {15684946},
   journal = {Applied Soft Computing},
   publisher = {Elsevier B.V.},
   title = {Statistical Genetic Programming for Symbolic Regression},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494617303939},
   year = {2017}
}
@article{MussaAbdella2005,
   author = {Tshilidzi Marwala Mussa Abdella},
   journal = {Computing and Informatics},
   keywords = {auto-associative,basis function,error function,genetic algorithms,missing data,multi-layer perceptron,neural networks,radial},
   pages = {577-589},
   title = {THE USE OF GENETIC ALGORITHMS AND NEURAL NETWORKS TO APPROXIMATE MISSING DATA Mussa Abdella , Tshilidzi Marwala},
   volume = {24},
   url = {http://www.cai.sk/ojs/index.php/cai/article/viewArticle/401},
   year = {2005}
}
@book{Hutchison2010,
   author = {David Hutchison and John C Mitchell and Anna Isabel Esparcia-alcázar and Anikó Ekárt and Sara Silva and Stephen Dignum},
   doi = {10.1007/978-3-642-12148-7},
   isbn = {978-3-642-12147-0},
   journal = {Theoretical Computer Science},
   title = {Genetic Programming},
   volume = {6021},
   url = {http://www.springerlink.com/index/10.1007/978-3-642-12148-7},
   year = {2010}
}
@inproceedings{Rahman2013,
   abstract = {The task of managing large installations of computer systems presents a number of unique challenges related to heterogeneity, consistency, information flow and documentation. The emerging field of DevOps borrows practices from software engineering to tackle complexity. In this paper we provide an insight in how automation can to improve scalability and testability while simultaneously reducing the operators' work. © 2013 Springer Science+Business Media.},
   author = {M. Mostafizur Rahman and D. N. Davis},
   doi = {10.1007/978-94-007-6190-2-19},
   isbn = {9789400761896},
   issn = {18761100},
   booktitle = {Lecture Notes in Electrical Engineering},
   pmid = {25246403},
   title = {Machine learning-based missing value imputation method for clinical datasets},
   volume = {229 LNEE},
   year = {2013}
}
@article{Ferro2014,
   abstract = {Purpose: The aim of this research was to examine, in an exploratory manner, whether cross-sectional multiple imputation generates valid parameter estimates for a latent growth curve model in a longitudinal data set with nonmonotone missingness. Methods: A simulated longitudinal data set of N= 5000 was generated and consisted of a continuous dependent variable, assessed at three measurement occasions and a categorical time-invariant independent variable. Missing data had a nonmonotone pattern and the proportion of missingness increased from the initial to the final measurement occasion (5%-20%). Three methods were considered to deal with missing data: listwise deletion, full-information maximum likelihood, and multiple imputation. A latent growth curve model was specified and analysis of variance was used to compare parameter estimates between the full data set and missing data approaches. Results: Multiple imputation resulted in significantly lower slope variance compared with the full data set. There were no differences in any parameter estimates between the multiple imputation and full-information maximum likelihood approaches. Conclusions: This study suggested that in longitudinal studies with nonmonotone missingness, cross-sectional imputation at each time point may be viable and produces estimates comparable with those obtained with full-information maximum likelihood. Future research pursuing the validity of this method is warranted. © 2014 Elsevier Inc.},
   author = {Mark A. Ferro},
   doi = {10.1016/j.annepidem.2013.10.007},
   isbn = {1873-2585 (Electronic)\r1047-2797 (Linking)},
   issn = {10472797},
   issue = {1},
   journal = {Annals of Epidemiology},
   pmid = {24210708},
   title = {Missing data in longitudinal studies: Cross-sectional multiple imputation provides similar estimates to full-information maximum likelihood},
   volume = {24},
   year = {2014}
}
@article{Yozgatligil2013,
   abstract = {This study aims to compare several imputation methods to complete the missing values of spatio--temporal meteorological time series. To this end, six imputation methods are assessed with respect to various criteria including accuracy, robustness, precision, and efficiency for artificially created missing data in monthly total precipitation and mean temperature series obtained from the Turkish State Meteorological Service. Of these methods, simple arithmetic average, normal ratio (NR), and NR weighted with correlations comprise the simple ones, whereas multilayer perceptron type neural network and multiple imputation strategy adopted by Monte Carlo Markov Chain based on expectation--maximization (EM-MCMC) are computationally intensive ones. In addition, we propose a modification on the EM-MCMC method. Besides using a conventional accuracy measure based on squared errors, we also suggest the correlation dimension (CD) technique of nonlinear dynamic time series analysis which takes spatio--temporal dependencies into account for evaluating imputation performances. Depending on the detailed graphical and quantitative analysis, it can be said that although computational methods, particularly EM-MCMC method, are computationally inefficient, they seem favorable for imputation of meteorological time series with respect to different missingness periods considering both measures and both series studied. To conclude, using the EM-MCMC algorithm for imputing missing values before conducting any statistical analyses of meteorological data will definitely decrease the amount of uncertainty and give more robust results. Moreover, the CD measure can be suggested for the performance evaluation of missing data imputation particularly with computational methods since it gives more precise results in meteorological time series.},
   author = {Ceylan Yozgatligil and Sipan Aslan and Cem Iyigun and Inci Batmaz},
   doi = {10.1007/s00704-012-0723-x},
   isbn = {0177-798X},
   issn = {0177798X},
   issue = {1-2},
   journal = {Theoretical and Applied Climatology},
   title = {Comparison of missing value imputation methods in time series: The case of Turkish meteorological data},
   volume = {112},
   year = {2013}
}
@article{Wang2013,
   abstract = {In this paper, bootstrapped wavelet neural network (BWNN) was developed for predicting monthly ammonia nitrogen (NH(4+)-N) and dissolved oxygen (DO) in Harbin region, northeast of China. The Morlet wavelet basis function (WBF) was employed as a nonlinear activation function of traditional three-layer artificial neural network (ANN) structure. Prediction intervals (PI) were constructed according to the calculated uncertainties from the model structure and data noise. Performance of BWNN model was also compared with four different models: traditional ANN, WNN, bootstrapped ANN, and autoregressive integrated moving average model. The results showed that BWNN could handle the severely fluctuating and non-seasonal time series data of water quality, and it produced better performance than the other four models. The uncertainty from data noise was smaller than that from the model structure for NH(4+)-N; conversely, the uncertainty from data noise was larger for DO series. Besides, total uncertainties in the low-flow period were the biggest due to complicated processes during the freeze-up period of the Songhua River. Further, a data missing-refilling scheme was designed, and better performances of BWNNs for structural data missing (SD) were observed than incidental data missing (ID). For both ID and SD, temporal method was satisfactory for filling NH(4+)-N series, whereas spatial imputation was fit for DO series. This filling BWNN forecasting method was applied to other areas suffering "real" data missing, and the results demonstrated its efficiency. Thus, the methods introduced here will help managers to obtain informed decisions.},
   author = {Yi Wang and Tong Zheng and Ying Zhao and Jiping Jiang and Yuanyuan Wang and Liang Guo and Peng Wang},
   doi = {10.1007/s11356-013-1874-8},
   isbn = {1135601318748},
   issn = {09441344},
   issue = {12},
   journal = {Environmental Science and Pollution Research},
   pmid = {23749372},
   title = {Monthly water quality forecasting and uncertainty assessment via bootstrapped wavelet neural networks under missing data for Harbin, China},
   volume = {20},
   year = {2013}
}
@article{Kang2013,
   abstract = {Most learning algorithms generally assume that data is complete so each attribute of all instances is filled with a valid value. However, missing values are very common in real datasets for various reasons. In this paper, we propose a new single imputation method based on locally linear reconstruction (LLR) that improves the prediction performance of supervised learning (classification & regression) with missing values. First, we investigate how missing values degrade the prediction performance with various missing ratios. Next, we compare the proposed missing value imputation method (LLR) with six well-known single imputation methods for five different learning algorithms based on 13 classification and nine regression datasets. The experimental results showed that (1) all imputation methods helped to improve the prediction accuracy, although some were very simple; (2) the proposed LLR imputation method enhanced the modeling performance more than all other imputation methods, irrespective of the learning algorithms and the missing ratios; and (3) LLR was outstanding when the missing ratio was relatively high and its prediction accuracy was similar to that of the complete dataset. ?? 2013 Elsevier B.V.},
   author = {Pilsung Kang},
   doi = {10.1016/j.neucom.2013.02.016},
   isbn = {8229707286},
   issn = {09252312},
   journal = {Neurocomputing},
   title = {Locally linear reconstruction based missing value imputation for supervised learning},
   volume = {118},
   year = {2013}
}
@book{Schaefer2013,
   abstract = {The task of managing large installations of computer systems presents a number of unique challenges related to heterogeneity, consistency, information flow and documentation. The emerging field of DevOps borrows practices from software engineering to tackle complexity. In this paper we provide an insight in how automation can to improve scalability and testability while simultaneously reducing the operators' work. © 2013 Springer Science+Business Media.},
   author = {Andreas Schaefer and Marc Reichenbach and Dietmar Fey},
   doi = {10.1007/978-94-007-4786-9},
   isbn = {978-94-007-4785-2},
   issn = {18761100},
   journal = {Lecture Notes in Electrical Engineering},
   keywords = {Administration,Automation,DevOps,Heterogeneous Systems,System Management,eLearning},
   pages = {345-358},
   pmid = {25246403},
   title = {IAENG Transactions on Engineering Technologies},
   volume = {170},
   url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84867136837&partnerID=tZOtx3y1},
   year = {2013}
}
@article{Gmez-Carracedo2014,
   abstract = {Datasets with missing data ratios ranging from 24% to 4%, corresponding to three air quality monitoring studies, were used to ascertain whether major differences occur when five currently used imputation methods are applied (four single imputation methods and a multiple imputation one). Unrotated and Varimax-rotated factor analyses performed on the imputed datasets were compared. All methods performed similarly, although multiple imputation yielded more disperse imputed values. Main differences occurred when a variable with missing values correlated poorly to the other features and when a variable had relevant loadings in several unrotated factors, which sometimes changed the order of the rotated factors. ?? 2014 Elsevier B.V.},
   author = {M. P. Gómez-Carracedo and J. M. Andrade and P. López-Mahía and S. Muniategui and D. Prada},
   doi = {10.1016/j.chemolab.2014.02.007},
   issn = {18733239},
   journal = {Chemometrics and Intelligent Laboratory Systems},
   keywords = {Air quality,Expectation-maximization,Missing data,Multiple imputation,Single imputation},
   pages = {23-33},
   publisher = {Elsevier B.V.},
   title = {A practical comparison of single and multiple imputation methods to handle complex missing data in air quality datasets},
   volume = {134},
   url = {http://dx.doi.org/10.1016/j.chemolab.2014.02.007},
   year = {2014}
}
@article{Soley-bori2013,
   abstract = {This tech report p resents the basic concepts and methods used to deal with missing data. After explaining the missing data mechanisms and the patterns of missingness, the main conventional methodologies are reviewed , including L istwise deletion, I mputation methods, Mu ltipl e Im putation, M aximum L ikelihood and B ayesian methods. A dvantages and limitations are specified so that the reader is able to identify the mai n trade - offs when using each method. The report also summarizes how to carry out Multiple Imputation and Maximum Likelihood using SAS and STATA},
   author = {Marina Soley-bori},
   issue = {4},
   journal = {PM931 Directed Study in Health Policy and Management},
   pages = {20},
   title = {Dealing with missing data: Key assumptions and methods for applied analysis},
   year = {2013}
}
@book{johnwGraham2007,
   author = {john w. Graham},
   doi = {10.1007/b12532},
   isbn = {9780387329161},
   issn = {0028-4793},
   journal = {Introduction to Variance estimation},
   title = {Missing Data Analysis and Design},
   year = {2007}
}
@article{Henley2006,
   abstract = {SQL is the (more or less) standardised language that is used by the majority of commercial database management systems. However, it is seriously flawed, as has been documented in detail by Date, Darwen, Pascal, and others. One of the most serious problems with SQL is the way it handles missing data. It uses a special value 'NULL' to represent data items whose value is not known. This can have a variety of meanings in different circumstances (such as 'inapplicable' or 'unknown'). The SQL language also allows an 'unknown' truth value in logical expressions. The resulting incomplete three-valued logic leads to inconsistencies in data handling within relational database management systems. Relational database theorists advocate that a strict two-valued logic (true/false) be used instead, with prohibition of the use of NULL, and justify this stance by assertion that it is a true representation of the 'real world'. Nevertheless, in real geoscience data there is a complete gradation between exact values and missing data: for example, geochemical analyses are inexact (and the uncertainty should be recorded); the precision of numeric or textual data may also be expressed qualitatively by terms such as 'approximately' or 'possibly'. Furthermore, some data are by their nature incomplete: for example, where samples could not be collected or measurements could not be taken because of inaccessibility. It is proposed in this paper that the best way to handle such data sets is to replace the closed-world assumption and its concomitant strict two-valued logic, upon which the present relational database model is based, by the open-world assumption which allows for other logical values in addition to the extremes of 'true' and 'false'. Possible frameworks for such a system are explored, and could use Codd's 'marks', Darwen's approach (recording the status of information known about each data item), or other approaches such as fuzzy logic. ?? 2006 Elsevier Ltd. All rights reserved.},
   author = {Stephen Henley},
   doi = {10.1016/j.cageo.2005.12.008},
   issn = {00983004},
   issue = {9},
   journal = {Computers and Geosciences},
   keywords = {Closed-world assumption,Fuzzy logic,Logic,Missing data,Open-world assumption,Relational database,SQL},
   pages = {1368-1377},
   pmid = {32134},
   title = {The problem of missing data in geoscience databases},
   volume = {32},
   year = {2006}
}
@article{Lakshminarayan1999,
   author = {Kamakshi Lakshminarayan and Steven A. Harp and Tariq Samad},
   doi = {10.1023/A:1008334909089},
   issn = {0924669X},
   issue = {3},
   journal = {Applied Intelligence},
   pages = {259-275},
   publisher = {Kluwer Academic Publishers},
   title = {Imputation of Missing Data in Industrial Databases},
   volume = {11},
   url = {http://link.springer.com/10.1023/A:1008334909089},
   year = {1999}
}
@article{Heeringa2000,
   abstract = {Ph.D.},
   author = {Steven George Heeringa},
   keywords = {Coarsened,Expectation,Gibbs Sampler,Household Wealth,Multivariate Imputation,Survey Data,maximization},
   title = {Multivariate imputation of coarsened survey data on household wealth.},
   url = {https://deepblue.lib.umich.edu/handle/2027.42/132369},
   year = {2000}
}
@article{Allison2003,
   abstract = {As with other statistical methods, missing data often create major problems for the estimation of structural equation models (SEMs). Conventional methods such as listwise or pairwise deletion generally do a poor job of using all the available information. However, structural equation modelers are fortunate that many programs for estimating SEMs now have maximum likelihood methods for handling missing data in an optimal fashion. In addition to maximum likelihood, this article also discusses multiple imputation. This method has statistical properties that are almost as good as those for maximum likelihood and can be applied to a much wider array of models and estimation methods.},
   author = {Paul D Allison},
   doi = {10.1037/0021-843X.112.4.545},
   isbn = {1939-1846},
   issn = {0021-843X},
   issue = {4},
   journal = {Journal of Abnormal Psychology; Journal of Abnormal Psychology},
   keywords = {Adult,Antisocial Personality Disorder,Antisocial Personality Disorder: epidemiology,Bias (Epidemiology),Child,Humans,Likelihood Functions,Mathematical Computing,Models,Monte Carlo Method,Probability,Psychometrics,Psychometrics: statistics & numerical data,Psychopathology,Psychopathology: statistics & numerical data,Statistical},
   pages = {545},
   pmid = {14674868},
   title = {Missing data techniques for structural equation modeling},
   volume = {112},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/14674868},
   year = {2003}
}
@article{Carter2006,
   abstract = {Many times in both educational and social science research it is impossible to collect data that is complete. When administering a survey, for example, people may answer some questions and not others. This missing data causes a problem for researchers using structural equation modeling (SEM) techniques for data analyses. Because SEM and multivariate methods require complete data, several methods have been proposed for dealing with these missing data. What follows is a review of several methods currently used, a description of strengths and weaknesses of each method, and a proposal for future research.},
   author = {Rufus Lynn Carter},
   issue = {1},
   journal = {Research \{\&\} Practice in Assessment},
   keywords = {Missing data,SEM,solutiona},
   pages = {1-6},
   title = {Solutions for missing data in structural equation modeling},
   volume = {1},
   url = {http://www.kokdemir.info/courses/psi748/docs/[p]%5CnSolutions%5Cnfor%5CnMissing%5CnData%5Cnin%5CnSEM.pdf},
   year = {2006}
}
@article{Bhaskaran2014,
   abstract = {The terminology describing missingness mechanisms is confusing. In particular the meaning of 'missing at random' is often misunderstood, leading researchers faced with missing data problems away from multiple imputation, a method with considerable advantages. The purpose of this article is to clarify how 'missing at random' differs from 'missing completely at random' via an imagined dialogue between a clinical researcher and statistician.},
   author = {Krishnan Bhaskaran and Liam Smeeth},
   doi = {10.1093/ije/dyu080},
   issn = {1464-3685},
   issue = {4},
   journal = {International journal of epidemiology},
   keywords = {missing at random,missing data,multiple imputation},
   month = {8},
   pages = {1336-9},
   pmid = {24706730},
   publisher = {Oxford University Press},
   title = {What is the difference between missing completely at random and missing at random?},
   volume = {43},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/24706730 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4121561},
   year = {2014}
}
@article{,
   abstract = {We address the problem of recoverability i.e. deciding whether there exists a con-sistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called 'Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this represen-tation, we derive conditions that the graph should satisfy to ensure recoverability and devise algorithms to detect the presence of these conditions in the graph.},
   author = {Karthika Mohan and Judea Pearl and Jin Tian},
   title = {Graphical Models for Inference with Missing Data},
   url = {http://ftp.cs.ucla.edu/pub/stat_ser/r410.pdf}
}
@article{Peugh2004,
   abstract = {Missing data analyses have received considerable recent attention in the methodological literature, and two “modern” methods, multiple imputation and maximum likelihood estimation, are recommended. The goals of this article are to (a) provide an overview of missing-data theory, maximum likelihood estimation, and multiple imputation; (b) conduct a methodological review of missing-data reporting practices in 23 applied research journals; and (c) provide a demonstration of multiple imputation and maximum likelihood estimation using the Longitudinal Study of American Youth data. The results indicated that explicit discussions of missing data increased substantially between 1999 and 2003, but the use of maximum likelihood estimation or multiple imputation was rare; the studies relied almost exclusively on listwise and pairwise deletion.},
   author = {J. L. Peugh and C. K. Enders},
   doi = {10.3102/00346543074004525},
   issn = {0034-6543},
   issue = {4},
   journal = {Review of Educational Research},
   keywords = {EM algorithm,NORM,maximum likelihood estimation,missing data,multiple imputation},
   month = {1},
   pages = {525-556},
   publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
   title = {Missing Data in Educational Research: A Review of Reporting Practices and Suggestions for Improvement},
   volume = {74},
   url = {http://rer.sagepub.com/cgi/doi/10.3102/00346543074004525},
   year = {2004}
}
@article{Dong2013,
   abstract = {The impact of missing data on quantitative research can be serious, leading to biased estimates of parameters, loss of information, decreased statistical power, increased standard errors, and weakened generalizability of findings. In this paper, we discussed and demonstrated three principled missing data methods: multiple imputation, full information maximum likelihood, and expectation-maximization algorithm, applied to a real-world data set. Results were contrasted with those obtained from the complete data set and from the listwise deletion method. The relative merits of each method are noted, along with common features they share. The paper concludes with an emphasis on the importance of statistical assumptions, and recommendations for researchers. Quality of research will be enhanced if (a) researchers explicitly acknowledge missing data problems and the conditions under which they occurred, (b) principled methods are employed to handle missing data, and (c) the appropriate treatment of missing data is incorporated into review standards of manuscripts submitted for publication.},
   author = {Yiran Dong and Chao-Ying Joanne Peng},
   doi = {10.1186/2193-1801-2-222},
   issn = {2193-1801},
   issue = {1},
   journal = {SpringerPlus},
   keywords = {EM,FIML,Listwise deletion,MAR,MCAR,MI,MNAR,Missing data},
   month = {12},
   pages = {222},
   pmid = {23853744},
   publisher = {Springer},
   title = {Principled missing data methods for researchers.},
   volume = {2},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/23853744 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3701793},
   year = {2013}
}
@misc{Fligner1986,
   abstract = {A class of ranking models is proposed for which the probability of a ranking decreases with increasing distance from a modal ranking. Some special distances, namely those associated with Kendall and Cayley, decompose into a sum of independent components under the uniform distribution. These distances lead to multiparameter generalizations whose parameters may be interpreted as information at various stages in a ranking process. Estimation of model parameters is described, and the results are applied to an example of word associations. A censoring argument motivates simple extensions of these models to include partial rankings. The generalized Cayley distance model is illustrated for random arrangements arising from mechanisms other than ranking.},
   author = {M. A. Fligner and J. S. Verducci},
   doi = {10.2307/2345433},
   journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
   keywords = {Censorship,Generating function,Mathematical independent variables,Mathematical moments,Parametric models,Probabilities,Probability distributions,Random variables,Statistical models,Statistics},
   pages = {359-369},
   publisher = {WileyRoyal Statistical Society},
   title = {Distance Based Ranking Models},
   volume = {48},
   url = {http://www.jstor.org/stable/2345433},
   year = {1986}
}
@article{Ali2012,
   abstract = {This paper performs a comparison of several methods for Kemeny rank aggregation (104 algorithms and combinations thereof in total) originating in social choice theory, machine learning, and theoretical computer science, with the goal of establishing the best trade-offs between search time and performance. We find that, for this theoretically NP-hard task, in practice the problems span three regimes: strong consensus, weak consensus, and no consensus. We make specific recommendations for each, and propose a computationally fast test to distinguish between the regimes.In spite of the great variety of algorithms, there are few classes that are consistently Pareto optimal. In the most interesting regime, the integer program exact formulation, local search algorithms and the approximate version of a theoretically exact branch and bound algorithm arise as strong contenders. © 2011 Elsevier B.V.},
   author = {Alnur Ali and Marina Meilǎ},
   doi = {10.1016/j.mathsocsci.2011.08.008},
   isbn = {01654896},
   issn = {01654896},
   issue = {1},
   journal = {Mathematical Social Sciences},
   pages = {28-40},
   publisher = {Elsevier B.V.},
   title = {Experiments with Kemeny ranking: What works when?},
   volume = {64},
   url = {http://dx.doi.org/10.1016/j.mathsocsci.2011.08.008},
   year = {2012}
}
@article{Meila2007,
   abstract = {We analyze a popular exponential model over rankings called the generalized Mallows model. Estimating the central ranking (or con- sensus ranking) of this model from data is NP-hard. We obtain the following new results: (1) We show that a standard search method can estimate both the central ranking π0 and the model parameters θ ex- actly. The search is n! in the worst case, but is tractable when the true distribution is concentrated around its mode. (2) From a statistical point of view, we show that the generalized Mallows model is jointly exponential in (π0 , θ), and introduce the conjugate prior for this model class. (3) The sufficient statistics are the pairwise marginal probabili- ties that item i is prefered to item j. These probabilities are of interest in various applications. This paper provides the first exact tractable algorithm for their evaluation. Preliminary experiments confirm the theoretical predictions and compare the new algorithm and existing heuristics.},
   author = {Marina Meila and Kapil Phadnis and Arthur Patterson and Julian Bilmes},
   isbn = {0-9749039-3-0},
   issue = {2},
   journal = {Uncertainty in Artificial Intelligence UAI},
   pages = {285-294},
   title = {Consensus ranking under the exponential model},
   volume = {1},
   year = {2007}
}
@article{Dalenius1959,
   author = {Tore Dalenius and Joseph L. Hodges},
   doi = {10.1080/01621459.1959.10501501},
   issn = {0162-1459},
   issue = {285},
   journal = {Journal of the American Statistical Association},
   month = {3},
   pages = {88-101},
   title = {Minimum Variance Stratification},
   volume = {54},
   url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1959.10501501},
   year = {1959}
}
@book{Drechsler2011,
   abstract = {applicability for this approach.},
   author = {Jörg Drechsler},
   doi = {10.1017/CBO9781107415324.004},
   isbn = {9788578110796},
   issn = {1098-6596},
   issue = {9},
   keywords = {icle},
   pages = {1689-1699},
   pmid = {25246403},
   title = {Synthetic Datasets for Statistical Disclosure Control},
   volume = {53},
   year = {2011}
}
@article{Honaker2012,
   author = {James Honaker and Gary King and Matthew Blackwell},
   title = {AMELIA II: A Program for Missing Data},
   url = {https://r.iq.harvard.edu/docs/amelia/amelia.pdf},
   year = {2012}
}
@article{Honghai2005,
   abstract = {In KDD procedure, to fill in missing data typically requires a very large investment of time and energy -often 80% to 90% of a data analysis pro-ject is spent in making the data reliable enough so that the results can be trust-ful. In this paper, we propose a SVM regression based algorithm for filling in missing data, i.e. set the decision attribute (output attribute) as the condition at-tribute (input attribute) and the condition attribute as the decision attribute, then use SVM regression to predict the condition attribute values. SARS data set ex-perimental results show that SVM regression method has the highest precision. The method with which the value of the example that has the minimum distance to the example with missing value will be taken to fill in the missing values takes the second place, and the mean and median methods have lower precision.},
   author = {Feng Honghai and Chen Guoshun and Yin Cheng},
   doi = {10.1007/11553939_83},
   isbn = {3540288961},
   issn = {03029743},
   journal = {International Conference on Knowledge-Based and Intelligent Information and Engineering Systems},
   pages = {581-587},
   title = {A SVM Regression Based Approach to Filling in Missing Values},
   year = {2005}
}
@article{Bouhlila2013,
   abstract = {In this paper, we document a study that involved applying a multiple imputation technique with chained equations to data drawn from the 2007 iteration of the TIMSS database. More precisely, we imputed missing variables contained in the student background datafile for Tunisia (one of the TIMSS 2007 participating countries), by using Van Buuren, Boshuizen, and Knook’s (SM 18:681-694,1999) chained equations approach. We imputed the data in a way that was congenial with the analysis model. We also carried out different diagnostics in order to determine if the imputations were reasonable. Our analysis of multiply imputed data confirmed that the power of multiple imputation lies in obtaining smaller standard errors and narrower confidence intervals in addition to allowing one to work with the entire dataset.},
   author = {Donia S Bouhlila and Fethi Sellaouti},
   doi = {10.1186/2196-0739-1-4},
   issn = {2196-0739},
   issue = {1},
   journal = {Large-scale Assessments in Education},
   pages = {4},
   title = {Multiple imputation using chained equations for missing data in TIMSS: a case study},
   volume = {1},
   url = {http://www.largescaleassessmentsineducation.com/content/1/1/4},
   year = {2013}
}
@article{Hruschka2003,
   abstract = {This work proposes and evaluates a Nearest-Neighbor Method to substitute missing values in datasets formed by continuous attributes. In the substitution process, each instance containing missing values is compared with complete instances, and the closest instance is used to assign the attribute missing value. We evaluate this method in simulations performed in four datasets that are usually employed as benchmarks for data mining methods - Iris Plants, Wisconsin Breast Cancer, Pima Indians Diabetes and Wine Recognition. First, we consider the substitution process as a prediction task. In this sense, we employ two metrics (Euclidean and Manhattan) to simulate substitutions both in original and normalized datasets. The obtained results were compared to those provided by a usually employed method to perform this task, i.e. substitution by the mean value. Based on these simulations, we propose a substitution procedure for the well-known K-Means Clustering Algorithm. Then, we perform clustering simulations, comparing the results obtained in the original datasets with the substituted ones. These results indicate that the proposed method is a suitable estimator for substituting missing values, i.e. it preserves the relationships between variables in the clustering process. Therefore, the proposed Nearest-Neigh or Method is an appropriate data preparation tool for the K-Means Clustering Algorithm.},
   author = {Eduardo R. Hruschka and Estevam R. Hruschka and Nelson F. F. Ebecken},
   doi = {10.1007/978-3-540-24581-0_62},
   isbn = {3-540-20646-9},
   issn = {03029743},
   pages = {723-734},
   title = {Evaluating a Nearest-Neighbor Method to Substitute Continuous Missing Values},
   url = {http://link.springer.com/10.1007/978-3-540-24581-0_62},
   year = {2003}
}
@article{Rubin1977,
   abstract = {Accessed: 03-01-2018 00:33 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A method is given for estimating, in a subjective sense, the effect of nonresponse in sample surveys. Based on Bayesian techniques, this method produces a subjective probability interval for the statistic that would have been calculated if all nonrespondents had responded. Background information which is recorded for both respondents and nonrespondents plays an important role in sharpening the subjective interval. Real survey data of 660 schools with 188 nonrespondents indicates that the method can be useful in practical problems. The general idea can be applied to any problem with nonrespondents or missing data.},
   author = {Donald B. Rubin},
   issue = {359},
   journal = {Source Journal of the American Statistical Association},
   keywords = {Bayesian inference,Incomplete data,Missing data,Nonresponse,Predictive distribution,Sample surveys},
   pages = {538-543},
   title = {Formalizing Subjective Notions About the Effect of Nonrespondents in Sample Surveys Formalizing Sub jective Notions About the Effect of Nonrespondents in Sample Surveys},
   volume = {72},
   url = {http://www.jstor.org/stable/2286214 http://about.jstor.org/terms},
   year = {1977}
}
@inproceedings{Rubin1978,
   abstract = {A general attack on the problem of non-response in sample surveys is outlined from the phenomenological Bayesian perspective. The ob-jective is to develop procedures that are useful in practice. The plan is to impute several values for each missing datum, where the imputed values reflect variation within an imputation model and sensitivity to different imputation models. The analysis of a resultant multi-imputed data set is viewed as simulating predictive dis-tributions of desired summary statistics under imputation models. Three tasks are defined that are needed to create the imputations: the impu-tation task, the estimation task and the model-ling task. The imputation task and estimation task are technical in nature. The modelling task requires the development of new tools appropriate for relating nonrespondents and respondents.},
   author = {Donald B. Rubin},
   booktitle = {Proceedings of the Survey Research Methods Section of the American Statistical Association},
   pages = {20-28},
   title = {Multiple imputations in sample surveys - A phenomeno- logical Bayesian approach to nonresponse},
   url = {http://ww2.amstat.org/sections/SRMS/Proceedings/papers/1978_004.pdf},
   year = {1978}
}
@inbook{Rubin2000,
   author = {Donald B. Rubin},
   city = {Heidelberg},
   doi = {10.1007/978-3-642-57678-2_1},
   booktitle = {COMPSTAT},
   pages = {3-14},
   publisher = {Physica-Verlag HD},
   title = {The broad role of multiple imputation in statistical science},
   url = {http://link.springer.com/10.1007/978-3-642-57678-2_1},
   year = {2000}
}
@article{Stefanakos2001,
   abstract = {A new methodology for the analysis, missing-value completion and simulation of an incomplete nonstationary time series of wave data is presented and validated. The method is based on the nonstationary modelling of long-term time series developed by the authors [J. Geophys. Res. 100 (1995) 16,149]. The missing-value completion is performed at the level of the series of the uncorrelated residuals, obtained after having removed both any systematic trend (e.g. monotone and periodic components) and the correlation structure of the remaining stationary part. The incomplete time series of uncorrelated residuals is then completed by means of simulated data from a population with the same probability law. Combining then all estimated components, a new nonstationary time series without missing values is constructed. Any number of time series with the same statistical structure can be obtained by using different populations of uncorrelated residuals. The missing-value completion procedure is applied to an incomplete time series of significant wave height, and validated using two synthetic time series having the typical structure of many-year long time series of significant wave height. The missing-value patterns used for validation have been obtained from existing (measured) wave datasets with 16.5 and 33% missing values, respectively.},
   author = {Ch.N. Stefanakos and G.A. Athanassoulis},
   doi = {10.1016/S0141-1187(01)00017-7},
   issn = {0141-1187},
   issue = {4},
   journal = {Applied Ocean Research},
   month = {8},
   pages = {207-220},
   publisher = {Elsevier},
   title = {A unified methodology for the analysis, completion and simulation of nonstationary time series with missing values, with application to wave data},
   volume = {23},
   url = {http://www.sciencedirect.com/science/article/pii/S0141118701000177?via%3Dihub},
   year = {2001}
}
@book{Bck2013,
   abstract = {ftp://lumpi.informatik.uni-dortmund.de/ES/papers/ecal95.ps.gz \n},
   author = {Thomas Bäck and Christophe Foussette and Peter Krause},
   doi = {10.1007/978-3-642-40137-4},
   isbn = {9783642401367},
   issn = {16197127},
   journal = {Natural Computing Series},
   title = {Contemporary Evolution Strategies},
   volume = {47},
   year = {2013}
}
@book{Poli2008,
   abstract = {A Field Guide to Genetic Programing is aimed at those new to genetic programming},
   author = {Riccardo Poli and W.B. Langdon and N.F. McPhee},
   isbn = {978-1-4092-0073-4},
   issue = {March},
   journal = {Wyvern},
   keywords = {genetic algorithms,genetic programming},
   pages = {8},
   title = {A Field Guide to Genetic Programing},
   url = {http://www.essex.ac.uk/wyvern/2008-04/Wyvern April 08 7126.pdf},
   year = {2008}
}
@book{Kramer2016,
   author = {Oliver Kramer},
   doi = {10.1007/978-3-319-33383-0},
   isbn = {9783319333816},
   pages = {120},
   title = {Studies in Big Data 20 Machine Learning for Evolution Strategies},
   year = {2016}
}
@article{White2011,
   abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments.},
   author = {Ian R. White and Patrick Royston and Angela M. Wood},
   doi = {10.1002/sim.4067},
   isbn = {1097-0258 (Electronic)\n0277-6715 (Linking)},
   issn = {02776715},
   issue = {4},
   journal = {Statistics in Medicine},
   keywords = {Fully conditional specification,Missing data,Multiple imputation},
   pages = {377-399},
   pmid = {21225900},
   title = {Multiple imputation using chained equations: Issues and guidance for practice},
   volume = {30},
   year = {2011}
}
@book{Cao2017,
   abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries im-portant knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a com-prehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
   author = {Longbing Cao},
   doi = {10.1145/3076253},
   isbn = {978-1-4842-2252-2},
   issn = {03600300},
   issue = {3},
   journal = {ACM Computing Surveys},
   pages = {1-42},
   title = {Data Science},
   volume = {50},
   url = {http://dl.acm.org/citation.cfm?doid=3101309.3076253},
   year = {2017}
}
@book{Tobergte2013,
   abstract = {applicability for this approach.},
   author = {David R. Tobergte and Shirley Curtis},
   doi = {10.1017/CBO9781107415324.004},
   isbn = {9788578110796},
   issn = {1098-6596},
   issue = {9},
   journal = {Journal of Chemical Information and Modeling},
   keywords = {icle},
   pages = {1689-1699},
   pmid = {25246403},
   title = {Machine learning with R},
   volume = {53},
   year = {2013}
}
@article{Drechsler2017,
   author = {Jörg Drechsler},
   title = {Imputation for Item Nonresponse},
   year = {2017}
}
@article{Snchez2007,
   author = {Joaquín Ortega Sánchez},
   pages = {1-93},
   title = {Cómputo Estadístico I Simulación},
   year = {2007}
}
@article{Graff2017,
   abstract = {© 2016 IEEE. Genetic Programming (GP) is an evolutionary algorithm that has received a lot of attention lately due to its success in solving hard real-world problems. Lately, there has been considerable interest in GP's community to develop semantic genetic operators, i.e., operators that work on the phenotype. In this contribution, we describe EvoDAG (Evolving Directed Acyclic Graph) which is a Python library that implements a steady-state semantic Genetic Programming with tournament selection using an extension of our previous crossover operators based on orthogonal projections in the phenotype space. To show the effectiveness of EvoDAG, it is compared against state-of-the-art classifiers on different benchmark problems, experimental results indicate that EvoDAG is very competitive.},
   author = {Mario Graff and Eric S. Tellez and Sabino Miranda-Jiménez and Hugo Jair Escalante},
   doi = {10.1109/ROPEC.2016.7830633},
   isbn = {9781509037940},
   journal = {2016 IEEE International Autumn Meeting on Power, Electronics and Computing, ROPEC 2016},
   title = {EvoDAG: A semantic Genetic Programming Python library},
   year = {2017}
}
@article{Boomsma2009,
   author = {Anne Boomsma},
   doi = {10.1207/S15328007SEM0703},
   issue = {3},
   journal = {Structural Equation Modeling: A Multidisciplinary Journal},
   pages = {461-483},
   title = {Structural Equation Modeling : A Reporting Analyses of Covariance Structures Reporting Analyses of Covariance Structures},
   volume = {7},
   year = {2009}
}
@book{Gail2012,
   author = {Mitchell Gail and Klaus Krickeberg and Jonathan M Samet and Anastasios Tsiatis and Wing Wong},
   isbn = {9781461413523},
   title = {Regression Methods in Biostatistics},
   year = {2012}
}
@book{Dietterich2009,
   abstract = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
   author = {Thomas G. Dietterich},
   doi = {10.1007/978-3-540-75488-6_2},
   isbn = {9781577354260},
   issn = {10450823},
   journal = {IJCAI International Joint Conference on Artificial Intelligence},
   pages = {8-13},
   pmid = {18292226},
   title = {Machine learning in ecosystem informatics and sustainability},
   year = {2009}
}
@book{Ramasubramanian2017,
   abstract = {This book is inspired by the Machine Learning Model Building Process Flow, which provides the reader the ability to understand a ML algorithm and apply the entire process of building a ML model from the raw data. This new paradigm of teaching Machine Learning will bring about a radical change in perception for many of those who think this subject is difficult to learn. Though theory sometimes looks difficult, especially when there is heavy mathematics involved, the seamless flow from the theoretical aspects to example-driven learning provided in Blockchain and Capitalism makes it easy for someone to connect the dots. For every Machine Learning algorithm covered in this book, a 3-D approach of theory, case-study and practice will be given. And where appropriate, the mathematics will be explained through visualization in R. All practical demonstrations will be explored in R, a powerful programming language and software environment for statistical computing and graphics. The various packages and methods available in R will be used to explain the topics. In the end, readers will learn some of the latest technological advancements in building a scalable machine learning model with Big Data.},
   author = {Karthik Ramasubramanian and Abhishek Singh},
   doi = {10.1007/978-1-4842-2334-5},
   isbn = {978-1-4842-2333-8},
   title = {Machine Learning Using R},
   url = {http://link.springer.com/10.1007/978-1-4842-2334-5},
   year = {2017}
}
@book{BeysolowII2017,
   author = {Taweh Beysolow II},
   doi = {10.1007/978-1-4842-2734-3},
   isbn = {978-1-4842-2733-6},
   title = {Introduction to Deep Learning Using R},
   url = {http://link.springer.com/10.1007/978-1-4842-2734-3},
   year = {2017}
}
@book{Blake2004,
   author = {Steve W Blake},
   doi = {10.1088/0031-9155/49/12/004},
   isbn = {9783319284934},
   journal = {Physics in Medicine and Biology},
   title = {Artificial neural network modelling of megavoltage},
   volume = {49},
   year = {2004}
}
@book{Buuren2000,
   abstract = {TNO report PG/VGZ/00.038.},
   author = {S. van Buuren and C. G. M. Oudshoorn},
   isbn = {9067436771},
   publisher = {TNO Prevention and Health, Public Health},
   title = {Multivariate imputation by chained equations : MICE V1.0 users's manual},
   url = {https://books.google.com.mx/books/about/Multivariate_Imputation_by_Chained_Equat.html?id=ZcOzAAAACAAJ&redir_esc=y},
   year = {2000}
}
@article{Buuren2011,
   abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range ofmodels under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
   author = {Stef van Buuren and Karin Groothuis-Oudshoorn},
   doi = {10.18637/jss.v045.i03},
   isbn = {9067436771},
   issn = {1548-7660},
   issue = {3},
   journal = {Journal of Statistical Software},
   keywords = {chained equations,fully conditional specification,gibbs sampler,mice,multiple imputation,passive imputation,predictor selection,r},
   month = {12},
   pages = {1-67},
   pmid = {22289957},
   title = {mice : Multivariate Imputation by Chained Equations in R},
   volume = {45},
   url = {http://www.jstatsoft.org/v45/i03/},
   year = {2011}
}
@article{Smola2004,
   abstract = {In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.},
   author = {Alex J. Smola and Bernhard Sc Olkopf and Bernhard Schölkopf},
   doi = {10.1023/B:STCO.0000035301.49549.88},
   isbn = {0960-3174},
   issn = {09603174},
   issue = {3},
   journal = {Statistics and Computing},
   keywords = {machine learning,regression estimation,support vector machines},
   pages = {199-222},
   pmid = {17969693},
   title = {A tutorial on support vector regression},
   volume = {14},
   url = {https://link.springer.com.svproxy01.cimat.mx/content/pdf/10.1023%2FB%3ASTCO.0000035301.49549.88.pdf},
   year = {2004}
}
@article{Fellegi1976,
   abstract = {This article is concerned with the automatic editing and imputation of survey data using the following criteria. 1. The data in each record should be made to satisfy all edits by changing the fewest possible items of data (fields); 2. As far as possible the frequency structure of the data file should be maintained; 3. Imputation rules should be derived from the corresponding edit rules without explicit specification. A set of procedures is developed for identifying the fewest number of fields which may be changed to make the resulting data satisfy all edits. Several methods are proposed for determining the specific values to be imputed into each field.},
   author = {I. P. Fellegi and D. Holt},
   doi = {10.2307/2285726},
   isbn = {01621459},
   issn = {01621459},
   issue = {353},
   journal = {Journal of the American Statistical Association},
   keywords = {Arithmetic,Code sets,Data imputation,Education,Logical theorems,Marital status,Married status,Questionnaires,Spouses,Statistics},
   month = {3},
   pages = {17},
   publisher = {Taylor \& Francis, Ltd.American Statistical Association},
   title = {A Systematic Approach to Automatic Edit and Imputation},
   volume = {71},
   url = {http://www.jstor.org/stable/2285726?origin=crossref},
   year = {1976}
}
@book{deWaal2011,
   abstract = {A practical, one-stop reference on the theory and applications of statistical data editing and imputation techniquesCollected survey data are vulnerable to error. In particular, the data collection stage is a potential source of errors and missing values. As a result, the important role of statistical data editing, and the amount of resources involved, has motivated considerable research efforts to enhance the efficiency and effectiveness of this process. Handbook of Statistical Data Editing and Imputation equips readers with the essential statistical procedures for detecting and correcting inconsistencies and filling in missing values with estimates. The authors supply an easily accessible treatment of the existing methodology in this field, featuring an overview of common errors encountered in practice and techniques for resolving these issues.The book begins with an overview of methods and strategies for statistical data editing and imputation. Subsequent chapters provide detailed treatment of the central theoretical methods and modern applications, with topics of coverage including:Localization of errors in continuous data, with an outline of selective editing strategies, automatic editing for systematic and random errors, and other relevant state-of-the-art methodsExtensions of automatic editing to categorical data and integer dataThe basic framework for imputation, with a breakdown of key methods and models and a comparison of imputation with the weighting approach to correct for missing valuesMore advanced imputation methods, including imputation under edit restraintsThroughout the book, the treatment of each topic is presented in a uniform fashion. Following an introduction, each chapter presents the key theories and formulas underlying the topic and then illustrates common applications. The discussion concludes with a summary of the main concepts and a real-world example that incorporates realistic data along with professional insight into common challenges and best practices.Handbook of Statistical Data Editing and Imputation is an essential reference for survey researchers working in the fields of business, economics, government, and the social sciences who gather, analyze, and draw results from data. It is also a suitable supplement for courses on survey methods at the upper-undergraduate and graduate levels.},
   author = {Ton de Waal and Jeroen Pannekoek and Sander Scholtus and Ton de Waal and Jeroen Pannekoek and Sander Scholtus},
   doi = {10.1002/9780470904848},
   isbn = {9780470542804},
   journal = {Handbook of Statistical Data Editing and Imputation},
   pmid = {16238011},
   title = {Statistical Data Editing and Imputation},
   year = {2011}
}
@article{Scheuren2005,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
   author = {Fritz Scheuren},
   doi = {10.2307/27643702},
   issue = {4},
   journal = {The American Statistician},
   keywords = {Censuses,Data imputation,Economic surveys,Public use,Statistical discrepancies,Statistical estimation,Statistical models,Statistical variance,Statistics,Survey data},
   pages = {315-319},
   publisher = {Taylor \& Francis, Ltd.American Statistical Association},
   title = {Multiple Imputation: How It Began and Continues},
   volume = {59},
   url = {http://www.jstor.org/stable/27643702 http://www.jstor.org/stable/27643702?seq=1&cid=pdf-reference#references_tab_contents http://about.jstor.org/terms http://www.jstor.org.svproxy01.cimat.mx/stable/27643702},
   year = {2005}
}
@article{Tsikriktsis2005,
   abstract = {The treatment of missing data has been overlooked by the OM literature, while other fields such as marketing, organizational behavior, economics, statistics and psychometrics have paid more attention to the issue. A review of 103 survey-based articles published in the Journal of Operations Management between 1993 and 2001 shows that listwise deletion, which is often the least accurate technique of dealing with missing data, is heavily utilized by OM researchers. The paper also discusses the research implications of missing data, types of missing data and concludes with recommendations on which techniques should be used under different circumstances in order to improve the treatment of missing data in OM survey research.},
   author = {Nikos Tsikriktsis},
   doi = {10.1016/J.JOM.2005.03.001},
   issn = {0272-6963},
   issue = {1},
   journal = {Journal of Operations Management},
   month = {12},
   pages = {53-62},
   publisher = {Elsevier},
   title = {A review of techniques for treating missing data in OM survey research},
   volume = {24},
   url = {http://www.sciencedirect.com/science/article/pii/S027269630500077X?via%3Dihub},
   year = {2005}
}
@article{Yu2017,
   abstract = {Artificial intelligence (AI) is intrinsically data-driven. It calls for the application of statistical concepts through human-machine collaboration during generation of data, development of algorithms, and evaluation of results. This paper discusses how such human-machine collaboration can be approached through the statistical concepts of population, question of interest, representativeness of training data, and scrutiny of results (PQRS). The PQRS workflow provides a conceptual framework for integrating statistical ideas with human input into AI products and research. These ideas include experimental design principles of randomization and local control as well as the principle of stability to gain reproducibility and interpretability of algorithms and data results. We discuss the use of these principles in the contexts of self-driving cars, automated medical diagnoses, and examples from the authors' collaborative research.},
   author = {Bin Yu and Karl Kumbier},
   month = {12},
   title = {Artificial Intelligence and Statistics},
   url = {http://arxiv.org/abs/1712.03779},
   year = {2017}
}
@article{Batista2003,
   author = {Gustavo E. A. P. A. Batista and Maria Carolina Monard},
   doi = {10.1080/713827181},
   issn = {0883-9514},
   issue = {5-6},
   journal = {Applied Artificial Intelligence},
   month = {5},
   pages = {519-533},
   title = {An analysis of four missing data treatment methods for supervised learning},
   volume = {17},
   url = {http://www.tandfonline.com/doi/abs/10.1080/713827181},
   year = {2003}
}
@article{Jenghara2017,
   author = {Moslem Mohammadi Jenghara and Hossein Ebrahimpour-Komleh and Vahideh Rezaie and Samad Nejatian and Hamid Parvin and Sharifah Kamilah Syed Yusof},
   doi = {10.1007/s10115-017-1118-1},
   issn = {0219-1377},
   journal = {Knowledge and Information Systems},
   month = {10},
   pages = {1-17},
   publisher = {Springer London},
   title = {Imputing missing value through ensemble concept based on statistical measures},
   url = {http://link.springer.com/10.1007/s10115-017-1118-1},
   year = {2017}
}
@inbook{Murtagh2004,
   author = {Fionn Murtagh},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_1},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {3-14},
   publisher = {Springer Berlin Heidelberg},
   title = {Thinking Ultrametrically},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_1},
   year = {2004}
}
@book{,
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1},
   editor = {David Banks and Frederick R. McMorris and Phipps Arabie and Wolfgang Gaul},
   isbn = {978-3-540-22014-5},
   publisher = {Springer Berlin Heidelberg},
   title = {Classification, Clustering, and Data Mining Applications},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1},
   year = {2004}
}
@inbook{Gunoche2004,
   author = {Alain Guénoche},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_2},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {15-23},
   publisher = {Springer Berlin Heidelberg},
   title = {Clustering by Vertex Density in a Graph},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_2},
   year = {2004}
}
@inbook{Trejos2004,
   author = {Javier Trejos and Alex Murillo and Eduardo Piza},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_3},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {25-32},
   publisher = {Springer Berlin Heidelberg},
   title = {Clustering by Ant Colony Optimization},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_3},
   year = {2004}
}
@inbook{ATdeCarvalho2004,
   author = {Francisco A. T. de Carvalho and Yves Lechevallier and Renata M. C. R. Souza},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_4},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {33-41},
   publisher = {Springer Berlin Heidelberg},
   title = {A Dynamic Cluster Algorithm Based on L r Distances for Quantitative Data},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_4},
   year = {2004}
}
@inbook{Piron2004,
   author = {Jean-Yves Pirçon and Jean-Paul Rasson},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_5},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {43-51},
   publisher = {Springer Berlin Heidelberg},
   title = {The Last Step of a New Divisive Monothetic Clustering Method: the Gluing-Back Criterion},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_5},
   year = {2004}
}
@inbook{Steinley2004,
   author = {Douglas Steinley},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_6},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {53-60},
   publisher = {Springer Berlin Heidelberg},
   title = {Standardizing Variables in K-means Clustering},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_6},
   year = {2004}
}
@inbook{Golli2004,
   author = {Aïcha Golli and Brieuc Conan-Guez and Fabrice Rossi},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_7},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {61-68},
   publisher = {Springer Berlin Heidelberg},
   title = {A Self-Organizing Map for Dissimilarity Data},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_7},
   year = {2004}
}
@inbook{Nadif2004,
   author = {Mohamed Nadif and Gérard Govaert},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_8},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {69-76},
   publisher = {Springer Berlin Heidelberg},
   title = {Another Version of the Block EM Algorithm},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_8},
   year = {2004}
}
@inbook{Dias2004,
   author = {José G. Dias},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_9},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {77-84},
   publisher = {Springer Berlin Heidelberg},
   title = {Controlling the Level of Separation of Components in Monte Carlo Studies of Latent Class Models},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_9},
   year = {2004}
}
@inbook{Bachar2004,
   author = {Kaddour Bachar and Israël-César Lerman},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_10},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {85-94},
   publisher = {Springer Berlin Heidelberg},
   title = {Fixing Parameters in the Constrained Hierarchical Classification Method: Application to Digital Image Segmentation},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_10},
   year = {2004}
}
@inbook{Ramnath2004,
   author = {Sarnath Ramnath and Mynul H. Khan and Zubair Shams},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_11},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {95-103},
   publisher = {Springer Berlin Heidelberg},
   title = {New Approaches for Sum-of-Diameters Clustering},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_11},
   year = {2004}
}
@inbook{Diday2004,
   author = {Edwin Diday},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_12},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {105-120},
   publisher = {Springer Berlin Heidelberg},
   title = {Spatial Pyramidal Clustering Based on a Tessellation},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_12},
   year = {2004}
}
@inbook{Mizuta2004,
   author = {Masahiro Mizuta and Shintaro Hiro},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_13},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {123-139},
   publisher = {Springer Berlin Heidelberg},
   title = {Relative Projection Pursuit and its Application},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_13},
   year = {2004}
}
@inbook{SousaFerreira2004,
   author = {Ana Sousa Ferreira},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_15},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {151-156},
   publisher = {Springer Berlin Heidelberg},
   title = {Combining Models in Discrete Discriminant Analysis Through a Committee of Methods},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_15},
   year = {2004}
}
@inbook{Lee2004,
   author = {Herbert K. H. Lee},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_14},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {141-150},
   publisher = {Springer Berlin Heidelberg},
   title = {Priors for Neural Networks},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_14},
   year = {2004}
}
@inbook{Conan-Guez2004,
   author = {Brieuc Conan-Guez and Fabrice Rossi},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_16},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {157-165},
   publisher = {Springer Berlin Heidelberg},
   title = {Phoneme Discrimination with Functional Multi-Layer Perceptrons},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_16},
   year = {2004}
}
@inbook{Preda2004,
   author = {Cristian Preda and Gilbert Saporta},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_17},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {167-176},
   publisher = {Springer Berlin Heidelberg},
   title = {PLS Approach for Clusterwise Linear Regression on Functional Data},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_17},
   year = {2004}
}
@book{GesellschaftfurKlassifikationJahrestagung28th2004UniversityofDortmund2005,
   abstract = {"This volume contains revised versions of selected papers presented during the 28th Annual Conference of the Gesellschaft für Klassifikation (GfKI), the German Classification Society. The conference was held at the Universität Dortmund in Dortmund, Germany."--Preface, p. [v]. (Semi- ) Plenary Presentations -- Classification and Data Analysis: Classification; Data Analysis -- Applications -- Archaeology; Astronomy; Bio-Sciences; Electronic Data and Web; Finance and Insurance; Library Science and Linguistics; Macro-Economics; Marketing; Music Science; Quality Assurance -- Contest: Social Milieus in Dortmund.},
   author = {Gesellschaft für Klassifikation. Jahrestagung (28th : 2004 : University of Dortmund) and Claus. Weihs and W. (Wolfgang) Gaul},
   issn = {1431-8814},
   pages = {704},
   publisher = {Springer},
   title = {Classification, the ubiquitous challenge : proceedings of the 28th Annual Conference of the Gesellschaft für Klassifikation e.V., University of Dortmund, March 9-11, 2004},
   year = {2005}
}
@article{Farhangfar2008,
   abstract = {Numerous industrial and research databases include missing values. It is not uncommon to encounter databases that have up to a half of the entries missing, making it very difficult to mine them using data analysis methods that can work only with complete data. A common way of dealing with this problem is to impute (fill-in) the missing values. This paper evaluates how the choice of different imputation methods affects the performance of classifiers that are subsequently used with the imputed data. The experiments here focus on discrete data. This paper studies the effect of missing data imputation using five single imputation methods (a mean method, a Hot deck method, a Naı¨ve-Bayes method, and the latter two methods with a recently proposed imputation framework) and one multiple imputation method (a polytomous regression based method) on classification accuracy for six popular classifiers (RIPPER, C4.5, K-nearest-neighbor, support vector machine with polynomial and RBF kernels, and Naı¨ve-Bayes) on 15 datasets. This experimental study shows that imputation with the tested methods on average improves classification accuracy when compared to classification without imputation. Although the results show that there is no universally best imputation method, Naı¨ve-Bayes imputation is shown to give the best results for the RIPPER classifier for datasets with high amount (i.e., 40% and 50%) of missing data, polytomous regression imputation is shown to be the best for support vector machine classifier with polynomial kernel, and the application of the imputation framework is shown to be superior for the support vector machine with RBF kernel and K-nearest-neighbor. The analysis of the quality of the imputation with respect to varying amounts of missing data (i.e., between 5% and 50%) shows that all imputation methods, except for the mean imputation, improve classification error for data with more than 10% of missing data. Finally, some classifiers such as C4.5 and Naı¨ve-Bayes were found to be missing data resistant, i.e., they can produce accurate classification in the presence of missing data, while other classifiers such as K-nearest-neighbor, SVMs and RIPPER benefit from the imputation.},
   author = {Alireza Farhangfar and Lukasz Kurgan and Jennifer Dy},
   issue = {12},
   journal = {Pattern Recognition},
   month = {12},
   pages = {3692-3705},
   publisher = {Pergamon},
   title = {Impact of imputation of missing values on classification error for discrete data},
   volume = {41},
   url = {http://linkinghub.elsevier.com/retrieve/pii/S003132030800201X http://www.sciencedirect.com/science/article/pii/S003132030800201X},
   year = {2008}
}
@book{Pyle1999,
   abstract = {This book focuses on the importance of clean, well-structured data as the first step to successful data mining. It shows how data should be prepared prior to mining in order to maximize mining performance. Preface -- Introduction -- Chapter 1. Data Exploration As a Process -- Chapter 2. The Nature of the World and Its Impact on Data Preparation -- Chapter 3. Data Preparation as a Process -- Chapter 4. Getting the Data: Basic Preparation -- Chapter 5. Sampling, Variability and Confidence -- Chapter 6. Handling Non-Numerical Variables -- Chapter 7. Normalizing and Redistributing Variables -- Chapter 8. Replacing Missing and Empty Values -- Chapter 9. Series Variables -- Chapter 10. Preparing the Data Set -- Chapter 11. The Data Survey -- Chapter 12. Using Prepared Data -- Appendix A. Using the Demonstration Code on the CD -- Appendix B. Further Reading -- Index.},
   author = {Dorian Pyle},
   pages = {540},
   publisher = {Morgan Kaufmann Publishers},
   title = {Data preparation for data mining},
   year = {1999}
}
@book{MRBertholdNCebronFDillTRGabrielTKoetterTMeinlPOhlCSiebKThiel2007,
   author = {B. Wiswedel M. R. Berthold, N. Cebron, F. Dill, T. R. Gabriel, T. Koetter, T. Meinl, P. Ohl, C. Sieb, K. Thiel},
   isbn = {9783540220145},
   title = {Studies in Classification, Data Analysis, and Knowledge Organization},
   year = {2007}
}
@inbook{Acua2004,
   author = {Edgar Acuña and Caroline Rodriguez},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-17103-1_60},
   booktitle = {Classification, Clustering, and Data Mining Applications},
   pages = {639-647},
   publisher = {Springer Berlin Heidelberg},
   title = {The Treatment of Missing Values and its Effect on Classifier Accuracy},
   url = {http://link.springer.com/10.1007/978-3-642-17103-1_60},
   year = {2004}
}
@article{Hruschka2007,
   author = {Estevam R. Hruschka and Eduardo R. Hruschka and Nelson F. F. Ebecken},
   doi = {10.1007/s10844-006-0016-x},
   issn = {0925-9902},
   issue = {3},
   journal = {Journal of Intelligent Information Systems},
   month = {11},
   pages = {231-252},
   publisher = {Springer US},
   title = {Bayesian networks for imputation in classification problems},
   volume = {29},
   url = {http://link.springer.com/10.1007/s10844-006-0016-x},
   year = {2007}
}
@inbook{Li2004,
   author = {Dan Li and Jitender Deogun and William Spaulding and Bill Shuart},
   doi = {10.1007/978-3-540-25929-9_70},
   pages = {573-579},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Towards Missing Data Imputation: A Study of Fuzzy K-means Clustering Method},
   url = {http://link.springer.com/10.1007/978-3-540-25929-9_70},
   year = {2004}
}
@article{Qin2011,
   author = {Biao Qin and Yuni Xia and Sunil Prabhakar},
   doi = {10.1007/s10115-010-0335-7},
   issn = {0219-1377},
   issue = {1},
   journal = {Knowledge and Information Systems},
   month = {10},
   pages = {103-130},
   publisher = {Springer-Verlag},
   title = {Rule induction for uncertain data},
   volume = {29},
   url = {http://link.springer.com/10.1007/s10115-010-0335-7},
   year = {2011}
}
@book{Zuur2009,
   abstract = {Building on the successful 'Analysing Ecological Data', the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. Limitations of linear regression applied on ecological data -- Things are not always linear : additive modeling -- Dealing with heterogeneity -- Mixed effects modeling for nested data -- Violation of independence. Part 1 -- Violation of independence. Part 2 -- Meet the exponential family -- GLM and GAM for count data -- GLM and GAM for absence-presence and proportional data -- Zero-truncated and zero-inflated models for count data -- Generalised estimation equations -- GLMM and GAMM -- Estimating trends for Antarctic birds in relation to climate change / A.F. Zuur [and others] -- Large scale impacts of land-use change in a Scottish farming catchment / A.F. Zuur [and others] -- Negative binomial GAM and GAMM to analyse amphibian roadkills / A.F. Zuur [and others] -- Additive mixed modelling applied on deep-sea pelagic bioluminescent organisms / A.F. Zuur [and others] -- Additive mixed modelling applied on phytoplankton time series data / A.F. Zuur [and others] -- Mixed effects modelling applied on American foulbrood affecting honey bees larvae / A.F. Zuur [and others] -- Three-way nested data for age determination techniques applied to cetaceans / E.N. Ieno [and others] -- GLMM applied on the spatial distribution of koalas in a fragmented landscape / J.R. Rhodes [and others] -- A comparison of GLM, GEE, and GLMM applied to badger activity data / N.J. Walker [and others] -- Incorporating temporal correlation in seal abundance data with MCMC / A.A. Saveliev [and others] -- Required pre-knowledge : a linear regression and additive modelling example.},
   author = {Alain F. Zuur},
   issn = {1431-8776},
   pages = {574},
   publisher = {Springer},
   title = {Mixed effects models and extensions in ecology with R},
   year = {2009}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_1},
   pages = {1-6},
   title = {Introduction},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_1},
   year = {2012}
}
@book{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   city = {Boston, MA},
   doi = {10.1007/978-1-4614-1353-0},
   isbn = {978-1-4614-1352-3},
   publisher = {Springer US},
   title = {Regression Methods in Biostatistics},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_2},
   pages = {7-26},
   title = {Exploratory and Descriptive Methods},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_2},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_4},
   pages = {69-138},
   title = {Linear Regression},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_4},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_3},
   pages = {27-67},
   title = {Basic Statistical Methods},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_3},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_5},
   pages = {139-202},
   title = {Logistic Regression},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_5},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_8},
   pages = {309-330},
   title = {Generalized Linear Models},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_8},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_6},
   pages = {203-259},
   title = {Survival Analysis},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_6},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_12},
   pages = {469-480},
   title = {Complex Surveys},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_12},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_11},
   pages = {431-467},
   title = {Missing Data},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_11},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_9},
   pages = {331-394},
   title = {Strengthening Causal Inference},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_9},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_10},
   pages = {395-429},
   title = {Predictor Selection},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_10},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_7},
   pages = {261-308},
   title = {Repeated Measures and Longitudinal Data Analysis},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_7},
   year = {2012}
}
@inbook{Vittinghoff2012,
   author = {Eric Vittinghoff and David V. Glidden and Stephen C. Shiboski and Charles E. McCulloch},
   doi = {10.1007/978-1-4614-1353-0_13},
   pages = {481-488},
   title = {Summary},
   url = {http://link.springer.com/10.1007/978-1-4614-1353-0_13},
   year = {2012}
}
@book{Zuur2009,
   abstract = {Building on the successful 'Analysing Ecological Data', the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. Limitations of linear regression applied on ecological data -- Things are not always linear : additive modeling -- Dealing with heterogeneity -- Mixed effects modeling for nested data -- Violation of independence. Part 1 -- Violation of independence. Part 2 -- Meet the exponential family -- GLM and GAM for count data -- GLM and GAM for absence-presence and proportional data -- Zero-truncated and zero-inflated models for count data -- Generalised estimation equations -- GLMM and GAMM -- Estimating trends for Antarctic birds in relation to climate change / A.F. Zuur [and others] -- Large scale impacts of land-use change in a Scottish farming catchment / A.F. Zuur [and others] -- Negative binomial GAM and GAMM to analyse amphibian roadkills / A.F. Zuur [and others] -- Additive mixed modelling applied on deep-sea pelagic bioluminescent organisms / A.F. Zuur [and others] -- Additive mixed modelling applied on phytoplankton time series data / A.F. Zuur [and others] -- Mixed effects modelling applied on American foulbrood affecting honey bees larvae / A.F. Zuur [and others] -- Three-way nested data for age determination techniques applied to cetaceans / E.N. Ieno [and others] -- GLMM applied on the spatial distribution of koalas in a fragmented landscape / J.R. Rhodes [and others] -- A comparison of GLM, GEE, and GLMM applied to badger activity data / N.J. Walker [and others] -- Incorporating temporal correlation in seal abundance data with MCMC / A.A. Saveliev [and others] -- Required pre-knowledge : a linear regression and additive modelling example.},
   author = {Alain F. Zuur},
   issn = {1431-8776},
   pages = {574},
   publisher = {Springer},
   title = {Mixed effects models and extensions in ecology with R},
   year = {2009}
}
@article{,
   abstract = {In this paper, we present a missing data imputation method based on one of the most popular techniques in Knowledge Discovery in Databases (KDD), i.e. clustering technique. We combine the cluster-ing method with soft computing, which tends to be more tolerant of imprecision and uncertainty, and apply a fuzzy clustering algorithm to deal with incomplete data. Our experiments show that the fuzzy impu-tation algorithm presents better performance than the basic clustering algorithm.},
   author = {Dan Li and Jitender Deogun and William Spaulding and Bill Shuart},
   title = {Towards Missing Data Imputation: A Study of Fuzzy K-means Clustering Method},
   url = {http://sci2s.ugr.es/sites/default/files/files/TematicWebSites/MVDM/deogun_spaulding04.pdf}
}
@article{MandelJ2015,
   abstract = {Missing data are part of almost all research and introduce an element of ambiguity into data analysis. It follows that we need to consider them appropriately in order to provide an efficient and valid analysis. In the present study, we compare 6 different imputation methods: Mean, K-nearest neighbors (KNN), fuzzy K-means (FKM), singular value decomposition (SVD), bayesian principal component analysis (bPCA) and multiple imputations by chained equations (MICE). Comparison was performed on four real datasets of various sizes (from 4 to 65 variables), under a missing completely at random (MCAR) assumption, and based on four evaluation criteria: Root mean squared error (RMSE), unsupervised classification error (UCE), supervised classification error (SCE) and execution time. Our results suggest that bPCA and FKM are two imputation methods of interest which deserve further consideration in practice.},
   author = {Schmitt P Mandel J and M and el J and Guedj M},
   doi = {10.4172/2155-6180.1000224},
   issn = {21556180},
   issue = {01},
   journal = {Journal of Biometrics \& Biostatistics},
   month = {5},
   publisher = {OMICS International},
   title = {A Comparison of Six Methods for Missing Data Imputation},
   volume = {06},
   url = {https://www.omicsonline.org/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.php?aid=54590},
   year = {2015}
}
@inbook{Hunt2017,
   abstract = {Multivariate data sets frequently have missing observations scattered throughout the data set. Many machine learning algorithms assume that there is no particular significance in the fact that a particular observation has an attribute value missing. A common approach in coping with these missing values is to replace the missing value using some plausible value, and the resulting completed data set is analysed using standard methods. We evaluate the effect that some commonly used imputation methods have on the accuracy of classifiers in supervised leaning. The effect is assessed in simulations performed on several classical datasets where observations have been made missing at random in different proportions. Our analysis finds that missing data imputation using hot deck, iterative robust model-based imputation (IRMI), factorial analysis for mixed data (FAMD) and Random Forest Imputation (MissForest) perform in a similar manner regardless of the amount of missing data and have the highest mean percentage of observations correctly classified. Other methods investigated did not perform as well.},
   author = {Lynette A Hunt},
   city = {Cham},
   doi = {10.1007/978-3-319-55723-6_1},
   editor = {Francesco Palumbo and Angela Montanari and Maurizio Vichi},
   isbn = {978-3-319-55723-6},
   booktitle = {Data Science : Innovative Developments in Data Analysis and Clustering},
   pages = {3-14},
   publisher = {Springer International Publishing},
   title = {Missing Data Imputation and Its Effect on the Accuracy of Classification},
   url = {https://doi.org/10.1007/978-3-319-55723-6_1},
   year = {2017}
}
@article{Bauer2013,
   author = {Johannes Bauer and Orazio Angelini and Alexander Denev},
   issue = {1},
   keywords = {empirical orthogonal functions,london,markit,matrix interpolation,multiple imputation,multiple singular spectral analysis,multivariate times series,united kingdom},
   pages = {1-5},
   title = {Imputation of multivariate time series data - performance benchmarks for multiple imputation and spectral techniques},
   volume = {XXI},
   year = {2013}
}
@article{Blend2008,
   abstract = {Missing and incomplete information in surveys or databases can be imputed using different statistical and soft-computing techniques. This paper comprehensively compares auto-associative neural networks (NN), neuro-fuzzy (NF) systems and the hybrid combinations the above methods with hot-deck imputation. The tests are conducted on an eight category antenatal survey and also under principal component analysis (PCA) conditions. The neural network outperforms the neuro-fuzzy system for all tests by an average of 5.8%, while the hybrid method is on average 15.9% more accurate yet 50% less computationally efficient than the NN or NF systems acting alone. The global impact assessment of the imputed data is performed by several statistical tests. It is found that although the imputed accuracy is high, the global effect of the imputed data causes the PCA inter-relationships between the dataset to become altered. The standard deviation of the imputed dataset is on average 36.7% lower than the actual dataset which may cause an incorrect interpretation of the results.},
   author = {Darren Blend and Tshilidzi Marwala},
   pages = {7},
   title = {Comparison of Data Imputation Techniques and their Impact},
   url = {http://arxiv.org/abs/0812.1539},
   year = {2008}
}
@article{Sullivan2015,
   abstract = {Multiple imputation (MI) is increasingly being used to handle missing data in epidemiologic research. When data on both the exposure and the outcome are missing, an alternative to standard MI is the "multiple imputation, then deletion" (MID) method, which involves deleting imputed outcomes prior to analysis. While MID has been shown to provide efficiency gains over standard MI when analysis and imputation models are the same, the performance of MID in the presence of auxiliary variables for the incomplete outcome is not well understood. Using simulated data, we evaluated the performance of standard MI and MID in regression settings where data were missing on both the outcome and the exposure and where an auxiliary variable associated with the incomplete outcome was included in the imputation model. When the auxiliary variable was unrelated to missingness in the outcome, both standard MI and MID produced negligible bias when estimating regression parameters, with standard MI being more efficient in most settings. However, when the auxiliary variable was also associated with missingness in the outcome, alarmingly MID produced markedly biased parameter estimates. On the basis of these results, we recommend that researchers use standard MI rather than MID in the presence of auxiliary variables associated with an incomplete outcome.},
   author = {Thomas R. Sullivan and Amy B. Salter and Philip Ryan and Katherine J. Lee},
   doi = {10.1093/aje/kwv100},
   isbn = {1476-6256 (Electronic)\r0002-9262 (Linking)},
   issn = {14766256},
   issue = {6},
   journal = {American Journal of Epidemiology},
   keywords = {Auxiliary variables,epidemiologic methods,missing data,multiple imputation,simulation},
   pages = {528-534},
   pmid = {26337075},
   title = {Bias and Precision of the "multiple Imputation, Then Deletion" Method for Dealing with Missing Outcome Data},
   volume = {182},
   year = {2015}
}
@book{Alpaydin2014,
   abstract = {The machine learning field, which can be briefly defined as enabling computers make successful predictions using past experiences, has exhibited an impressive development recently with the help of the rapid increase in the storage capacity and processing power of computers. Together with many other disciplines, machine learning methods have been widely employed in bioinformatics. The difficulties and cost of biological analyses have led to the development of sophisticated machine learning approaches for this application area. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning and types of classification. Then, we point out the main issues of designing machine learning experiments and their performance evaluation. Finally, we introduce some supervised learning methods.},
   author = {Ethem. Alpaydin},
   doi = {10.1007/978-1-62703-748-8_7},
   isbn = {9780262012430},
   issn = {1940-6029},
   journal = {Methods in molecular biology (Clifton, N.J.)},
   pages = {105-28},
   pmid = {24272434},
   publisher = {MIT Press},
   title = {Introduction to Machine Learning Second Edition},
   volume = {1107},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/24272434},
   year = {2014}
}
@book{Simon2013,
   abstract = {"This book is a clear and lucid presentation of Evolutionary Algorithms, with a straightforward, bottom-up approach that provides the reader with a firm grasp of the basic principles of EAs. Covering the theory, history, mathematics, and applications of evolutionary optimization algorithms, this timely and practical book offers lengthy examples, a companion website, MATLAB code, and a Solutions Manual--making it perfect for advanced undergraduates, graduates, and practicing engineers involved in engineering and computer science"-- "Provides a straightforward, bottom-up approach that assists the reader in obtaining a clear (but theoretically rigorous) understanding of Evolutionary Algorithms, with an emphasis on implementation rather than models"--},
   author = {Dan Simon},
   isbn = {9780470937419},
   publisher = {John Wiley \& Sons Inc},
   title = {Evolutionary optimization algorithms : biologically-Inspired and population-based approaches to computer intelligence},
   url = {https://www.wiley.com/en-us/Evolutionary+Optimization+Algorithms-p-9780470937419},
   year = {2013}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_1},
   pages = {1-70},
   title = {Mathematical Preliminaries},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_1},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_4},
   pages = {177-189},
   title = {Physiological Interpretation of SOM},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_4},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_2},
   pages = {71-104},
   title = {Neural Modeling},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_2},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_3},
   pages = {105-176},
   title = {The Basic SOM},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_3},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_5},
   pages = {191-243},
   title = {Variants of SOM},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_5},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_6},
   pages = {245-261},
   title = {Learning Vector Quantization},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_6},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_7},
   pages = {263-310},
   title = {Applications},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_7},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_9},
   pages = {329-345},
   title = {Hardware for SOM},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_9},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_8},
   pages = {311-328},
   title = {Software Tools for SOM},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_8},
   year = {2001}
}
@inbook{Kohonen2001,
   author = {Teuvo Kohonen},
   doi = {10.1007/978-3-642-56927-2_10},
   pages = {347-371},
   title = {An Overview of SOM Literature},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2_10},
   year = {2001}
}
@book{Kohonen2001,
   author = {Teuvo Kohonen},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-56927-2},
   isbn = {978-3-540-67921-9},
   publisher = {Springer Berlin Heidelberg},
   title = {Self-Organizing Maps},
   volume = {30},
   url = {http://link.springer.com/10.1007/978-3-642-56927-2},
   year = {2001}
}
@book{Kohonen2001,
   abstract = {Third edition. The Self-Organizing Map (SOM), with its variants, is the most popular artificial neural network algorithm in the unsupervised learning category. About 4000 research articles on it have appeared in the open literature, and many industrial projects use the SOM as a tool for solving hard real-world problems. Many fields of science have adopted the SOM as a standard analytical tool: in statistics, signal processing, control theory, financial analyses, experimental physics, chemistry and medicine. The SOM solves difficult high-dimensional and nonlinear problems such as feature extraction and classification of images and acoustic patterns, adaptive control of robots, and equalization, demodulation, and error-tolerant transmission of signals in telecommunications. A new area is organization of very large document collections. Last but not least, it may be mentioned that the SOM is one of the most realistic models of the biological brain function. This new edition includes a survey of over 2000 contemporary studies to cover the newest results; case examples were provided with detailed formulae, illustrations, and tables; a new chapter on Software Tools for SOM was written, other chapters were extended or reorganized. Mathematical Preliminaries -- Neural Modeling -- The Basic SOM -- Physiological Interpretation of SOM -- Variants of SOM -- Learning Vector Quantization -- Applications -- Software Tools for SOM -- Hardware for SOM -- An Overview of SOM Literature -- Glossary of "Neural" Terms -- References.},
   author = {Teuvo. Kohonen},
   isbn = {9783642569272},
   pages = {501},
   publisher = {Springer Berlin Heidelberg},
   title = {Self-Organizing Maps},
   year = {2001}
}
@book{Schroeder1997,
   abstract = {3rd ed. 1. Introduction -- 2. The Natural Numbers -- 3. Primes -- 4. The Prime Distribution -- 5. Fractions: Continued, Egyptian and Farey -- 6. Linear Congruences -- 7. Diophantine Equations -- 8. The Theorems of Fermat, Wilson and Euler -- 9. Euler Trap Doors and Public-Key Encryption -- 10. The Divisor Functions -- 11. The Prime Divisor Functions -- 12. Certified Signatures -- 13. Primitive Roots -- 14. Knapsack Encryption -- 15. Quadratic Residues -- 16. The Chinese Remainder Theorem and Simultaneous Congruences -- 17. Fast Transformation and Kronecker Products -- 18. Quadratic Congruences -- 19. Pseudoprimes, Poker and Remote Coin Tossing -- 20. The Mobius Function and the Mobius Transform -- 21. Generating Functions and Partitions -- 22. Cyclotomic Polynomials -- 23. Linear Systems and Polynomials -- 24. Polynomial Theory -- 25. Galois Fields -- 26. Spectral Properties of Galois Sequences -- 27. Random Number Generators -- 28. Waveforms and Radiation Patterns -- 29. Number Theory, Randomness and "Art" -- 30. Self-Similarity, Fractals, Deterministic Chaos and a New State of Matter.},
   author = {M. R. (Manfred Robert) Schroeder},
   issn = {0720-678X},
   pages = {362},
   publisher = {Springer},
   title = {Number theory in science and communication : with applications in cryptography, physics, digital information, computing, and self-similarity},
   year = {1997}
}
@book{Kohonen2001,
   abstract = {3rd ed. 1. Mathematical Preliminaries -- 2. Neural Modeling -- 3. The Basic SOM -- 4. Physiological Interpretation of SOM -- 5. Variants of SOM -- 6. Learning Vector Quantization -- 7. Applications -- 8. Software Tools for SOM -- 9. Hardware for SOM -- 10. An Overview of SOM Literature -- 11. Glossary of "Neural" Terms.},
   author = {Teuvo. Kohonen},
   isbn = {9783540679219},
   pages = {501},
   publisher = {Springer},
   title = {Self-organizing maps},
   year = {2001}
}
@article{Fessant2002,
   author = {Fran&#x000E7;oise Fessant and Sophie Midenet},
   doi = {10.1007/s005210200002},
   issn = {0941-0643},
   issue = {4},
   journal = {Neural Computing \& Applications},
   month = {4},
   pages = {300-310},
   publisher = {Springer-Verlag London Limited},
   title = {Self-Organising Map for Data Imputation and Correction in Surveys},
   volume = {10},
   url = {http://link.springer.com/10.1007/s005210200002},
   year = {2002}
}
@article{Salzberg1994,
   author = {Steven L. Salzberg},
   doi = {10.1007/BF00993309},
   issn = {0885-6125},
   issue = {3},
   journal = {Machine Learning},
   month = {9},
   pages = {235-240},
   publisher = {Kluwer Academic Publishers},
   title = {C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},
   volume = {16},
   url = {http://link.springer.com/10.1007/BF00993309},
   year = {1994}
}
@article{Cooper1992,
   author = {Gregory F. Cooper and Edward Herskovits},
   doi = {10.1007/BF00994110},
   issn = {0885-6125},
   issue = {4},
   journal = {Machine Learning},
   month = {10},
   pages = {309-347},
   publisher = {Kluwer Academic Publishers},
   title = {A Bayesian method for the induction of probabilistic networks from data},
   volume = {9},
   url = {http://link.springer.com/10.1007/BF00994110},
   year = {1992}
}
@article{Chen2012,
   abstract = {Bayesian networks (BNs) are increasingly being used to model environmental systems, in order to: integrate multiple issues and system components; utilise information from different sources; and handle missing data and uncertainty. BNs also have a modular architecture that facilitates iterative model development. For a model to be of value in generating and sharing knowledge or providing decision support, it must be built using good modelling practice. This paper provides guidelines to developing and evaluating Bayesian network models of environmental systems, and presents a case study habitat suitability model for juvenile Astacopsis gouldi, the giant freshwater crayfish of Tasmania. The guidelines entail clearly defining the model objectives and scope, and using a conceptual model of the system to form the structure of the BN, which should be parsimonious yet capture all key components and processes. After the states and conditional probabilities of all variables are defined, the BN should be assessed by a suite of quantitative and qualitative forms of model evaluation. All the assumptions, uncertainties, descriptions and reasoning for each node and linkage, data and information sources, and evaluation results must be clearly documented. Following these standards will enable the modelling process and the model itself to be transparent, credible and robust, within its given limitations.},
   author = {Serena H. Chen and Carmel A. Pollino},
   doi = {10.1016/J.ENVSOFT.2012.03.012},
   issn = {1364-8152},
   journal = {Environmental Modelling \& Software},
   month = {11},
   pages = {134-145},
   publisher = {Elsevier},
   title = {Good practice in Bayesian network modelling},
   volume = {37},
   url = {https://www.sciencedirect.com/science/article/pii/S1364815212001041},
   year = {2012}
}
@article{Bae2016,
   abstract = {Bayesian networks are probabilistic models that represent complex distributions in a modular way and have become very popular in many fields. There are many methods to build Bayesian networks from a random sample of independent and identically distributed observations. However, many observational studies are designed using some form of clustered sampling that introduces correlations between observations within the same cluster and ignoring this correlation typically inflates the rate of false positive associations. We describe a novel parameterization of Bayesian networks that uses random effects to model the correlation within sample units and can be used for structure and parameter learning from correlated data without inflating the Type I error rate. We compare different learning metrics using simulations and illustrate the method in two real examples: an analysis of genetic and non-genetic factors associated with human longevity from a family-based study, and an example of risk factors for complications of sickle cell anemia from a longitudinal study with repeated measures.},
   author = {Harold Bae and Stefano Monti and Monty Montano and Martin H. Steinberg and Thomas T. Perls and Paola Sebastiani},
   journal = {Scientific Reports},
   title = {Learning Bayesian Networks from Correlated Data},
   year = {2016}
}
@article{Dong2013,
   abstract = {The impact of missing data on quantitative research can be serious, leading to biased estimates of parameters, loss of information, decreased statistical power, increased standard errors, and weakened generalizability of findings. In this paper, we discussed and demonstrated three principled missing data methods: multiple imputation, full information maximum likelihood, and expectation-maximization algorithm, applied to a real-world data set. Results were contrasted with those obtained from the complete data set and from the listwise deletion method. The relative merits of each method are noted, along with common features they share. The paper concludes with an emphasis on the importance of statistical assumptions, and recommendations for researchers. Quality of research will be enhanced if (a) researchers explicitly acknowledge missing data problems and the conditions under which they occurred, (b) principled methods are employed to handle missing data, and (c) the appropriate treatment of missing data is incorporated into review standards of manuscripts submitted for publication.},
   author = {Yiran Dong and Chao Ying Joanne Peng},
   journal = {SpringerPlus},
   title = {Principled missing data methods for researchers},
   year = {2013}
}
@article{265420385007,
   abstract = {El análisis de regresión es una técnica estadística que busca deducir el patrón de una serie de datos o investigar la relación estadística entre una variable dependiente (Y) y una o más variables independientes, el resultado es una expresión algebraica del tipo Y=F(X1, X2, ¿Xn). En este artículo se trabaja con el tipo de análisis de regresión más usual: la regresión lineal que tiene una variable independiente Y=F(X). El usuario común tiene contacto con la regresión lineal al usar las hojas electrónicas que implementen la deducción de líneas de tendencia dada una serie de datos. Sin embargo, se percatará que existen varios límites en esta técnica, por ejemplo, los datos tienen comportamientos sinusoidales o siguen un comportamiento de alguna función algebraica o combinación de funciones algebraicas por fuera del menú ofrecido: lineal, polinomial, potencial, logarítmica o exponencial. La regresión simbólica (una aplicación de la Programación Genética) tiene el mismo objetivo de la regresión lineal pero con un espectro mucho mayor de búsqueda y menos limitaciones: Dados los datos, buscará el patrón (expresión algebraica) que identifique el comportamiento de estos accediendo a todo tipo de funciones y combinaciones algebraicas. Regression analysis is a statistical analysis that aims to deduct the pattern in a series of data or research the statistical relation between a dependent variable (Y) and one or more dependent variables, the result is an algebraic expression type Y=F (X1, X2, ¿Xn). This article has the most common regression analysis: lineal regression which has one independent variable Y=F(X). A common user comes into contact with lineal regression when using electronic sheets that implement tendency line deduction given a series of data. However, he/she will notice there are certain limits to this technique for example, the data has sinusoidal behavior or follows some algebraic function behavior or a combination of algebraic functions beyond the offered menu: lineal, polynomial, potential, logarithmic or exponential. Symbolic regression (a genetic programming application) has the same objective as lineal regression but with a much greater search spectrum and much less limitations: Given the data, it will search for the pattern (algebraic expression) that identifies their behavior ascending to all types of functions and algebraic combinations.},
   author = {Rafael Alberto M.},
   issn = {1900-3803},
   journal = {Entramado},
   keywords = {Genetic programming,Programación genética,análisis de regresión,artificial evolution,artificial intelligence,computación evolutiva.,evolución artificial,evolutionary computation.,inteligencia artificial,regresión simbólica,regression analysis,symbolic regression},
   pages = {76-85},
   title = {Programación genética: La regresión simbólica},
   volume = {3},
   url = {http://www.redalyc.org/articulo.oa?id=265420385007},
   year = {2007}
}
@article{Zhang2016,
   abstract = {Multiple imputation (MI) is an advanced technique for handing missing values. It is superior to single imputation in that it takes into account uncertainty in missing value imputation. However, MI is underutilized in medical literature due to lack of familiarity and computational challenges. The article provides a step-by-step approach to perform MI by using R multivariate imputation by chained equation (MICE) package. The procedure firstly imputed m sets of complete dataset by calling mice() function. Then statistical analysis such as univariate analysis and regression model can be performed within each dataset by calling with() function. This function sets the environment for statistical analysis. Lastly, the results obtained from each analysis are combined by using pool() function.},
   author = {Zhongheng Zhang},
   doi = {10.3978/j.issn.2305-5839.2015.12.63},
   isbn = {2305-5839 (Print)2305-5839 (Linking)},
   issn = {2305-5839},
   journal = {Annals of translational medicine},
   keywords = {10,12,15,2015,2305-5839,3978,63,accepted for publication dec,big-data clinical trial,doi,equation,imputed complete dataset,issn,j,mi,mice,multiple imputation,multivariate imputation by chained,package,r,submitted nov 05},
   pmid = {26889483},
   title = {Multiple imputation with multivariate imputation by chained equation (MICE) package.},
   year = {2016}
}
@article{Shah2014,
   abstract = {Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The "true" imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001-2010) with complete data on all covariates. Variables were artificially made "missing at random," and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.},
   author = {Anoop D. Shah and Jonathan W. Bartlett and James Carpenter and Owen Nicholas and Harry Hemingway},
   doi = {10.1093/aje/kwt312},
   isbn = {1476-6256 (Electronic)\r0002-9262 (Linking)},
   issn = {14766256},
   journal = {American Journal of Epidemiology},
   keywords = {angina stable,imputation,missing data,missingness at random,regression trees,simulation,survival},
   pmid = {24589914},
   title = {Comparison of random forest and parametric imputation models for imputing missing data using MICE: A CALIBER study},
   year = {2014}
}
@article{Lee2014,
   abstract = {Missing data are common in both observational and experimental studies. Multiple imputation (MI) is a two-stage approach where missing values are imputed a number of times using a statistical model based on the available data and then inference is combined across the completed datasets. This approach is becoming increasingly popular for handling missing data. In this paper, we introduce the method of MI, as well as a discussion surrounding when MI can be a useful method for handling missing data and the drawbacks of this approach. We illustrate MI when exploring the association between current asthma status and forced expiratory volume in 1 s after adjustment for potential confounders using data from a population-based longitudinal cohort study.},
   author = {Katherine J. Lee and Julie A. Simpson},
   doi = {10.1111/resp.12226},
   isbn = {1440-1843},
   issn = {13237799},
   journal = {Respirology},
   keywords = {experimental study,missing data,multiple imputation,observational study},
   pmid = {24372814},
   title = {Introduction to multiple imputation for dealing with missing data},
   year = {2014}
}
@article{Lee2017,
   abstract = {Multiple imputation (MI) is becoming increasingly popular for handling missing data. Standard approaches for MI assume normality for continuous variables (conditionally on the other variables in the imputation model). However, it is unclear how to impute non-normally distributed continuous variables. Using simulation and a case study, we compared various transformations applied prior to imputation, including a novel non-parametric transformation, to imputation on the raw scale and using predictive mean matching (PMM) when imputing non-normal data. We generated data from a range of non-normal distributions, and set 50% to missing completely at random or missing at random. We then imputed missing values on the raw scale, following a zero-skewness log, Box-Cox or non-parametric transformation and using PMM with both type 1 and 2 matching. We compared inferences regarding the marginal mean of the incomplete variable and the association with a fully observed outcome. We also compared results from these approaches in the analysis of depression and anxiety symptoms in parents of very preterm compared with term-born infants. The results provide novel empirical evidence that the decision regarding how to impute a non-normal variable should be based on the nature of the relationship between the variables of interest. If the relationship is linear in the untransformed scale, transformation can introduce bias irrespective of the transformation used. However, if the relationship is non-linear, it may be important to transform the variable to accurately capture this relationship. A useful alternative is to impute the variable using PMM with type 1 matching. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.},
   author = {Katherine J. Lee and John B. Carlin},
   doi = {10.1002/sim.7173},
   isbn = {0277-6715},
   issn = {10970258},
   issue = {4},
   journal = {Statistics in Medicine},
   pmid = {27862164},
   title = {Multiple imputation in the presence of non-normal data},
   volume = {36},
   year = {2017}
}
@article{Aydilek2013,
   abstract = {Missing values in datasets should be extracted from the datasets or should be estimated before they are used for classification, association rules or clustering in the preprocessing stage of data mining. In this study, we utilize a fuzzy c-means clustering hybrid approach that combines support vector regression and a genetic algorithm. In this method, the fuzzy clustering parameters, cluster size and weighting factor are optimized and missing values are estimated. The proposed novel hybrid method yields sufficient and sensible imputation performance results. The results are compared with those of fuzzy c-means genetic algorithm imputation, support vector regression genetic algorithm imputation and zero imputation. © 2013 Elsevier Inc. All rights reserved.},
   author = {Ibrahim Berkan Aydilek and Ahmet Arslan},
   doi = {10.1016/j.ins.2013.01.021},
   isbn = {00200255},
   issn = {00200255},
   journal = {Information Sciences},
   keywords = {Fuzzy c-means,Imputation,Missing data,Missing values,Support vector regression},
   pages = {25-35},
   title = {A hybrid method for imputation of missing values using optimized fuzzy c-means with support vector regression and a genetic algorithm},
   volume = {233},
   url = {http://dx.doi.org/10.1016/j.ins.2013.01.021},
   year = {2013}
}
@article{Silva-Ramrez2011,
   abstract = {Data mining is based on data files which usually contain errors in the form of missing values. This paper focuses on a methodological framework for the development of an automated data imputation model based on artificial neural networks. Fifteen real and simulated data sets are exposed to a perturbation experiment, based on the random generation of missing values. These data set sizes range from 47 to 1389 records. A perturbation experiment was performed for each data set where the probability of missing value was set to 0.05. Several architectures and learning algorithms for the multilayer perceptron are tested and compared with three classic imputation procedures: mean/mode imputation, regression and hot-deck. The obtained results, considering different performance measures, not only suggest this approach improves the quality of a database with missing values, but also the best results are clearly obtained using the Multilayer Perceptron model in data sets with categorical variables. Three learning rules (Levenberg-Marquardt, BFGS Quasi-Newton and Conjugate Gradient Fletcher-Reeves Update) and a small number of hidden nodes are recommended. © 2010 Elsevier Ltd.},
   author = {Esther Lydia Silva-Ramírez and Rafael Pino-Mejías and Manuel López-Coello and María Dolores Cubiles-de-la-Vega},
   doi = {10.1016/j.neunet.2010.09.008},
   isbn = {0893-6080},
   issn = {08936080},
   issue = {1},
   journal = {Neural Networks},
   keywords = {Hot-deck model,Imputation,Mean/mode model,Missing data,Multilayer perceptron,Regression model},
   pages = {121-129},
   pmid = {20875726},
   publisher = {Elsevier Ltd},
   title = {Missing value imputation on missing completely at random data using multilayer perceptrons},
   volume = {24},
   url = {http://dx.doi.org/10.1016/j.neunet.2010.09.008},
   year = {2011}
}
@article{Rubright2014,
   abstract = {When exploring missing data techniques in a realistic scenario, the current literature is limited: most studies only consider consequences with data missing on a single variable. This simulation study compares the relative bias of two commonly used missing data techniques when data are missing on more than one variable. Factors varied include type of missingness (MCAR, MAR), degree of missingness (10%, 25%, and 50%), and where missingness occurs (one predictor, two predictors, or two predictors with overlap). Using a real dataset, cells are systematically deleted to create various scenarios of missingness so that parameter estimates from listwise deletion and multiple imputation may be compared to the “true” estimates from the full dataset. Results suggest the multiple imputation works well, even when the imputation model itself is missing data.},
   author = {Jonathan D. Rubright and Ratna Nandakumar and Joseph J. Glutting},
   doi = {http://dx.doi.org/10.472/2155-6180.1000224},
   issn = {15317714},
   journal = {Practical Assessment, Research \& Evaluation},
   title = {A Simulation Study of Missing Data with Multiple Missing X’s},
   url = {http://pareonline.net/getvn.asp?v=19&n=10},
   year = {2014}
}
@article{Ariga2016,
   abstract = {Deep generative models such as generative adversarial networks (GANs) and vari-ational auto-encoders (VAEs) are attracting high attention these days, but their theory and their relation to simpler generative models are not well studied. In this work, we first reformulate the well-known statistical method, PCA, as a genera-tive latent Gaussian variable model and derive EM algorithm as a corresponding inference method. Then, we compare those model-inference pair of the Proba-bilistic PCA (PPCA) to VAEs which consists of a " deep " generative latent Gaus-sian model and variational inference as the inference method. The comparison is done by fitting both models to the MNIST hand-written database and completing hidden parts of digits by maximum likelihood estimation. VAEs reconstruct more realistic digits, and we conclude that the improvement comes from the represen-tational power of the multi-layer neural network and the robustness to overfitting of variational inference which are used in VAEs.},
   author = {Kousuke Ariga},
   title = {Generative models for missing value completion},
   url = {https://homes.cs.washington.edu/~koar8470/docs/paper/2016-generative_models.pdf},
   year = {2016}
}
@article{,
   abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed genera-tive models, endowed with a new algorithm for scalable inference and learning. Our algo-rithm introduces a recognition model to rep-resent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic back-propagation – rules for gradient backpropa-gation through stochastic variables – and de-rive an algorithm that allows for joint optimi-sation of the parameters of both the genera-tive and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
   author = {Danilo J Rezende and Shakir Mohamed and Daan Wierstra and Google DeepMind},
   title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
   url = {https://arxiv.org/pdf/1401.4082.pdf}
}
@article{Brakel2013,
   abstract = {Imputing missing values in high dimensional time-series is a difficult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difficul-ties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strat-egy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-field iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data it-self during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artificial and two real-world data sets.},
   author = {Philémon Brakel and Philemon Brakel@ugent Be and Dirk Stroobandt and Dirk Stroobandt@ugent Be and Benjamin Schrauwen and Benjamin Schrauwen@ugent Be},
   journal = {Journal of Machine Learning Research},
   keywords = {energy-based models,missing values,neural networks,optimization,time-series},
   pages = {2771-2797},
   title = {Training Energy-Based Models for Time-Series Imputation},
   volume = {14},
   url = {http://www.jmlr.org/papers/volume14/brakel13a/brakel13a.pdf},
   year = {2013}
}
@inproceedings{Smaragdis2009,
   author = {Paris Smaragdis and Bhiksha Raj and Madhusudana Shashanka},
   doi = {10.1109/MLSP.2009.5306194},
   isbn = {978-1-4244-4947-7},
   booktitle = {2009 IEEE International Workshop on Machine Learning for Signal Processing},
   month = {9},
   pages = {1-6},
   publisher = {IEEE},
   title = {Missing data imputation for spectral audio signals},
   url = {http://ieeexplore.ieee.org/document/5306194/},
   year = {2009}
}
@article{Smaragdis2011,
   abstract = {With the recent attention towards audio processing in the time-frequency domain we increas-ingly encounter the problem of missing data within that representation. In this paper we present an ap-proach that allows us to recover missing values in the time-frequency domain of audio signals. The presented approach is able to deal with real-world polyphonic signals by operating seamlessly even in the presence of complex acoustic mixtures. We demonstrate that this approach outperforms generic missing data ap-proaches, and we present a variety of situations that highlight its utility.},
   author = {Paris Smaragdis and Bhiksha Raj and Madhusudana Shashanka},
   doi = {10.1007/s11265-010-0512-7},
   journal = {J Sign Process Syst},
   keywords = {Audio ·,Imputation,Missing data ·,Time-frequency ·},
   pages = {361-370},
   title = {Missing Data Imputation for Time-Frequency Representations of Audio Signals},
   volume = {65},
   url = {https://link.springer.com/content/pdf/10.1007%2Fs11265-010-0512-7.pdf},
   year = {2011}
}
@article{Li2011,
   abstract = {Discriminative training for machine transla-tion has been well studied in the recent past. A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. We present an unsupervised discriminative train-ing framework to incorporate the usually plen-tiful target-language monolingual data by us-ing a rough " reverse " translation system. Intu-itively, our method strives to ensure that prob-abilistic " round-trip " translation from a target-language sentence to the source-language and back will have low expected loss. Theoret-ically, this may be justified as (discrimina-tively) minimizing an imputed empirical risk. Empirically, we demonstrate that augment-ing supervised training with unsupervised data improves translation performance over the su-pervised case for both IWSLT and NIST tasks.},
   author = {Zhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason Eisner and Brian Roark},
   pages = {920-929},
   title = {Minimum Imputed Risk: Unsupervised Discriminative Training for Machine Translation},
   url = {http://delivery.acm.org/10.1145/2150000/2145533/p920-li.pdf?ip=177.234.11.86&id=2145533&acc=OPEN&key=6F4CCF05E2930152%2E3A406A232738A87B%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1528164293_244e3c51bc528c3fac069ab60efe1d7c},
   year = {2011}
}
@article{Smaragdis2009,
   abstract = {With the recent attention to audio processing in the time - frequency domain we increasingly encounter the problem of missing data. In this paper we present an approach that allows for imputing missing values in the time-frequency domain of audio signals. The presented approach is able to deal with real-world polyphonic signals by performing imputation even in the presence of complex mixtures. We show that this ap- proach outperforms generic imputation approaches, and we present a variety of situations that highlight its utility.},
   author = {Paris Smaragdis and Bhiksha Raj and Madhusudana Shashanka},
   isbn = {9781424449484},
   title = {Missing Data Imputation for Spectral Audio Signals},
   year = {2009}
}
@article{Vellido2006,
   abstract = {The Generative Topographic Mapping (GTM) was originally conceived as a probabilistic alternative to the well-known, neural network-inspired, Self-Organizing Maps. The GTM can also be interpreted as a constrained mixture of distribution models. In recent years, much attention has been directed towards Student t-distributions as an alternative to Gaussians in mixture models due to their robustness towards outliers. In this paper, the GTM is redefined as a constrained mixture of t-distributions: the t-GTM, and the Expectation–Maximization algorithm that is used to fit the model to the data is modified to carry out missing data imputation. Several experiments show that the t-GTM successfully detects outliers, while minimizing their impact on the estimation of the model parameters. It is also shown that the t-GTM provides an overall more accurate imputation of missing values than the standard Gaussian GTM.},
   author = {Alfredo Vellido},
   doi = {10.1016/j.neunet.2005.11.003},
   journal = {Neural Networks},
   keywords = {Data visualization,Generative topographic mapping,Missing data,Outliers,Robust imputation,Student multivariate t-distributions},
   pages = {1624-1635},
   title = {Missing data imputation through GTM as a mixture of t-distributions},
   volume = {19},
   url = {www.elsevier.com/locate/neunet},
   year = {2006}
}
@article{,
   abstract = {Synonyms Generative models Definition A language model assigns a probability to a piece of unseen text, based on some training data. For example, a language model based on a big English newspaper archive is expected to assign a higher probability to ''a bit of text'' than to ''aw pit tov tags,'' because the words in the former phrase (or word pairs or word triples if so-called N-Gram Models are used) occur more fre-quently in the data than the words in the latter phrase. For information retrieval, typical usage is to build a language model for each document. At search time, the top ranked document is the one whose language model assigns the highest probability to the query. Historical Background The term language models originates from probabilistic models of language generation developed for automa-tic speech recognition systems in the early 1980s [9]. Speech recognition systems use a language model to complement the results of the acoustic model which models the relation between words (or parts of words called phonemes) and the acoustic signal. The history of language models, however, goes back to the begin-ning of the twentieth century when Andrei Markov used language models (Markov models) to model letter sequences in works of Russian literature [3]. Another famous application of language models are Claude Shannon's models of letter sequences and word sequences, which he used to illustrate the implications of coding and information theory [17]. In the 1990s, language models were applied as a general tool for several natural language processing applica-tions, such as part-of-speech tagging, machine transla-tion, and optical character recognition. Language models were applied to information retrieval by a number of research groups in the late 1990s [4,7,14,15]. They became rapidly popular in informa-tion retrieval research. By 2001, the ACM SIGIR con-ference had two separate sessions on language models containing five papers in total [12]. In 2003, a group of leading information retrieval researchers published a research roadmap ''challenges in information retrieval and language modeling'' [1], indicating that the future of information retrieval and the future of language modeling can not be seen separate from each other. Foundations},
   author = {Djoerd Hiemstra},
   title = {Language Models},
   url = {https://link-springer-com.svproxy01.cimat.mx/content/pdf/10.1007%2F978-0-387-39940-9_923.pdf}
}
@inbook{Wang2017,
   author = {Hongning Wang and ChengXiang Zhai},
   doi = {10.1007/978-3-319-55394-8_6},
   pages = {107-134},
   publisher = {Springer, Cham},
   title = {Generative Models for Sentiment Analysis and Opinion Mining},
   url = {http://link.springer.com/10.1007/978-3-319-55394-8_6},
   year = {2017}
}
@article{,
   author = {Erik Cambria and Dipankar Das and Sivaji Bandyopadhyay and Antonio Feraco Editors},
   title = {Socio-Affective Computing 5 A Practical Guide to Sentiment Analysis},
   url = {https://link-springer-com.svproxy01.cimat.mx/content/pdf/10.1007%2F978-3-319-55394-8.pdf}
}
@article{,
   abstract = {Supervised deep learning has been successfully applied to many recognition prob-lems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model com-plex structured output representations that effectively perform probabilistic infer-ence and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward infer-ence. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed al-gorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic in-ference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
   author = {Kihyuk Sohn and Xinchen Yan and Honglak Lee},
   title = {Learning Structured Output Representation using Deep Conditional Generative Models},
   url = {https://pdfs.semanticscholar.org/3f25/e17eb717e5894e0404ea634451332f85d287.pdf}
}
@book{PMurphy1991,
   abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
   author = {Kevin P. Murphy},
   doi = {10.1007/SpringerReference_35834},
   isbn = {9780262018029},
   issn = {0262018020},
   journal = {Machine Learning: A Probabilistic Perspective},
   pages = {73-78,216-244},
   pmid = {20236947},
   title = {Machine Learning: A Probabilistic Perspective},
   url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0_2},
   year = {1991}
}
@article{,
   abstract = {Background: Multiple imputation (MI) was developed as a method to enable valid inferences to be obtained in the presence of missing data rather than to re-create the missing values. Within the applied setting, it remains unclear how important it is that imputed values should be plausible for individual observations. One variable type for which MI may lead to implausible values is a limited-range variable, where imputed values may fall outside the observable range. The aim of this work was to compare methods for imputing limited-range variables, with a focus on those that restrict the range of the imputed values. Methods: Using data from a study of adolescent health, we consider three variables based on responses to the General Health Questionnaire (GHQ), a tool for detecting minor psychiatric illness. These variables, based on different scoring methods for the GHQ, resulted in three continuous distributions with mild, moderate and severe positive skewness. In an otherwise complete dataset, we set 33% of the GHQ observations to missing completely at random or missing at random; repeating this process to create 1000 datasets with incomplete data for each scenario. For each dataset, we imputed values on the raw scale and following a zero-skewness log transformation using: univariate regression with no rounding; post-imputation rounding; truncated normal regression; and predictive mean matching. We estimated the marginal mean of the GHQ and the association between the GHQ and a fully observed binary outcome, comparing the results with complete data statistics. Results: Imputation with no rounding performed well when applied to data on the raw scale. Post-imputation rounding and imputation using truncated normal regression produced higher marginal means than the complete data estimate when data had a moderate or severe skew, and this was associated with under-coverage of the complete data estimate. Predictive mean matching also produced under-coverage of the complete data estimate. For the estimate of association, all methods produced similar estimates to the complete data. Conclusions: For data with a limited range, multiple imputation using techniques that restrict the range of imputed values can result in biased estimates for the marginal mean when data are highly skewed.},
   author = {Laura Rodwell and Katherine J Lee and Helena Romaniuk and John B Carlin},
   doi = {10.1186/1471-2288-14-57},
   keywords = {Limited-range,Missing data,Multiple imputation,Rounding,Skewed data,Truncated regression},
   title = {Comparison of methods for imputing limited-range variables: a simulation study},
   url = {https://bmcmedresmethodol.biomedcentral.com/track/pdf/10.1186/1471-2288-14-57}
}
@article{,
   abstract = {Graphs are fundamental data structures which con-cisely capture the relational structure in many im-portant real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learn-ing generative models over arbitrary graphs, and opens new directions for moving away from re-strictions of vector-and sequence-like knowledge representations, toward more expressive and flexi-ble relational data structures.},
   author = {Yujia Li and Oriol Vinyals and Chris Dyer and Razvan Pascanu and Peter Battaglia},
   title = {Learning Deep Generative Models of Graphs},
   url = {https://arxiv.org/pdf/1803.03324.pdf}
}
@article{,
   abstract = {We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary con-tribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih & Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.},
   author = {Chris J Maddison and Daniel Tarlow},
   title = {Structured Generative Models of Natural Source Code},
   url = {http://proceedings.mlr.press/v32/maddison14.pdf}
}
@article{Hinton1997,
   abstract = {We describe a hierarchical, generative model that can be viewed as a non-linear gener-alization of factor analysis and can be implemented in a neural network. The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference cor-rectly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demon-strate that the network learns to extract sparse, distributed, hierarchical representations.},
   author = {Geoorey E Hinton and Zoubin Ghahramani},
   journal = {Philosophical Transactions of the Royal Society B},
   title = {Generative Models for Discovering Sparse Distributed Representations},
   url = {http://mlg.eng.cam.ac.uk/zoubin/papers/RGBN.pdf},
   year = {1997}
}
@article{,
   abstract = {Supervised deep learning has been successfully applied to many recognition prob-lems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model com-plex structured output representations that effectively perform probabilistic infer-ence and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward infer-ence. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed al-gorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic in-ference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
   author = {Kihyuk Sohn and Xinchen Yan and Honglak Lee},
   title = {Learning Structured Output Representation using Deep Conditional Generative Models},
   url = {https://pdfs.semanticscholar.org/3f25/e17eb717e5894e0404ea634451332f85d287.pdf}
}
@article{,
   abstract = {Probabilistic generative models can be used for compression, denoising, inpaint-ing, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often dif-ficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most com-monly used criteria—average log-likelihood, Parzen window estimates, and vi-sual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
   author = {Lucas Theis and Aäron Van Den Oord and Matthias Bethge},
   title = {A NOTE ON THE EVALUATION OF GENERATIVE MODELS},
   url = {https://arxiv.org/pdf/1511.01844.pdf}
}
@article{,
   abstract = {For many applications of machine learning the goal is to predict the value of a vector c given the value of a vector x of input features. In a classification problem c represents a discrete class label, whereas in a regression problem it corresponds to one or more continuous variables. From a probabilistic perspective, the goal is to find the conditional distribution p(c|x). The most common approach to this problem is to represent the conditional distribution using a parametric model, and then to determine the parameters using a training set consisting of pairs \{xn, cn\} of input vectors along with their corresponding target output vectors. The resulting conditional distribution can be used to make predictions of c for new values of x. This is known as a discriminative approach, since the conditional distribution discriminates directly between the different values of c. An alternative approach is to find the joint distribution p(x, c), expressed for instance as a parametric model, and then subsequently uses this joint distribution to evaluate the conditional p(c|x) in order to make predictions of c for new values of x. This is known as a generative approach since by sampling from the joint distribution it is possible to generate synthetic examples of the feature vector x. In practice, the generalization performance of generative models is often found to be poorer than than of discriminative models due to differences between the model and the true distribution of the data. When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, although collection of data is often easy, the process of labelling it can be expensive. Consequently there is increasing interest in generative methods since these can exploit unlabelled data in addition to labelled data. Although the generalization performance of generative models can often be improved by 'training them discriminatively', they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. Julia Lasserre is funded by the Microsoft Research European PhD Scholarship programme. 4 Christopher M. Bishop and Julia Lasserre Here we discuss a new perspective which says that there is only one correct way to train a given model, and that a 'discriminatively trained' generative model is fundamentally a new model (Minka, 2006). From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of 'dis-criminative training', this approach opens the door to very general ways of interpolating between generative and discriminative extremes through alter-native choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely gen-erative and the purely discriminative. We conclude by discussing how to use a Bayesian approach to find automatically the appropriate trade-off between the generative and discriminative extremes.},
   author = {J M Bernardo and M J Bayarri and J O Berger and A P Dawid and D Heckerman and A F M Smith and M West and Christopher M Bishop and Julia Lasserre},
   keywords = {Bayesian inference,Generative,and Phrases,discriminative,machine learning,semi-supervized,unlabelled data},
   title = {Generative or Discriminative? Getting the Best of Both Worlds},
   url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-Valencia-07.pdf}
}
@article{Fraley2002,
   author = {Chris Fraley and Adrian E Raftery},
   issue = {458},
   journal = {Journal of the American Statistical Association Jun},
   title = {Model-based clustering, discriminant analysis, and density estimation},
   volume = {97},
   url = {https://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf},
   year = {2002}
}
@article{,
   abstract = {Here we consider the relationship between supervised learning, or function ap-proximation problems, and Bayesian reasoning. We begin by considering how to design learning algorithms based on Bayes rule. Consider a supervised learning problem in which we wish to approximate an unknown target function f : X → Y , or equivalently P(Y |X). To begin, we will assume Y is a boolean-valued random variable, and X is a vector containing n boolean attributes. In other words, X = X 1 , X 2 . . . , X n , where X i is the boolean random variable denoting the ith attribute of X. Applying Bayes rule, we see that P(Y = y i |X) can be represented as P(Y = y i |X = x k) = P(X = x k |Y = y i)P(Y = y i)},
   title = {Learning Classifiers based on Bayes Rule},
   url = {www.cs.cmu.edu/∼tom/mlbook.html.}
}
@article{Jebara1996,
   abstract = {I propose a common framework that combines three different paradigms in machine learning: gen-erative, discriminative and imitative learning. A generative probabilistic distribution is a principled way to model many machine learning and machine perception problems. Therein, one provides do-main specific knowledge in terms of structure and parameter priors over the joint space of variables. Bayesian networks and Bayesian statistics provide a rich and flexible language for specifying this knowledge and subsequently refining it with data and observations. The final result is a distribution that is a good generator of novel exemplars. Conversely, discriminative algorithms adjust a possibly non-distributional model to data optimizing for a specific task, such as classification or prediction. This typically leads to superior performance yet compromises the flexibility of generative modeling. I present Maximum Entropy Discrimination (MED) as a framework to combine both discriminative estimation and generative probability den-sities. Calculations involve distributions over parameters, margins, and priors and are provably and uniquely solvable for the exponential family. Extensions include regression, feature selection, and transduction. SVMs are also naturally subsumed and can be augmented with, for example, feature selection, to obtain substantial improvements.},
   author = {Tony Jebara},
   journal = {M.Sc., Media Arts and Sciences},
   title = {Discriminative, Generative and Imitative Learning},
   url = {http://www.cs.columbia.edu/~jebara/papers/jebara4.pdf},
   year = {1996}
}
@article{,
   abstract = {Generative probability models such as hidden ~larkov models pro-vide a principled way of treating missing information and dealing with variable length sequences. On the other hand , discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification per-formance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combina-tion by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability mod-els. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of D~A and protein sequence analysis.},
   author = {Tommi S Jaakkola and David Haussler},
   title = {Exploiting generative models discriminative classifiers},
   url = {https://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf}
}
@article{Tipping1999,
   author = {Michael E. Tipping and Christopher M. Bishop},
   doi = {10.1111/1467-9868.00196},
   issn = {1369-7412},
   issue = {3},
   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   keywords = {Density estimation,EM algorithm,Gaussian mixtures,Maximum likelihood,Principal component analysis,Probability model},
   month = {8},
   pages = {611-622},
   publisher = {Wiley/Blackwell (10.1111)},
   title = {Probabilistic Principal Component Analysis},
   volume = {61},
   url = {http://doi.wiley.com/10.1111/1467-9868.00196},
   year = {1999}
}
@article{Jordan1999,
   abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
   author = {Michael I Jordan and Tommi S Jaakkola and Lawrence K Saul},
   journal = {Machine Learning},
   keywords = {Bayesian networks,Boltzmann machines,approximate infer-ence,belief networks,graphical models,hidden Markov models,mean field methods,neural networks,probabilistic inference,variational methods},
   pages = {183-233},
   title = {An Introduction to Variational Methods for Graphical Models},
   volume = {37},
   url = {https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf},
   year = {1999}
}
@article{Wainwright2008,
   abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statisti-cal, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimiza-tion, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representa-tions of the problems of computing likelihoods, marginal probabili-ties and most probable configurations. We describe how a wide variety},
   author = {J Wainwright and M I Jordan and Martin J Wainwright and Michael I Jordan},
   doi = {10.1561/2200000001},
   journal = {Machine Learning},
   pages = {1-2},
   title = {Graphical Models, Exponential Families, and Variational Inference},
   volume = {1},
   url = {https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf},
   year = {2008}
}
@article{,
   abstract = {Generative model learning is one of the key problems in machine learning and computer vision. Currently the use of generative models is limited due to the difficulty in effec-tively learning them. A new learning framework is proposed in this paper which progressively learns a target genera-tive distribution through discriminative approaches. This framework provides many interesting aspects to the liter-ature. From the generative model side: (1) A reference distribution is used to assist the learning process, which removes the need for a sampling processes in the early stages. (2) The classification power of discriminative ap-proaches, e.g. boosting, is directly utilized. (3) The abil-ity to select/explore features from a large candidate pool allows us to make nearly no assumptions about the train-ing data. From the discriminative model side: (1) This framework improves the modeling capability of discrimina-tive models. (2) It can start with source training data only and gradually " invent " negative samples. (3) We show how sampling schemes can be introduced to discriminative mod-els. (4) The learning procedure helps to tighten the decision boundaries for classification, and therefore, improves ro-bustness. In this paper, we show a variety of applications including texture modeling and classification, non-photo-realistic rendering, learning image statistics/denoising, and face modeling. The framework handles both homogeneous patterns, e.g. textures, and inhomogeneous patterns, e.g. faces, with nearly an identical parameter setting for all the tasks in the learning stage.},
   author = {Zhuowen Tu},
   title = {Learning Generative Models via Discriminative Approaches},
   url = {http://pages.ucsd.edu/~ztu/publication/cvpr07_gdl.pdf}
}
@article{Salomn2014,
   author = {Edgar Salomón and García Treviño},
   title = {Structural Generative Descriptions for Temporal Data},
   url = {https://core.ac.uk/download/pdf/76990053.pdf},
   year = {2014}
}
@article{Hyvrinen2013,
   abstract = {We propose a machine learning framework to capture the dynamics of high- frequency limit order books in financial equity markets and automate real-time prediction of metrics such as mid-price movement and price spread crossing. By characterizing each entry in a limit order book with a vector of attributes such as price and volume at different levels, the proposed framework builds a learning model for each metric with the help of multi-class support vector machines (SVMs). Experiments with real data establish that features selected by the proposed framework are effective for short term price movement forecasts.},
   author = {a Hyvärinen and E Oja and Lijuan J Cao and Francis E.H. H Tay and Neil D Lawrence and Russell Jurney and Francis E.H. H Tay and Lijuan J Cao and Julia A. Lasserre and Christopher M. Bishop and Thomas P. Minka and Alec N Kercheval and Francis E.H. H Tay and Lijuan J Cao and Jens Keilwagen and Jan Grau and Stefan Posch and Marc Strickert and Ivo Grosse and Shian-Chang Huang and Nan-Yu Wang and Tzu-Ying Li and Yi-Ching Lee and Lung-Fu Chang and Tzu-Hui Pan and Alec N Kercheval and Subhabrata Choudhury and Subhajyoti Ghosh and Arnab Bhattacharya and Kiran Jude Fernandes and Manoj Kumar Tiwari and Hong Tang and Ali A Ghorbani and Marina Meila and Qiang Miao and Hong-Zhong Huang and Xianfeng Fan and Ajith Abraham and Emilio Corchado and Juan M. Corchado and Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei a Rusu and Joel Veness and Marc G Bellemare and Alex Graves and Martin Riedmiller and Andreas K Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis and R S Sutton and a G Barto and Francis E.H. H Tay and Lijuan J Cao and Alejandro Cañete and Jorge Constanzo and Luis Salinas and Michelangelo Diligenti and Marco Gori and Marco Maggini and Leonardo Rigutini and Machine Learning Spring and J. Melorose and R. Perroy and S. Careas},
   doi = {10.1080/14697688.2015.1032546},
   isbn = {1441990119},
   issn = {09252312},
   issue = {2},
   journal = {Neurocomputing},
   keywords = {60g35,60h07,93e10,93e14,Algorithms,Artifacts,Brain,Brain: physiology,Clustering,First-order logic,Humans,Kernel machines,Kernels,Learning from constraints,Learning with prior knowledge,Magnetoencephalography,Multi-task learning,Neural Networks (Computer),Normal Distribution,Online classification,Price pattern,Self-Organizing Maps,Semantic-based regularization,Stock market,Support vector machine,Technical analysis,Trading,Trading strategy,back propagation algorithm,chines,feature extraction,financial,function,generalisation,hidden markov model,high-frequency limit order book,icle,machine learning,mathematics subject classification,multi-class classifiers,multi-layer,non-stationary ¢nancial time series,non-stationary ÿnancial time series,pattern recognition,perceptron,regularized risk,structural risk minimization principle,support vector ma-,support vector machine,support vector machines,svms,time series forecasting,tube size},
   pages = {1-36},
   pmid = {25246403},
   title = {A real time clustering and SVM based price-volatility prediction for optimal trading strategy},
   volume = {1},
   url = {http://dx.doi.org/10.1038/nature14236%0Ahttp://dx.doi.org/10.1016/j.neucom.2009.02.017%0Ahttp://link.springer.com/10.1007/BF03026965%0Ahttp://link.springer.com/10.1007/BF02987011%0Ahttp://dx.doi.org/10.1016/j.neucom.2013.10.002%0Ahttp://www.tandfonline.co},
   year = {2013}
}
@article{Severson2017,
   author = {Kristen Severson and Mark Molaro and Richard Braatz},
   doi = {10.3390/pr5030038},
   issn = {2227-9717},
   issue = {3},
   journal = {Processes},
   keywords = {chemometrics,learning,machine,missing data,multivariable statistical process control,principal component analysis,process data analytics,process monitoring,tennessee eastman problem},
   pages = {38},
   title = {Principal Component Analysis of Process Datasets with Missing Values},
   volume = {5},
   url = {http://www.mdpi.com/2227-9717/5/3/38},
   year = {2017}
}
@article{Josse2011,
   abstract = {The available methods to handle missing values in principal component analysis only provide point estimates of the parameters (axes and components) and estimates of the missing values. To take into account the variability due to missing values a multiple imputation method is proposed. First a method to generate multiple imputed data sets from a principal component analysis model is defined. Then, two ways to visualize the uncertainty due to missing values onto the principal component analysis results are described. The first one consists in projecting the imputed data sets onto a reference configuration as supplementary elements to assess the stability of the individuals (respectively of the variables). The second one consists in performing a principal component analysis on each imputed data set and fitting each obtained con-figuration onto the reference one with Procrustes rotation. The latter strategy allows to assess the variability of the principal component analysis parameters induced by the missing values. The methodology is then evaluated from a real data set.},
   author = {Julie Josse and Jérôme Pagès and François Husson},
   doi = {10.1007/s11634-011-0086-7},
   journal = {Adv Data Anal Classif},
   keywords = {Bootstrap ·,Classification (2000) 62H25 · 62G09,Mathematics,Missing values · EM algorithm ·,Multiple imputation ·,Principal component analysis ·,Procrustes rotation,Subject},
   pages = {231-246},
   title = {Multiple imputation in principal component analysis},
   volume = {5},
   url = {https://link.springer.com/content/pdf/10.1007%2Fs11634-011-0086-7.pdf},
   year = {2011}
}
@article{,
   title = {full-text}
}
@book{,
   abstract = {Intro; Contents; Preface; 1. Does Data Science Need Statistics?; 1. Introduction; 2. Breiman's Two Cultures; 3. Data Science at GCHQ; 4. Data Mining End to End; 5. The Science in Data Science; 6. Two Challenges; References; 2. Principled Statistical Inference in Data Science; 1. Introduction; 2. Key Principles; 3. Classical and 'Post-selection' Inference; 4. Example: File Drawer Effect; 5. Borrowing from Classical Theory; 6. Example: Bivariate Regression; 7. Some Other Points; 8. Conclusion; Acknowledgement; References 3. Evaluating Statistical and Machine Learning Supervised Classification Methods1. Introduction; 2. Performance Measures; 2.1. Measures based on the classification table; 2.2. Choosing the classification threshold; 2.2.1. Misclassification rate; 2.2.2. Kolmogorov-Smirnov measure; 2.2.3. Given misclassification costs; 2.2.4. The F-measure; 2.3. Distributions of misclassification costs; 2.3.1. The H-measure; 2.3.2. Screening; 3. Conclusion; References; 4. Diversity as a Response to User Preference Uncertainty; 1. Introduction; 2. Problem Formulation; 3. Solution Method; 4. Simulation Study 4.1. Experimental results: CTR4.2. Experimental results: Diversity; 5. Discussion; Acknowledgements; References; 5. L-kernel Density Estimation for Bayesian Model Selection; 1. Introduction; 2. Markov Chain Monte Carlo; 2.1. Markov chains; 2.1.1. Transition kernel; 2.1.2. Convergence of Markov chains; 2.1.3. Ergodicity; 2.2. Metropolis-Hastings kernel; 2.2.1. Metropolis-Hastings algorithm; 2.2.2. Gibbs sampling algorithm; 2.3. LLN and the CLT for MCMC; 2.4. Sequential Monte Carlo samplers; 3. Density Estimation for Markov Chain Simulation; 3.1. Invariance of the Markov chain 3.2. L-kernel density estimation3.2.1. Choice of L-kernel; 3.2.2. Relationship to the existing literature; 4. Marginal Likelihood Estimation; 4.1. L-kernel estimation of marginal likelihood; 4.2. Mixture modelling; 4.2.1. Likelihood; 4.2.2. Prior distribution; 4.2.3. Label switching; 4.3. Inference; 4.4. Example: Galaxy data; 5. Discussion; Acknowledgements; References; 6. Bayesian Numerical Methods as a Case Study for Statistical Data Science; 1. The Rise of Data Science; 1.1. Data science: Does it matter, or is it all hype?; 1.2. Nightmares and opportunities for statisticians 2. Probabilistic Numerical Methods2.1. What is probabilistic numerics?; 2.2. The future of probabilistic numerics; 3. Conclusion; Acknowledgements; References; 7. Phylogenetic Gaussian Processes for Bat Echolocation; 1. Introduction; 2. Echolocation Calls as Function-Valued Traits; 3. Phylogenetic Gaussian Processes; 4. Results; 4.1. Data description; 4.2. Hyperparameter estimation and ancestral trait reconstruction with phylogenetic Gaussian processes; 5. Conclusions and Further Work; Acknowledgement; References},
   author = {Niall M. Adams and Ed (Statistician) Cohen},
   isbn = {9781786345394},
   title = {Statistical data science},
   url = {https://catalog.loc.gov/vwebv/search?searchCode=LCCN&searchArg=2018010673&searchType=1&permalink=y}
}
@article{Hastings1970,
   abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
   author = {W. K. Hastings},
   doi = {10.2307/2334940},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   month = {4},
   pages = {97},
   publisher = {Oxford University PressBiometrika Trust},
   title = {Monte Carlo Sampling Methods Using Markov Chains and Their Applications},
   volume = {57},
   url = {https://www.jstor.org/stable/2334940?origin=crossref},
   year = {1970}
}
@article{Metropolis1953,
   abstract = {Downloaded 23 Aug 2013 to 128.252.11.181. This article is copyrighted as indicated in the abstract. Reuse of AIP content is subject to the terms at: http://jcp.aip.org/about/rights_and_permissions THE 0 R Y 0 F T RAe KEF FEe T SIN R A D I 0 L Y SIS 0 F W ATE R 1087 instead, only water molecules with different amounts of excitation energy. These may follow any of three paths: (a) The excitation energy is lost without dissociation into radicals (by collision, or possibly radiation, as in aromatic hydrocarbons). (b) The molecules dissociate, but the resulting radi-cals recombine without escaping from the liquid cage. (c) The molecules dissociate and escape from the cage. In this case we would not expect them to move more than a few molecular diameters through the dense medium before being thermalized. In accordance with the notation introduced by Burton, Magee, and Samuel,22 the molecules following 22 Burton, Magee, and Samuel, J. Chern. Phys. 20, 760 (1952). THE JOURNAL OF CHEMICAL PHYSICS paths (a) and (b) can be designated H 2 0* and those following path (c) can be designated H 2 0t. It seems reasonable to assume for the purpose of these calcula-tions that the ionized H 2 0 molecules will become the H 20 t molecules, but this is not likely to be a complete correspondence. In conclusion we would like to emphasize that the qualitative result of this section is not critically de-pendent on the exact values of the physical parameters used. However, this treatment is classical, and a correct treatment must be wave mechanical; therefore the result of this section cannot be taken as an a priori theoretical prediction. The success of the radical diffu-sion model given above lends some plausibility to the occurrence of electron capture as described by this crude calculation. Further work is clearly needed. A general method, suitable for fast computing machines, for investigatiflg such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
   author = {Nicholas Metropolis and Arianna W Rosenbluth and Marshall N Rosenbluth and Augusta H Teller and Edward Teller},
   doi = {10.1063/1.1699114},
   issue = {6},
   journal = {J. Chem. Phys. J. Chem. Phys. Journal Homepage},
   title = {Equation of State Calculations by Fast Computing Machines},
   volume = {21},
   url = {http://dx.doi.org/10.1063/1.1699114 http://jcp.aip.org/resource/1/JCPSA6/v21/i6 http://jcp.aip.org/ http://jcp.aip.org/about/about_the_journal http://jcp.aip.org/features/most_downloaded http://jcp.aip.org/authors},
   year = {1953}
}
@book{Enders2010,
   abstract = {\{"Walking\} readers step by step through complex concepts, this book translates missing data techniques into something that applied researchers and graduate students can understand and utilize in their own research. Enders explains the rationale and procedural details for maximum likelihood estimation, Bayesian estimation, multiple imputation, and models for handling missing not at random (\{MNAR)\} data. Easy-to-follow examples and small simulated data sets illustrate the techniques and clarify the underlying principles. The companion website includes data files and syntax for the examples in the book as well as up-to-date information on software. The book is accessible to substantive researchers while providing a level of detail that will satisfy quantitative specialists"--Back cover.},
   author = {Craig K. Enders},
   doi = {10.1017/CBO9781107415324.004},
   isbn = {9781606236390},
   issn = {1098-6596},
   journal = {Library},
   pages = {401},
   pmid = {16153636},
   title = {Applied Missing Data Analysis},
   year = {2010}
}
@article{Allison2001,
   abstract = {Sooner or later anyone who does statistical analysis runs into problems with missing data in which information for some variables is missing for some cases. Why is this a problem? Because most statistical methods presume that every case has information on all the variables to be included in the analysis. Using numerous examples and practical tips, this book offers a nontechnical explanation of the standard methods for missing data (such as listwise or casewise deletion) as well as two newer (and, better) methods, maximum likelihood and multiple imputation. Anyone who has been relying on ad-hoc methods that are statistically inefficient or biased will find this book a welcome and accessible solution to their problems with handling missing data.},
   author = {Paul D Allison},
   doi = {10.1136/bmj.38977.682025.2C},
   isbn = {9780761916727},
   issn = {1468-5833},
   journal = {Quantitative Applications in the Social Sciences},
   pages = {104},
   pmid = {17322261},
   title = {Missing Data},
   url = {http://www.sagepub.com/booksProdDesc.nav?prodId=Book9419},
   year = {2001}
}
@article{Hotelling1933,
   author = {H. Hotelling},
   doi = {10.1037/h0071325},
   issn = {0022-0663},
   issue = {6},
   journal = {Journal of Educational Psychology},
   pages = {417-441},
   title = {Analysis of a complex of statistical variables into principal components.},
   volume = {24},
   url = {http://content.apa.org/journals/edu/24/6/417},
   year = {1933}
}
@book{Bishop2013,
   abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
   author = {Christopher M Bishop},
   doi = {10.1117/1.2819119},
   isbn = {978-0-387-31073-2},
   issn = {1098-6596},
   issue = {9},
   journal = {Journal of Chemical Information and Modeling},
   pages = {1689-1699},
   pmid = {25246403},
   title = {Pattern Recognition and Machine Learning},
   volume = {53},
   year = {2013}
}
@article{Donoho2015,
   abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In 'The Future of Data Analysis', he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or 'data analysis'. Ten to twenty years ago, John Chambers, Bill Cleveland and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland even suggested the catchy name "Data Science" for his envisioned field. A recent and growing phenomenon is the emergence of "Data Science" programs at major universities, including UC Berkeley, NYU, MIT, and most recently the Univ. of Michigan, which on September 8, 2015 announced a $100M "Data Science Initiative" that will hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; in general, though, the new initiatives steer away from close involvement with academic statistics departments. This paper reviews some ingredients of the current "Data Science moment", including recent commentary about data science in the popular media, and about how/whether Data Science is really different from Statistics. The now-contemplated field of Data Science amounts to a superset of the fields of statistics and machine learning which adds some technology for 'scaling up' to 'big data'. This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next fifty years. Because all of science itself will soon become data that can be mined, the imminent revolution in Data Science is not about mere 'scaling up', but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers and Breiman, I present a vision of data science based on the activities of people who are 'learning from data', and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s Data Science Initiatives, while being able to accommodate the same short-term goals.},
   author = {David Donoho},
   doi = {10.1080/10618600.2017.1384734},
   isbn = {0672326965},
   issn = {15372715},
   journal = {R Software},
   pages = {41},
   pmid = {25246403},
   title = {50 Years of Data Science},
   year = {2015}
}
@book{Kuehl2000,
   abstract = {2nd ed. Revised edition of: Statistical principles of research design and analysis. c1994. Research design principles -- Getting started with completely randomized designs -- Treatment comparisons -- Diagnosing agreement between the data and the model -- Experiments to study variances -- Factorial treatment designs -- Factorial treatment designs, random and mixed mdels -- Complete block designs -- Incomplete block designs, an introduction -- Incomplete block designs, resolvable and cyclic designs -- Incomplete block designs, factorial treatment designs -- Response surface designs -- Split plot designs -- Repeated measures designs -- Crossover designs -- Analysis of covariance.},
   author = {R. O. Kuehl and R. O. Kuehl},
   isbn = {0534368344},
   pages = {666},
   publisher = {Duxbury/Thomson Learning},
   title = {Design of experiments : statistical principles of research design and analysis},
   url = {https://books.google.com.mx/books/about/Design_of_Experiments.html?id=mIV2QgAACAAJ&redir_esc=y},
   year = {2000}
}
@book{Montgomery2009,
   abstract = {7th ed. 1. Introduction -- 2. Simple comparative experiments -- 3. Experiments with a single factor : the analysis of variance -- 4. Randomized blocks, Latin squares, and related designs -- 5. Introduction to factorial designs -- 6. 2[superscript k] factorial design -- 7. Blocking and confounding in the 2[superscript k] factorial design -- 8. Two-level fractional factorial designs -- 9. Three-level and mixed-level factorial and fractional factorial designs -- 10. Fitting regression models -- 11. Response surface methods and designs -- 12. Robust parameter design and process robustness studies -- 13. Experiments with random factors -- 14. Nested and split-plot designs -- 15. Other design and analysis topics.},
   author = {Douglas C. Montgomery},
   isbn = {0470128666},
   pages = {656},
   publisher = {Wiley},
   title = {Design and analysis of experiments},
   url = {https://books.google.com.mx/books/about/Design_and_Analysis_of_Experiments.html?id=kMMJAm5bD34C&redir_esc=y},
   year = {2009}
}
@book{Box2005,
   abstract = {2nd ed. The second edition of Statistics for Experimenters focuses on applications in the physical, engineering, biological, and social sciences. From the beginning, the book's source of ideas is the scientific method itself and the need of the investigator to make his or her research as effective as possible through proper choice and conduct of experiments and appropriate analysis of data. After a problem is stated, appropriate statistical methods of design and analysis are discussed. And frequently, examples are presented for which standard mathematical assumptions are wrong, thus forcing the reader's attention onto the essential precautions necessary in the conduct of the experiment to ensure valid conclusions. Catalyzing the generation of knowledge -- Basics (probability, parameters and statistics) -- Comparing two entities : relevant reference distributions, tests, and confidence intervals -- Comparing a number of entities, randomized blocks and Latin squares -- Factorial designs at two levels -- Fraction factorial designs -- Additional fractionals and analysis -- Factorial designs and data transformation -- Multiple sources of variation -- Least squares and why we need designed experiments -- Modeling, geometry, and experimental design -- Some applications of response surface methods -- Designing robust products and processes --Process control, forecasting, and times series : an introduction -- Evolutionary process operation.},
   author = {George E. P. Box and J. Stuart Hunter and William Gordon Hunter},
   isbn = {9780471718130},
   pages = {633},
   publisher = {Wiley-Interscience},
   title = {Statistics for experimenters : design, innovation, and discovery},
   url = {https://www.wiley.com/en-mx/Statistics+for+Experimenters:+Design,+Innovation,+and+Discovery,+2nd+Edition-p-9780471718130},
   year = {2005}
}
@book{DominguezDominguez2018,
   abstract = {1ª ed. Incluye clave de acceso para el contenido adicional en línea Índice},
   author = {Jorge Domínguez Domínguez and Eduardo Castaño Tostado},
   isbn = {8426725945},
   publisher = {Marcombo},
   title = {Diseño de experimentos : estrategias y análisis en ciencias e ingenierías},
   year = {2018}
}
@book{Castillo2007,
   author = {Enrique Del Castillo},
   city = {Boston, MA},
   doi = {10.1007/978-0-387-71435-6},
   isbn = {978-0-387-71434-9},
   publisher = {Springer US},
   title = {Process Optimization},
   volume = {105},
   url = {http://link.springer.com/10.1007/978-0-387-71435-6},
   year = {2007}
}
@book{Khuri2006,
   author = {André I Khuri},
   doi = {10.1142/5915},
   isbn = {978-981-256-458-0},
   month = {1},
   publisher = {WORLD SCIENTIFIC},
   title = {Response Surface Methodology and Related Topics},
   url = {https://www.worldscientific.com/worldscibooks/10.1142/5915},
   year = {2006}
}
@book{Wu2009,
   abstract = {2nd ed. "Experimentation is one of the most common activities in which all people engage. In this thoroughly updated Second Edition, Experiments presents the most modern, up-to-date treatment in the design and analysis of experiment topics currently available. The authors - highly recognized researchers in the field - introduce some of the newest discoveries and shed further light on existing ones. Drawing from their impressive roster of industrial clients, the authors modernize accepted methodologies while refining many cutting-edge topics in a single, easily accessible source suitable for upper-undergraduate or beginning-graduate students, practicing engineers, and statisticians."--Publisher's website. 1. Basic Concepts for Experimental Design and Introductory Regression Analysis -- 2. Experiments with a Single Factor -- 3. Experiments with More Than One Factor -- 4. Full Factorial Experiments at Two Levels -- 5. Fractional Factorial Experiments at Two Levels -- 6. Full Factorial and Fractional Factorial Experiments at Three Levels -- 7. Other Design and Analysis Techniques for Experiments at More Than Two Levels -- 8. Nonregular Designs : Construction and Properties -- 9. Experiments with Complex Aliasing -- 10. Response Surface Methodology -- 11. Introduction to Robust Parameter Design -- 12. Robust Parameter Design for Signal--Response Systems -- 13. Experiments for Improving Reliability -- 14. Analysis of Experiments with Nonnormal Data -- Appendix A : Upper Tail Probabilities of the Standard Normal Distribution, [actual symbol not reproducible] -- Appendix B : Upper Percentiles of the t Distribution -- Appendix C : Upper Percentiles of the x² Distribution -- Appendix D : Upper Percentiles of the F Distribution -- Appendix E : Upper Percentiles of the Studentized Range Distribution -- Appendix F : Upper Percentiles of the Studentized Maximum Modulus Distribution -- Appendix G : Coefficients of Orthogonal Contrast Vectors -- Appendix H : Critical Values for Lenth's Method.},
   author = {Chien-Fu. Wu and Michael Hamada},
   isbn = {9780471699460},
   pages = {716},
   publisher = {Wiley},
   title = {Experiments : planning, analysis, and optimization},
   url = {https://www.wiley.com/en-mx/Experiments:+Planning,+Analysis,+and+Optimization,+2nd+Edition-p-9780471699460},
   year = {2009}
}
@book{Myers2016,
   abstract = {Fourth edition Raymond H. Myers, Douglas C. Montgomery, Christine M. Anderson-Cook.},
   author = {Raymond H.; Anderson-Cook Myers},
   isbn = {9781118916018},
   publisher = {Wiley},
   title = {Response Surface Methodology : Process and Product Optimization Using Designed Experiments.},
   url = {https://www.wiley.com/en-mx/Response+Surface+Methodology:+Process+and+Product+Optimization+Using+Designed+Experiments,+4th+Edition-p-9781118916018},
   year = {2016}
}
@article{Uria2013,
   abstract = {The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.},
   author = {Benigno Uria and Iain Murray and Hugo Larochelle},
   month = {10},
   title = {A Deep and Tractable Density Estimator},
   url = {http://arxiv.org/abs/1310.1757},
   year = {2013}
}
@article{Uria2013,
   abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
   author = {Benigno Uria and Iain Murray and Hugo Larochelle},
   month = {6},
   title = {RNADE: The real-valued neural autoregressive density-estimator},
   url = {http://arxiv.org/abs/1306.0186},
   year = {2013}
}
@techReport{Larochelle2011,
   abstract = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function , which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivari-ate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.},
   author = {Hugo Larochelle and Iain Murray},
   title = {The Neural Autoregressive Distribution Estimator},
   url = {http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf},
   year = {2011}
}
@article{Lecun1998,
   author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
   doi = {10.1109/5.726791},
   issn = {00189219},
   issue = {11},
   journal = {Proceedings of the IEEE},
   pages = {2278-2324},
   title = {Gradient-based learning applied to document recognition},
   volume = {86},
   url = {http://ieeexplore.ieee.org/document/726791/},
   year = {1998}
}
@misc{Dua:2017,
   author = {Dua Dheeru and Efi Karra Taniskidou},
   institution = {University of California, Irvine, School of Information and Computer Sciences},
   title = {\{UCI\} Machine Learning Repository},
   url = {http://archive.ics.uci.edu/ml},
   year = {2017}
}
@article{Kingma2013,
   abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
   author = {Diederik P Kingma and Max Welling},
   month = {12},
   title = {Auto-Encoding Variational Bayes},
   url = {http://arxiv.org/abs/1312.6114},
   year = {2013}
}
@techReport{Williams2018,
   abstract = {Latent variable models can be used to probabilistically "fill-in" missing data entries. The variational autoencoder architecture (Kingma and Welling, 2014; Rezende et al., 2014) includes a "recognition" or "encoder" network that infers the latent variables given the data variables. However, it is not clear how to handle missing data variables in this network. The factor analysis (FA) model is a basic autoencoder, using linear encoder and decoder networks. We show how to calculate exactly the latent posterior distribution for the FA model in the presence of missing data, and note that this solution exhibits a non-trivial dependence on the pattern of missingness. We also discuss various approximations to the exact solution. Experiments compare the effectiveness of various approaches to imputing the missing data.},
   author = {Christopher K I Williams and Charlie Nash and Alfredo Nazábal},
   title = {Autoencoders and Probabilistic Inference with Missing Data: An Exact Solution for The Factor Analysis Case},
   url = {https://arxiv.org/pdf/1801.03851.pdf},
   year = {2018}
}
@article{Schafer2002,
   abstract = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
   author = {Joseph L Schafer and John W Graham},
   issn = {1082-989X},
   issue = {2},
   journal = {Psychological methods},
   month = {6},
   pages = {147-77},
   pmid = {12090408},
   title = {Missing data: our view of the state of the art.},
   volume = {7},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/12090408},
   year = {2002}
}
@techReport{,
   title = {Handbook of Missing Data},
   url = {http://www.stefvanbuuren.nl/publications/2014%20FCS%20-%20Chapter%2014%20HMD.pdf}
}
@book{Buuren2011,
   author = {Stef van Buuren},
   isbn = {9781498770163},
   title = {Multiple imputation in practice - Second course},
   year = {2011}
}
@article{Gulrajani2017,
   abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
   author = {Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
   month = {3},
   title = {Improved Training of Wasserstein GANs},
   url = {http://arxiv.org/abs/1704.00028},
   year = {2017}
}
@article{Arjovsky2017,
   abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
   author = {Martin Arjovsky and Soumith Chintala and Léon Bottou},
   month = {1},
   title = {Wasserstein GAN},
   url = {http://arxiv.org/abs/1701.07875},
   year = {2017}
}
@article{Athalye2017,
   abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
   author = {Anish Athalye and Logan Engstrom and Andrew Ilyas and Kevin Kwok},
   issn = {1938-7228},
   title = {Synthesizing Robust Adversarial Examples},
   url = {http://arxiv.org/abs/1707.07397},
   year = {2017}
}
@article{Goodfellow2016,
   abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
   author = {Ian Goodfellow},
   doi = {10.1001/jamainternmed.2016.8245},
   isbn = {1581138285},
   issn = {0253-0465},
   pmid = {15040217},
   title = {NIPS 2016 Tutorial: Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1701.00160},
   year = {2016}
}
@article{Peeters2015,
   abstract = {Many researchers face the problem of missing data in longitudinal research. Especially, high risk samples are characterized by missing data which can complicate analyses and the interpretation of results. In the current study, our aim was to find the most optimal and best method to deal with the missing data in a specific study with many missing data on the outcome variable. Therefore, different techniques to handle missing data were evaluated, and a solution to efficiently handle substantial amounts of missing data was provided. A simulation study was conducted to determine the most optimal method to deal with the missing data. Results revealed that multiple imputation (MI) using predictive mean matching was the most optimal method with respect to lowest bias and the smallest confidence interval (CI) while maintaining power. Listwise deletion and last observation carried backward also scored acceptable with respect to bias; however, CIs were much larger and sample size almost halved using these methods. ...},
   author = {Margot Peeters and Mariëlle Zondervan-Zwijnenburg and Gerko Vink and Rens van de Schoot},
   doi = {10.1080/17405629.2015.1049526},
   issn = {1740-5629},
   issue = {4},
   journal = {European Journal of Developmental Psychology},
   keywords = {high risk sample,longitudinal research,missing data,multiple imputation},
   month = {7},
   pages = {377-394},
   publisher = {Routledge},
   title = {How to handle missing data: A comparison of different approaches},
   volume = {12},
   url = {http://www.tandfonline.com/doi/full/10.1080/17405629.2015.1049526},
   year = {2015}
}
@article{Theis2015,
   abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
   author = {Lucas Theis and Aäron van den Oord and Matthias Bethge},
   month = {11},
   title = {A note on the evaluation of generative models},
   url = {http://arxiv.org/abs/1511.01844},
   year = {2015}
}
@techReport{Yoon2018,
   abstract = {We propose a novel method for imputing missing data by adapting the well-known Generative Ad-versarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Impu-tation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the miss-ingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.},
   author = {Jinsung Yoon and James Jordon and Mihaela Van Der Schaar},
   title = {GAIN: Missing Data Imputation using Generative Adversarial Nets},
   url = {https://arxiv.org/pdf/1806.02920.pdf},
   year = {2018}
}
@techReport{,
   abstract = {In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no information can be based on in the specific view to impute data for such samples. The commonly-used simple method of removing samples with a missing view can dramatically reduce sample size, thus diminishing the statistical power of a subsequent analysis. In this paper, we propose a novel approach for view imputation via generative adversarial networks (GANs), which we name by VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain mappings via a GAN using randomly-sampled data from each view, and then employs a multi-modal denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired data across the views. Then, by optimizing the GAN and DAE jointly, our model enables the knowledge integration for domain mappings and view correspondences to effectively recover the missing view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against the state of the art. The evaluation of VIGAN in a genetic study of substance use disorders further proves the effectiveness and usability of this approach in life science.},
   author = {Chao Shang and Aaron Palmer and Jiangwen Sun and Ko-Shin Chen and Jin Lu and Jinbo Bi},
   keywords = {autoencoder,cycle-consistent,domain mapping,generative adversarial networks,missing data,missing view},
   title = {VIGAN: Missing View Imputation with Generative Adversarial Networks},
   url = {https://arxiv.org/pdf/1708.06724.pdf}
}
@article{Armina2017,
   abstract = {The presence of the missing value in the data set has always been a major problem for precise prediction. The method for imputing missing value needs to minimize the effect of incomplete data sets for the prediction model. Many algorithms have been proposed for countermeasure of missing value problem. In this review, we provide a comprehensive analysis of existing imputation algorithm, focusing on the technique used and the implementation of global or local information of data sets for missing value estimation. In addition validation method for imputation result and way to measure the performance of imputation algorithm also described. The objective of this review is to highlight possible improvement on existing method and it is hoped that this review gives reader better understanding of imputation method trend.},
   author = {Roslan Armina and Azlan Mohd Zain and Nor Azizah Ali and Roselina Sallehuddin},
   doi = {10.1088/1742-6596/892/1/012004},
   journal = {IOP Conf. Series: Journal of Physics: Conf. Series},
   keywords = {affordable,fast,flexible,open access,proceedings,template},
   pages = {12004},
   title = {A Review On Missing Value Estimation Using Imputation Algorithm Related content A Comparative Study of Imputation Methods for Estimation of Missing Values of Per Capita Expenditure in Central Java Y Susianto, K A Notodiputro, A Kurnia et al.-Developing of method for primary frequency control droop and deadband actual values estimation A A Nikiforov and A G Chaplin A Review On Missing Value Estimation Using Imputation Algorithm},
   volume = {892},
   url = {http://iopscience.iop.org/article/10.1088/1742-6596/892/1/012004/pdf},
   year = {2017}
}
@techReport{,
   abstract = {Previous work has shown that the difficulties in learning deep generative or discrim-inative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoen-coders on a pattern classification benchmark suite.},
   author = {Pascal Vincent and Hugo Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},
   title = {Extracting and Composing Robust Features with Denoising Autoencoders},
   url = {http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/176}
}
@techReport{,
   abstract = {Missing data is a significant problem impacting all domains. State-of-the-art framework for minimizing missing data bias is multiple imputation, for which the choice of an imputation model remains non-trivial. We propose a multiple imputation model based on overcomplete deep denoising autoencoders. Our proposed model is capable of handling different data types, missingness patterns, missingness proportions and distributions. Evaluation on several real life datasets show our proposed model significantly outperforms current state-of-the-art methods under varying conditions while simultaneously improving end of the line ana-lytics.},
   author = {Lovedeep Gondara and Ke Wang},
   title = {MIDA: Multiple Imputation using Denoising Autoencoders},
   url = {https://arxiv.org/pdf/1705.02737.pdf}
}
@article{Mackinnon2010,
   abstract = {Background.  Multiple imputation (MI) is an advanced, principled method of dealing with missing data in statistical analyses, a common problem in medical research. This paper sought to document the use of MI in general medical journals and to evaluate the information provided to readers about the application of the procedure in studies.\r\n\r\nMethods.  Research articles using MI in analyses published in JAMA, New England Journal of Medicine, BMJ and the Lancet were identified using full text searches from the earliest date each journal offered such searches until the end of 2008. Ninety-nine articles were found. Studies were classified according to their design.\r\n\r\nResults.  Multiple imputation was used in 49 RCTs and 50 other types of studies. A third of the articles (n = 33) reported no details of the procedure used. In a third of these (n = 11), it was not possible to infer the approach used from references cited or software used. The nature of the imputation model was rarely reported. MI was frequently used as a secondary analysis (n = 40) either to justify reporting a simpler approach or as a form of sensitivity analysis.\r\n\r\nConclusions.  Whilst still relatively uncommon, the use of MI has risen substantially, particularly in trials. MI is rarely adequately reported, leading to doubt about its appropriateness in some cases. This gives rise to uncertainty about conclusions reached and poses a barrier to attempts to replicate analyses. Guidelines for the reporting of MI should be developed.},
   author = {A. Mackinnon},
   doi = {10.1111/j.1365-2796.2010.02274.x},
   isbn = {1365-2796 (Electronic)\r0954-6820 (Linking)},
   issn = {09546820},
   issue = {6},
   journal = {Journal of Internal Medicine},
   keywords = {Biostatistics,Missing data handling,Multiple imputation,Randomized controlled trials,Reporting standards},
   pages = {586-593},
   pmid = {20831627},
   title = {The use and reporting of multiple imputation in medical research - a review},
   volume = {268},
   year = {2010}
}
@misc{,
   author = {Yann LeCun and Corinna Cortes and Christopher J. C. Burges},
   title = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges},
   url = {http://yann.lecun.com/exdb/mnist/}
}
@techReport{Goodfellow2014,
   abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
   author = {Ian J Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
   title = {Generative Adversarial Nets},
   url = {http://www.github.com/goodfeli/adversarial},
   year = {2014}
}
@misc{,
   title = {Image Completion with Deep Learning in TensorFlow},
   url = {http://bamos.github.io/2016/08/09/deep-completion/}
}
@article{Luengo2012,
   abstract = {In real-life data, information is frequently lost in data mining, caused by the presence of missing values in attributes. Several schemes have been studied to overcome the drawbacks produced by missing values in data mining tasks; one of the most well known is based on preprocessing, formerly known as imputation. In this work, we focus on a classification task with twenty-three classification methods and fourteen different imputation approaches to missing values treatment that are presented and analyzed. The analysis involves a group-based approach, in which we distinguish between three different categories of classification methods. Each category behaves differently, and the evidence obtained shows that the use of determined missing values imputation methods could improve the accuracy obtained for these methods. In this study, the convenience of using imputation methods for prepro-cessing data sets with missing values is stated. The analysis suggests that the use of particular imputation methods conditioned to the groups is required.},
   author = {Julián Luengo and Salvador García and Francisco Herrera},
   doi = {10.1007/s10115-011-0424-2},
   journal = {Knowl Inf Syst},
   keywords = {Approximate models ·,Classification ·,Imputation ·,Lazy learning ·,Missing values ·,Rule induction learning ·,Single imputation},
   pages = {77-108},
   title = {On the choice of the best imputation methods for missing values considering three groups of classification methods},
   volume = {32},
   url = {http://keel.es.},
   year = {2012}
}
@misc{Garciarena2017,
   author = {Unai Garciarena and Roberto Santana and Alexander Mendiburu},
   isbn = {1706.01120v2},
   keywords = {genetic programming,imputation methods,missing data,supervised classification},
   month = {6},
   title = {Evolving imputation strategies for missing data in classification problems with TPOT},
   url = {http://arxiv.org/abs/1706.01120 https://arxiv.org/pdf/1706.01120.pdf},
   year = {2017}
}
@article{Masconi2015,
   abstract = {Background},
   author = {Katya L Masconi and Tandi E Matsha and Rajiv T Erasmus and Andre P Kengne},
   doi = {10.1371/journal.pone.0139210},
   title = {Effects of Different Missing Data Imputation Techniques on the Performance of Undiagnosed Diabetes Risk Prediction Models in a Mixed-Ancestry Population of South Africa},
   url = {https://com-mendeley-prod-publicsharing-pdfstore.s3.eu-west-1.amazonaws.com/c580-PUBMED/10.1371/journal.pone.0139210/pone_0139210_pdf.pdf?X-Amz-Security-Token=FQoGZXIvYXdzECIaDENYbMKPSe7MdZHXJyKfBET0P6K2E%2BynA%2FjH9u4t3AN9F8eK1faSIwh5wpCFTKgtjuAiiKB8zPcrhhsIL4XLzHbCTjEJ4Ui%2FzfVswi8Ko9FJKBV3eELC8H%2Fg6yNjhppGg0Z2Ll8mW32nuHkA553KfkkUhibYrkhDbekAO2AA6vqTnugQxyLe37Clo7sP5FyGUdx231UBttfXgxrsz7Kxzl4KTSF50CMBx9uP0sjYIZOib70W6gakV4rd%2FZ1LkvOtTt8zirTw%2FdU1rfsy6ShmrLVo6ZA9n2uN2n2VoPJ4IZEb7EydOukGAVKarmdQ9e12nPg%2FYilv2IYvyqFsZjev506JDHhtIRo4ec9q8g8g3glX4wmgIreVuq%2FV7RDFiT71XX5rWzndBnEYmAl0PNimZuO5uzXTDOYZd2jrCpyUqiSM%2FQw%2FG06Wbfbe6F0y11N0BEHm%2FPVHR3HI25nNt9t%2BrBwMxbpkkWrrkYDv%2F4hs2MQAXNzhqVv0gr2wdxPQWNlC7vb5lmmdsH8vxiJRXLpjarrEQhJXcVtyJiGNDUwQJy4FZ%2FyOlrKPjEHpfBwBwc30H61jF8ZiWfJ1CzyMnR2P6xQedpvTjugtoBcvjUv7izVXj9%2BLxDdkhld33i%2BBE6unbrGcPcXyDidaWYRFwIruVd9TYoQnqoDntqX3WUcGlkxzVRu8MUP1e66Qz2gg6ZBAlXTLgG934lTTGXXvPq1n%2FCsiWYPHA7lLNUcDjP3owyib3sfpBQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20190719T182157Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIARSLZVEVESOW67EVF%2F20190719%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Signature=dabe7cff726a73b5b7f77fda46df4a3924e553cbd1a5f4e9a9f90cd87aac22d0},
   year = {2015}
}
@article{Papageorgiou2018,
   abstract = {Missing data are a common challenge encountered in research which can compromise the results of statistical inference when not handled appropriately. This paper aims to introduce basic concepts of missing data to a non-statistical audience, list and compare some of the most popular approaches for handling missing data in practice and provide guidelines and recommendations for dealing with and reporting missing data in scientific research. Complete case analysis and single imputation are simple approaches for handling missing data and are popular in practice, however, in most cases they are not guaranteed to provide valid inferences. Multiple imputation is a robust and general alternative which is appropriate for data missing at random, surpassing the disadvantages of the simpler approaches, but should always be conducted with care. The aforementioned approaches are illustrated and compared in an example application using Cox regression.},
   author = {Grigorios Papageorgiou and Stuart W. Grant and Johanna J.M. Takkenberg and Mostafa M. Mokhles},
   doi = {10.1093/icvts/ivy102},
   issn = {15699285},
   issue = {2},
   journal = {Interactive Cardiovascular and Thoracic Surgery},
   title = {Statistical primer: How to deal with missing data in scientific research?},
   volume = {27},
   year = {2018}
}
@article{Kowarik2016,
   abstract = {The package VIM (Templ, Alfons, Kowarik, and Prantner 2016) is developed to explore and analyze the structure of missing values in data using visualization methods, to impute these missing values with the built-in imputation methods and to verify the imputation process using visualization tools, as well as to produce high-quality graphics for publications. This article focuses on the different imputation techniques available in the package. Four different imputation methods are currently implemented in VIM, namely hot-deck imputation, k-nearest neighbor imputation, regression imputation and iterative robust model-based imputation (Templ, Kowarik, and Filzmoser 2011). All of these methods are implemented in a flexible manner with many options for customization. Furthermore in this article practical examples are provided to highlight the use of the implemented methods on real-world applications. In addition, the graphical user interface of VIM has been re-implemented from scratch resulting in the package VIMGUI (Schopfhauser, Templ, Alfons, Kowarik, and Prantner 2016) to enable users without extensive R skills to access these imputation and visualization methods.},
   author = {Alexander Kowarik and Matthias Templ},
   doi = {10.18637/jss.v074.i07},
   issn = {15487660},
   journal = {Journal of Statistical Software},
   keywords = {Imputation methods,Missing values,R},
   publisher = {American Statistical Association},
   title = {Imputation with the R package VIM},
   volume = {74},
   year = {2016}
}
@article{Schouten2018,
   abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
   author = {Rianne Margaretha Schouten and Peter Lugtig and Gerko Vink},
   doi = {10.1080/00949655.2018.1491577},
   issn = {15635163},
   issue = {15},
   journal = {Journal of Statistical Computation and Simulation},
   title = {Generating missing values for simulation purposes: a multivariate amputation procedure},
   volume = {88},
   year = {2018}
}
@article{HayatiRezvan2015,
   abstract = {BACKGROUND: Multiple imputation (MI) is a well-recognised statistical technique for handling missing data. As usually implemented in standard statistical software, MI assumes that data are 'Missing at random' (MAR); an assumption that in many settings is implausible. It is not possible to distinguish whether data are MAR or 'Missing not at random' (MNAR) using the observed data, so it is desirable to discover the impact of departures from the MAR assumption on the MI results by conducting sensitivity analyses. A weighting approach based on a selection model has been proposed for performing MNAR analyses to assess the robustness of results obtained under standard MI to departures from MAR.\n\nMETHODS: In this article, we use simulation to evaluate the weighting approach as a method for exploring possible departures from MAR, with missingness in a single variable, where the parameters of interest are the marginal mean (and probability) of a partially observed outcome variable and a measure of association between the outcome and a fully observed exposure. The simulation studies compare the weighting-based MNAR estimates for various numbers of imputations in small and large samples, for moderate to large magnitudes of departure from MAR, where the degree of departure from MAR was assumed known. Further, we evaluated a proposed graphical method, which uses the dataset with missing data, for obtaining a plausible range of values for the parameter that quantifies the magnitude of departure from MAR.\n\nRESULTS: Our simulation studies confirm that the weighting approach outperformed the MAR approach, but it still suffered from bias. In particular, our findings demonstrate that the weighting approach provides biased parameter estimates, even when a large number of imputations is performed. In the examples presented, the graphical approach for selecting a range of values for the possible departures from MAR did not capture the true parameter value of departure used in generating the data.\n\nCONCLUSIONS: Overall, the weighting approach is not recommended for sensitivity analyses following MI, and further research is required to develop more appropriate methods to perform such sensitivity analyses.},
   author = {Panteha Hayati Rezvan and Ian R. White and Katherine J. Lee and John B. Carlin and Julie A. Simpson},
   doi = {10.1186/s12874-015-0074-2},
   issn = {14712288},
   issue = {1},
   journal = {BMC Medical Research Methodology},
   keywords = {Missing not at random,Multiple imputation,Selection model,Sensitivity analysis,Weighting approach},
   pages = {1-16},
   publisher = {BMC Medical Research Methodology},
   title = {Evaluation of a weighting approach for performing sensitivity analysis after multiple imputation},
   volume = {15},
   year = {2015}
}
@article{Lin2019,
   abstract = {Missing value imputation (MVI) has been studied for several decades being the basic solution method for incomplete dataset problems, specifically those where some data samples contain one or more missing attribute values. This paper aims at reviewing and analyzing related studies carried out in recent decades, from the experimental design perspective. Altogether, 111 journal papers published from 2006 to 2017 are reviewed and analyzed. In addition, several technical issues encountered during the MVI process are addressed, such as the choice of datasets, missing rates and missingness mechanisms, and the MVI techniques and evaluation metrics employed, are discussed. The results of analysis of these issues allow limitations in the existing body of literature to be identified based upon which some directions for future research can be gleaned.},
   author = {Wei Chao Lin and Chih Fong Tsai},
   doi = {10.1007/s10462-019-09709-4},
   issn = {15737462},
   journal = {Artificial Intelligence Review},
   keywords = {Data mining,Imputation,Incomplete dataset,Missing values,Supervised learning},
   publisher = {Springer Netherlands},
   title = {Missing value imputation: a review and analysis of the literature (2006–2017)},
   year = {2019}
}
@article{Madley-Dowd2019,
   abstract = {Objectives: Researchers are concerned whether multiple imputation (MI) or complete case analysis should be used when a large proportion of data are missing. We aimed to provide guidance for drawing conclusions from data with a large proportion of missingness. Study Design and Setting: Via simulations, we investigated how the proportion of missing data, the fraction of missing information (FMI), and availability of auxiliary variables affected MI performance. Outcome data were missing completely at random or missing at random (MAR). Results: Provided sufficient auxiliary information was available; MI was beneficial in terms of bias and never detrimental in terms of efficiency. Models with similar FMI values, but differing proportions of missing data, also had similar precision for effect estimates. In the absence of bias, the FMI was a better guide to the efficiency gains using MI than the proportion of missing data. Conclusion: We provide evidence that for MAR data, valid MI reduces bias even when the proportion of missingness is large. We advise researchers to use FMI to guide choice of auxiliary variables for efficiency gain in imputation analyses, and that sensitivity analyses including different imputation models may be needed if the number of complete cases is small.},
   author = {Paul Madley-Dowd and Rachael Hughes and Kate Tilling and Jon Heron},
   doi = {10.1016/j.jclinepi.2019.02.016},
   issn = {18785921},
   journal = {Journal of Clinical Epidemiology},
   keywords = {ALSPAC,Bias,Methods,Missing data,Multiple imputation,Simulation},
   month = {6},
   pages = {63-73},
   publisher = {Elsevier USA},
   title = {The proportion of missing data should not be used to guide decisions on multiple imputation},
   volume = {110},
   year = {2019}
}
@article{Santos2019,
   abstract = {The performance evaluation of imputation algorithms often involves the generation of missing values. Missing values can be inserted in only one feature (univariate configuration) or in several features (multivariate configuration) at different percentages (missing rates) and according to distinct missing mechanisms, namely, missing completely at random, missing at random, and missing not at random. Since the missing data generation process defines the basis for the imputation experiments (configuration, missing rate, and missing mechanism), it is essential that it is appropriately applied; otherwise, conclusions derived from ill-defined setups may be invalid. The goal of this paper is to review the different approaches to synthetic missing data generation found in the literature and discuss their practical details, elaborating on their strengths and weaknesses. Our analysis revealed that creating missing at random and missing not at random scenarios in datasets comprising qualitative features is the most challenging issue in the related work and, therefore, should be the focus of future work in the field.},
   author = {Miriam Seoane Santos and Ricardo Cardoso Pereira and Adriana Fonseca Costa and Jastin Pompeu Soares and Joao Santos and Pedro Henriques Abreu},
   doi = {10.1109/ACCESS.2019.2891360},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Data preprocessing,missing data,missing data generation,missing data mechanisms},
   pages = {11651-11667},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Generating synthetic missing data: A review by missing mechanism},
   volume = {7},
   year = {2019}
}
@misc{Tiwari2017,
   abstract = {Machine learning refers to the changes in systems that perform tasks associated with artificial intelligence. This chapter presents introduction types and application of machine learning. This chapter also presents the basic concepts related to feature selection techniques such as filter, wrapper and hybrid methods and various machine learning techniques such as artificial neural network, Naive Bayes classifier, support vector machine, k-nearest-neighbor, decision trees, bagging, boosting, random subspace method, randomforests, k-means clustering and deep learning. In the last the performance measure of the classifier is presented.},
   author = {Arvind Kumar Tiwari},
   doi = {10.4018/978-1-5225-2545-5.ch001},
   isbn = {9781522525462},
   journal = {Ubiquitous Machine Learning and Its Applications},
   month = {3},
   pages = {1-14},
   publisher = {IGI Global},
   title = {Introduction to machine learning},
   year = {2017}
}
@article{Hughes2019,
   abstract = {Background: Missing data are unavoidable in epidemiological research, potentially leading to bias and loss of precision. Multiple imputation (MI) is widely advocated as an improvement over complete case analysis (CCA). However, contrary to widespread belief, CCA is preferable to MI in some situations. Methods: We provide guidance on choice of analysis when data are incomplete. Using causal diagrams to depict missingness mechanisms, we describe when CCA will not be biased by missing data and compare MI and CCA, with respect to bias and efficiency, in a range of missing data situations. We illustrate selection of an appropriate method in practice. Results: For most regression models, CCA gives unbiased results when the chance of being a complete case does not depend on the outcome after taking the covariates into consideration, which includes situations where data are missing not at random. Consequently, there are situations in which CCA analyses are unbiased while MI analyses, assuming missing at random (MAR), are biased. By contrast MI, unlike CCA, is valid for all MAR situations and has the potential to use information contained in the incomplete cases and auxiliary variables to reduce bias and/or improve precision. For this reason, MI was preferred over CCA in our real data example. Conclusions: Choice of method for dealing with missing data is crucial for validity of conclusions, and should be based on careful consideration of the reasons for the missing data, missing data patterns and the availability of auxiliary information.},
   author = {Rachael A. Hughes and Jon Heron and Jonathan A.C. Sterne and Kate Tilling},
   doi = {10.1093/ije/dyz032},
   issn = {14643685},
   issue = {4},
   journal = {International Journal of Epidemiology},
   keywords = {Complete case analysis,inverse probability weighting,missing data,missing data mechanisms,missing data patterns,multiple imputation},
   month = {8},
   pages = {1294-1304},
   publisher = {Oxford University Press},
   title = {Accounting for missing data in statistical analyses: Multiple imputation is not always the answer},
   volume = {48},
   year = {2019}
}
@article{Pampaka2016,
   abstract = {Missing data is endemic in much educational research. However, practices such as step-wise regression common in the educational research literature have been shown to be dangerous when significant data are missing, and multiple imputation (MI) is generally recommended by statisticians. In this paper, we provide a review of these advances and their implications for educational research. We illustrate the issues with an educational, longitudinal survey in which missing data was significant, but for which we were able to collect much of these missing data through subsequent data collection. We thus compare methods, that is, step-wise regression (basically ignoring the missing data) and MI models, with the model from the actual enhanced sample. The value of MI is discussed and the risks involved in ignoring missing data are considered. Implications for research practice are discussed.},
   author = {Maria Pampaka and Graeme Hutcheson and Julian Williams},
   doi = {10.1080/1743727X.2014.979146},
   issn = {17437288},
   issue = {1},
   journal = {International Journal of Research and Method in Education},
   keywords = {missing data,modelling,multiple imputation,regression,surveys},
   pages = {19-37},
   publisher = {Taylor \& Francis},
   title = {Handling missing data: analysis of a challenging data set using multiple imputation},
   volume = {39},
   year = {2016}
}
@article{McNeish2017,
   abstract = {Missing data are a prevalent and widespread data analytic issue and previous studies have performed simulations to compare the performance of missing data methods in various contexts and for various models; however, one such context that has yet to receive much attention in the literature is the handling of missing data with small samples, particularly when the missingness is arbitrary. Prior studies have either compared methods for small samples with monotone missingness commonly found in longitudinal studies or have investigated the performance of a single method to handle arbitrary missingness with small samples but studies have yet to compare the relative performance of commonly implemented missing data methods for small samples with arbitrary missingness. This study conducts a simulation study to compare and assess the small sample performance of maximum likelihood, listwise deletion, joint multiple imputation, and fully conditional specification multiple imputation for a single-level regression model with a continuous outcome. Results showed that, provided assumptions are met, joint multiple imputation unanimously performed best of the methods examined in the conditions under study.},
   author = {Daniel McNeish},
   doi = {10.1080/02664763.2016.1158246},
   issn = {13600532},
   issue = {1},
   journal = {Journal of Applied Statistics},
   keywords = {Monte Carlo simulation,Small sample,finite sample,full information maximum likelihood,incomplete data,missing data,multiple imputation},
   pages = {24-39},
   title = {Missing data methods for arbitrary missingness with small samples},
   volume = {44},
   url = {https://doi.org/10.1080/02664763.2016.1158246},
   year = {2017}
}
@article{Bertsimas2018,
   abstract = {Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including Knearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, K-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3% against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50% data missing, the average out-of-sample R2 is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1% in the classification tasks, compared to 0.315 and 84.4% for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered.},
   author = {Dimitris Bertsimas and Colin Pawlowski and Ying Daisy Zhuo},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {K-NN,Missing data imputation,Optimal decision trees,SVM},
   month = {4},
   pages = {1-39},
   publisher = {Microtome Publishing},
   title = {From predictive methods to missing data imputation: An optimization approach},
   volume = {18},
   year = {2018}
}
@article{Audigier2017,
   abstract = {We propose a multiple imputation method to deal with incomplete categorical data. This method imputes the missing entries using the principal component method dedicated to categorical data: multiple correspondence analysis (MCA). The uncertainty concerning the parameters of the imputation model is reflected using a non-parametric bootstrap. Multiple imputation using MCA (MIMCA) requires estimating a small number of parameters due to the dimensionality reduction property of MCA. It allows the user to impute a large range of data sets. In particular, a high number of categories per variable, a high number of variables or a small number of individuals are not an issue for MIMCA. Through a simulation study based on real data sets, the method is assessed and compared to the reference methods (multiple imputation using the loglinear model, multiple imputation by logistic regressions) as well to the latest works on the topic (multiple imputation by random forests or by the Dirichlet process mixture of products of multinomial distributions model). The proposed method provides a good point estimate of the parameters of the analysis model considered, such as the coefficients of a main effects logistic regression model, and a reliable estimate of the variability of the estimators. In addition, MIMCA has the great advantage that it is substantially less time consuming on data sets of high dimensions than the other multiple imputation methods.},
   author = {Vincent Audigier and François Husson and Julie Josse},
   doi = {10.1007/s11222-016-9635-4},
   issn = {15731375},
   issue = {2},
   journal = {Statistics and Computing},
   keywords = {Bootstrap,Categorical data,Missing values,Multiple correspondence analysis,Multiple imputation},
   month = {3},
   pages = {501-518},
   publisher = {Springer New York LLC},
   title = {MIMCA: multiple imputation for categorical variables with multiple correspondence analysis},
   volume = {27},
   year = {2017}
}
@article{Yadav2018,
   abstract = {In real world data are often plagued by missing values which adversely affects the final outcome of the analysis based on such data. The missing values can be handled using various techniques like deletion or imputation. Of late, R has become one of the most preferred platform for carrying out data analysis, and its popularity is growing further. R provides various packages for handling missing values through imputation. The presence of multiple packages however, calls for an analysis of their comparative performance and examine their suitability for handling a given set of data. The performance of different R packages may differ for different datasets and may depend on the size of the dataset and richness of the missing values in the datasets. In this paper, the authors perform comparative study of the performance of the common R packages, namely VIM, MICE, MissForest, and HMISC, used for missing value imputation. The authors measured the performances of the said packages in terms of their imputation time, imputation efficiency and the effect on the variance. The imputation efficiency was measured in terms of the difference in predictive performance of a model built using original dataset vis-à-vis a dataset with imputed values. Similarly, the variance of the variables in the original dataset was compared that of corresponding variables in the imputed dataset. A missing value imputation package can be considered to be better if it consumes less imputation time and provides high imputation accuracy. Also in terms of variance, one would like to have the imputation package maintain the original variance of the variables. On analysing the four imputation packages on two datasets over three predictive algorithms–Logistic Regression, Support Vector Machines, and Artificial Neural Networks–it was observed that the performances varies depending on the size of the dataset, and the missing values present in them. The study highlights that certain missing value package used in conjunction with a given predictive algorithm provides better performance, which is again a function of the dataset characteristics.},
   author = {Madan Lal Yadav and Basav Roychoudhury},
   doi = {10.1016/j.knosys.2018.06.012},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   keywords = {HMISC,Imputation Accuracy,Imputation Time,MICE,MissForest,Missing value handling,VIM},
   month = {11},
   pages = {104-118},
   publisher = {Elsevier B.V.},
   title = {Handling missing values: A study of popular imputation packages in R},
   volume = {160},
   year = {2018}
}
@article{Akande2017,
   abstract = {Multiple imputation is a common approach for dealing with missing values in statistical databases. The imputer fills in missing values with draws from predictive models estimated from the observed data, resulting in multiple, completed versions of the database. Researchers have developed a variety of default routines to implement multiple imputation; however, there has been limited research comparing the performance of these methods, particularly for categorical data. We use simulation studies to compare repeated sampling properties of three default multiple imputation methods for categorical data, including chained equations using generalized linear models, chained equations using classification and regression trees, and a fully Bayesian joint distribution based on Dirichlet process mixture models. We base the simulations on categorical data from the American Community Survey. In the circumstances of this study, the results suggest that default chained equations approaches based on generalized linear models are dominated by the default regression tree and Bayesian mixture model approaches. They also suggest competing advantages for the regression tree and Bayesian mixture model approaches, making both reasonable default engines for multiple imputation of categorical data. Supplementary material for this article is available online.},
   author = {Olanrewaju Akande and Fan Li and Jerome Reiter},
   doi = {10.1080/00031305.2016.1277158},
   issn = {15372731},
   issue = {2},
   journal = {American Statistician},
   keywords = {Latent,Missing,Mixture,Nonresponse,Tree},
   month = {4},
   pages = {162-170},
   publisher = {American Statistical Association},
   title = {An Empirical Comparison of Multiple Imputation Methods for Categorical Data},
   volume = {71},
   year = {2017}
}
@techReport{Poulos2018,
   abstract = {Missing data imputation can help improve the performance of prediction models in situations where missing data hide useful information. This paper compares methods for imputing missing categorical data for supervised classification tasks. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different levels of additional missing-data perturbation. We show imputation methods can increase predictive accuracy in the presence of missing-data perturbation, which can actually improve prediction accuracy by regularizing the classifier. We achieve the state-of-the-art on the Adult dataset with missing-data perturbation and k-nearest-neighbors (k-NN) imputation.},
   author = {Jason Poulos and Rafael Valle},
   keywords = {decision trees,imputation methods,missing data,neural networks,perturbation,random forests †},
   title = {MISSING DATA IMPUTATION FOR SUPERVISED LEARNING †},
   year = {2018}
}
@article{Lall2016,
   abstract = {Political scientists increasingly recognize that multiple imputation represents a superior strategy for analyzing missing data to the widely used method of listwise deletion. However, there has been little systematic investigation of how multiple imputation affects existing empirical knowledge in the discipline. This article presents the first large-scale examination of the empirical effects of substituting multiple imputation for listwise deletion in political science. The examination focuses on research in the major subfield of comparative and international political economy (CIPE) as an illustrative example. Specifically, I use multiple imputation to reanalyze the results of almost every quantitative CIPE study published during a recent five-year period in International Organization and World Politics, two of the leading subfield journals in CIPE. The outcome is striking: in almost half of the studies, key results "disappear" (by conventional statistical standards) when reanalyzed.},
   author = {Ranjit Lall},
   doi = {10.1093/pan/mpw020},
   issn = {14764989},
   issue = {4},
   journal = {Political Analysis},
   month = {10},
   pages = {414-433},
   publisher = {Oxford University Press},
   title = {How multiple imputation makes a difference},
   volume = {24},
   year = {2016}
}
@article{Josse2016,
   abstract = {We present the R package missMDA which performs principal component methods on incomplete data sets, aiming to obtain scores, loadings and graphical representations despite missing values. Package methods include principal component analysis for continuous variables, multiple correspondence analysis for categorical variables, factorial analysis on mixed data for both continuous and categorical variables, and multiple factor analysis for multi-table data. Furthermore, missMDA can be used to perform single imputation to complete data involving continuous, categorical and mixed variables. A multiple imputation method is also available. In the principal component analysis framework, variability across different imputations is represented by confidence areas around the row and column positions on the graphical outputs. This allows assessment of the credibility of results obtained from incomplete data sets.},
   author = {Julie Josse and François Husson},
   doi = {10.18637/jss.v070.i01},
   issn = {15487660},
   journal = {Journal of Statistical Software},
   keywords = {Missing values,Mixed data,Multi-table data,Multiple correspondence analysis,Multiple factor analysis,Multiple imputation,Principal component analysis,Single imputation},
   publisher = {American Statistical Association},
   title = {missMDA: A package for handling missing values in multivariate data analysis},
   volume = {70},
   year = {2016}
}
@article{Pedersen2017,
   abstract = {Missing data are ubiquitous in clinical epidemiological research. Individuals with missing data may differ from those with no missing data in terms of the outcome of interest and prognosis in general. Missing data are often categorized into the following three types: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). In clinical epidemiological research, missing data are seldom MCAR. Missing data can constitute considerable challenges in the analyses and interpretation of results and can potentially weaken the validity of results and conclusions. A number of methods have been developed for dealing with missing data. These include complete-case analyses, missing indicator method, single value imputation, and sensitivity analyses incorporating worst-case and best-case scenarios. If applied under the MCAR assumption, some of these methods can provide unbiased but often less precise estimates. Multiple imputation is an alternative method to deal with missing data, which accounts for the uncertainty associated with missing data. Multiple imputation is implemented in most statistical software under the MAR assumption and provides unbiased and valid estimates of associations based on information from the available data. The method affects not only the coefficient estimates for variables with missing data but also the estimates for other variables with no missing data.},
   author = {Alma B. Pedersen and Ellen M. Mikkelsen and Deirdre Cronin-Fenton and Nickolaj R. Kristensen and Tra My Pham and Lars Pedersen and Irene Petersen},
   doi = {10.2147/CLEP.S129785},
   issn = {11791349},
   journal = {Clinical Epidemiology},
   keywords = {MAR,MCAR,MNAR,Missing data,Multiple imputation,Observational study},
   month = {3},
   pages = {157-166},
   pmid = {28352203},
   publisher = {Dove Medical Press Ltd},
   title = {Missing data and multiple imputation in clinical epidemiological research},
   volume = {9},
   year = {2017}
}
@article{Che2018,
   abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
   author = {Zhengping Che and Sanjay Purushotham and Kyunghyun Cho and David Sontag and Yan Liu},
   doi = {10.1038/s41598-018-24271-9},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   publisher = {Nature Publishing Group},
   title = {Recurrent Neural Networks for Multivariate Time Series with Missing Values},
   volume = {8},
   year = {2018}
}
@article{Hughes2016,
   abstract = {Appropriate imputation inference requires both an unbiased imputation estimator and an unbiased variance estimator. The commonly used variance estimator, proposed by Rubin, can be biased when the imputation and analysis models are misspecified and/or incompatible. Robins and Wang proposed an alternative approach, which allows for such misspecification and incompatibility, but it is considerably more complex. It is unknown whether in practice Robins and Wang's multiple imputation procedure is an improvement over Rubin's multiple imputation. We conducted a critical review of these two multiple imputation approaches, a re-sampling method called full mechanism bootstrapping and our modified Rubin's multiple imputation procedure via simulations and an application to data. We explored four common scenarios of misspecification and incompatibility. In general, for a moderate sample size (n = 1000), Robins and Wang's multiple imputation produced the narrowest confidence intervals, with acceptable coverage. For a small sample size (n = 100) Rubin's multiple imputation, overall, outperformed the other methods. Full mechanism bootstrapping was inefficient relative to the other methods and required modelling of the missing data mechanism under the missing at random assumption. Our proposed modification showed an improvement over Rubin's multiple imputation in the presence of misspecification. Overall, Rubin's multiple imputation variance estimator can fail in the presence of incompatibility and/or misspecification. For unavoidable incompatibility and/or misspecification, Robins and Wang's multiple imputation could provide more robust inferences.},
   author = {Rachael A. Hughes and J. A.C. Sterne and K. Tilling},
   doi = {10.1177/0962280214526216},
   issn = {14770334},
   issue = {6},
   journal = {Statistical Methods in Medical Research},
   keywords = {bootstrap confidence intervals,imputation inference,missing data,multiple imputation,variance estimator},
   month = {12},
   pages = {2541-2557},
   publisher = {SAGE Publications Ltd},
   title = {Comparison of imputation variance estimators},
   volume = {25},
   year = {2016}
}
@article{Wulff2017,
   abstract = {Multiple imputation by chained equations (MICE) is an effective tool to handle missing data - an almost unavoidable problem in quantitative data analysis. However, despite the empirical and theoretical evidence supporting the use of MICE, researchers in the social sciences often resort to inferior approaches unnecessarily risking erroneous results. The complexity of the decision process when encountering missing data may be what is discouraging potential users from adopting the appropriate technique. In this article, we develop straightforward step-by-step graphical guidelines on how to handle missing data based on a comprehensive literature review. It is our hope that these guidelines can help improve current standards of handling missing data. The guidelines incorporate recent innovations on how to handle missing data such as random forests and predictive mean matching. Thus, the data analysts who already actively apply MICE may use it to review some of the newest developments. We demonstrate how the guidelines can be used in praxis using the statistical program R and data from the European Social Survey. We demonstrate central decisions such as variable selection and number of imputations as well as how to handle typical challenges such as skewed distributions and data transformations. These guidelines will enable a social science researcher to go through the process of handling missing data while adhering to the newest developments in the field.},
   author = {Jesper N. Wulff and Linda Ejlskov},
   issn = {14777029},
   issue = {1},
   journal = {Electronic Journal of Business Research Methods},
   keywords = {Guidelines,MICE,Missing data,Multiple imputation by chained equations,R,Review},
   month = {4},
   pages = {41-56},
   publisher = {Academic Publishing Limited},
   title = {Multiple imputation by chained equations in praxis: Guidelines and review},
   volume = {15},
   year = {2017}
}
@article{Choudhury2019,
   abstract = {We propose a mechanism to use data with missing values for designing classifiers which is different from predicting missing values for classification. Our imputation method uses an auto-encoder neural network. We make an innovative use of the training data without missing values to train the auto-encoder so that it is better equipped to predict missing values. It is a two-stage training scheme. Unlike most of the existing auto-encoder based methods which use a bottleneck layer for missing data handling, we justify and use a latent space of much higher dimension than that of the input. Now to design a classifier using a training set with missing values, we use the trained auto-encoder to predict missing values based on the hypothesis that a good choice for a missing value would be the one which can reconstruct itself via the auto-encoder. For this we make an initial guess of the missing value using the nearest neighbor rule and then refine the missing value minimizing the reconstruction error. We train several classifiers using the union of the imputed instances and the remaining training instances without missing values. We also train another classifier of the same type with the same configuration using the corresponding complete dataset. The performances of these classifiers are compared. We compare the proposed method with eight state-of-the-art imputation techniques using fourteen datasets and eight classification strategies.},
   author = {Suvra Jyoti Choudhury and Nikhil R. Pal},
   doi = {10.1016/j.knosys.2019.07.009},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   keywords = {Classification,Data imputation,Gradient decent,Missing attribute value,Neural network},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Imputation of missing data with neural networks for classification},
   volume = {182},
   year = {2019}
}
@article{Morris2019,
   abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
   author = {Tim P. Morris and Ian R. White and Michael J. Crowther},
   doi = {10.1002/sim.8086},
   issn = {10970258},
   issue = {11},
   journal = {Statistics in Medicine},
   keywords = {Monte Carlo,graphics for simulation,simulation design,simulation reporting,simulation studies},
   month = {5},
   pages = {2074-2102},
   publisher = {John Wiley and Sons Ltd},
   title = {Using simulation studies to evaluate statistical methods},
   volume = {38},
   year = {2019}
}
@article{Grund2016,
   abstract = {The treatment of missing data can be difficult in multilevel research because state-of-the-art procedures such as multiple imputation (MI) may require advanced statistical knowledge or a high degree of familiarity with certain statistical software. In the missing data literature, pan has been recommended for MI of multilevel data. In this article, we provide an introduction to MI of multilevel missing data using the R package pan, and we discuss its possibilities and limitations in accommodating typical questions in multilevel research. To make pan more accessible to applied researchers, we make use of the mitml package, which provides a user-friendly interface to the pan package and several tools for managing and analyzing multiply imputed data sets. We illustrate the use of pan and mitml with two empirical examples that represent common applications of multilevel models, and we discuss how these procedures may be used in conjunction with other software.},
   author = {Simon Grund and Oliver Lüdtke and Alexander Robitzsch},
   doi = {10.1177/2158244016668220},
   issn = {21582440},
   issue = {4},
   journal = {SAGE Open},
   keywords = {R,missing data,multilevel,multiple imputation},
   month = {10},
   publisher = {SAGE Publications Inc.},
   title = {Multiple Imputation of Multilevel Missing Data: An Introduction to the R Package pan},
   volume = {6},
   year = {2016}
}
@article{Murray2018,
   abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
   author = {Jared S. Murray},
   doi = {10.1214/18-STS644},
   issn = {08834237},
   issue = {2},
   journal = {Statistical Science},
   keywords = {Chained equations,Congeniality,Fully conditional specification,Missing data,Proper imputation,Sequential regression multivariate imputation},
   month = {5},
   pages = {142-159},
   publisher = {Institute of Mathematical Statistics},
   title = {Multiple imputation: A review of practical and theoretical findings},
   volume = {33},
   year = {2018}
}
@article{Jordanov2018,
   abstract = {In this paper we investigate further and extend our previous work on radar signal identification and classification based on a data set which comprises continuous, discrete and categorical data that represent radar pulse train characteristics such as signal frequencies, pulse repetition, type of modulation, intervals, scan period, scanning type, etc. As the most of the real world datasets, it also contains high percentage of missing values and to deal with this problem we investigate three imputation techniques: Multiple Imputation (MI); K-Nearest Neighbour Imputation (KNNI); and Bagged Tree Imputation (BTI). We apply these methods to data samples with up to 60% missingness, this way doubling the number of instances with complete values in the resulting dataset. The imputation models performance is assessed with Wilcoxon's test for statistical significance and Cohen's effect size metrics. To solve the classification task, we employ three intelligent approaches: Neural Networks (NN); Support Vector Machines (SVM); and Random Forests (RF). Subsequently, we critically analyse which imputation method influences most the classifiers' performance, using a multiclass classification accuracy metric, based on the area under the ROC curves. We consider two superclasses ('military' and 'civil'), each containing several 'subclasses', and introduce and propose two new metrics: inner class accuracy (IA); and outer class accuracy (OA), in addition to the overall classification accuracy (OCA) metric. We conclude that they can be used as complementary to the OCA when choosing the best classifier for the problem at hand.},
   author = {Ivan Jordanov and Nedyalko Petrov and Alessio Petrozziello},
   doi = {10.1515/jaiscr-2018-0002},
   issn = {24496499},
   issue = {1},
   journal = {Journal of Artificial Intelligence and Soft Computing Research},
   keywords = {machine learning,missing data,model-based imputation,neural networks,radar signal classification,random forests,support vector machines},
   month = {1},
   pages = {31-48},
   publisher = {De Gruyter Open Ltd},
   title = {Classifiers Accuracy Improvement Based on Missing Data Imputation},
   volume = {8},
   year = {2018}
}
@article{Beretta2016,
   abstract = {Background: Nearest neighbor (NN) imputation algorithms are efficient methods to fill in missing data where each missing value on some records is replaced by a value obtained from related cases in the whole set of records. Besides the capability to substitute the missing data with plausible values that are as close as possible to the true value, imputation algorithms should preserve the original data structure and avoid to distort the distribution of the imputed variable. Despite the efficiency of NN algorithms little is known about the effect of these methods on data structure. Methods: Simulation on synthetic datasets with different patterns and degrees of missingness were conducted to evaluate the performance of NN with one single neighbor (1NN) and with k neighbors without (kNN) or with weighting (wkNN) in the context of different learning frameworks: plain set, reduced set after ReliefF filtering, bagging, random choice of attributes, bagging combined with random choice of attributes (Random-Forest-like method). Results: Whatever the framework, kNN usually outperformed 1NN in terms of precision of imputation and reduced errors in inferential statistics, 1NN was however the only method capable of preserving the data structure and data were distorted even when small values of k neighbors were considered; distortion was more severe for resampling schemas. Conclusions: The use of three neighbors in conjunction with ReliefF seems to provide the best trade-off between imputation error and preservation of the data structure. The very same conclusions can be drawn when imputation experiments were conducted on the single proton emission computed tomography (SPECTF) heart dataset after introduction of missing data completely at random.},
   author = {Lorenzo Beretta and Alessandro Santaniello},
   doi = {10.1186/s12911-016-0318-z},
   issn = {14726947},
   journal = {BMC Medical Informatics and Decision Making},
   month = {7},
   publisher = {BioMed Central Ltd.},
   title = {Nearest neighbor imputation algorithms: A critical evaluation},
   volume = {16},
   year = {2016}
}
@article{Audigier2016,
   abstract = {We propose a multiple imputation method based on principal component analysis (PCA) to deal with incomplete continuous data. To reflect the uncertainty of the parameters from one imputation to the next, we use a Bayesian treatment of the PCA model. Using a simulation study and real data sets, the method is compared to two classical approaches: multiple imputation based on joint modelling and on fully conditional modelling. Contrary to the others, the proposed method can be easily used on data sets where the number of individuals is less than the number of variables and when the variables are highly correlated. In addition, it provides unbiased point estimates of quantities of interest, such as an expectation, a regression coefficient or a correlation coefficient, with a smaller mean squared error. Furthermore, the widths of the confidence intervals built for the quantities of interest are often smaller whilst ensuring a valid coverage.},
   author = {Vincent Audigier and François Husson and Julie Josse},
   doi = {10.1080/00949655.2015.1104683},
   issn = {15635163},
   issue = {11},
   journal = {Journal of Statistical Computation and Simulation},
   keywords = {Bayesian principal component analysis,Missing values,continuous data,data augmentation,multiple imputation},
   month = {7},
   pages = {2140-2156},
   publisher = {Taylor and Francis Ltd.},
   title = {Multiple imputation for continuous variables using a Bayesian principal component analysis},
   volume = {86},
   year = {2016}
}
@article{Nguyen2017,
   abstract = {Background: Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models. Analysis: In this paper, we provide an overview of currently available methods for checking imputation models. These include graphical checks and numerical summaries, as well as simulation-based methods such as posterior predictive checking. These model checking techniques are illustrated using an analysis affected by missing data from the Longitudinal Study of Australian Children. Conclusions: As multiple imputation becomes further established as a standard approach for handling missing data, it will become increasingly important that researchers employ appropriate model checking approaches to ensure that reliable results are obtained when using this method.},
   author = {Cattram D. Nguyen and John B. Carlin and Katherine J. Lee},
   doi = {10.1186/s12982-017-0062-6},
   issn = {17427622},
   issue = {1},
   journal = {Emerging Themes in Epidemiology},
   title = {Model checking in multiple imputation: An overview and case study},
   volume = {14},
   year = {2017}
}
@article{McCoy2018,
   abstract = {Missing data values and differing sampling rates, particularly for important parameters such as particle size and stream composition, are a common problem in minerals processing plants. Missing data imputation is used to avoid information loss (due to downsampling or discarding incomplete records). A recent deep-learning technique, variational autoencoders (VAEs), has been used for missing data imputation in image data, and was compared here to imputation by mean replacement and by principal component analysis (PCA) imputation. The techniques were compared using a synthetic, nonlinear dataset, and a simulated milling circuit dataset, which included process disturbances, measurement noise, and feedback control. Each dataset was corrupted with missing values in 20% of records (lightly corrupted) and in 90% of records (heavily corrupted). For both lightly and heavily corrupted datasets, the root mean squared error of prediction for VAE imputation was lower than the traditional methods. Possibilities for the extension of missing data imputation to inferential sensing are discussed.},
   author = {John T. McCoy and Steve Kroon and Lidia Auret},
   doi = {10.1016/j.ifacol.2018.09.406},
   issn = {24058963},
   issue = {21},
   journal = {IFAC-PapersOnLine},
   keywords = {Machine Learning,Missing Data Imputation,Variational Autoencoder},
   pages = {141-146},
   publisher = {Elsevier B.V.},
   title = {Variational Autoencoders for Missing Data Imputation with Application to a Simulated Milling Circuit},
   volume = {51},
   url = {https://doi.org/10.1016/j.ifacol.2018.09.406},
   year = {2018}
}
@book{Gelman2006,
   abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/~gelman/arm/},
   author = {Andrew Gelman and Jennifer Hill},
   doi = {10.1017/cbo9780511790942},
   journal = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
   publisher = {Cambridge University Press},
   title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
   year = {2006}
}
@inproceedings{Aljuaid2017,
   abstract = {Data mining requires a pre-processing task in which the data are prepared and cleaned for ensuring the quality. Missing value occurs when no data value is stored for a variable in an observation. This has a significant effect on the results especially when it leads to biased parameter estimates. It will not only diminish the quality of the result, but also disqualify for analysis purposes. Hence there are risks associated with missing values in a dataset. Imputation is a technique of replacing missing data with substituted values. This research presents a comparison of imputation techniques such as Mean\Mode, K-Nearest Neighbor, Hot-Deck, Expectation Maximization and C5.0 for missing data. The choice of proper imputation method is based on datatypes, missing data mechanisms, patterns and methods. Datatype can be numerical, categorical or mixed. Missing data mechanism can be missing completely at random, missing at random, or not missing at random. Patterns of missing data can be with respect to cases or attributes. Methods can be a pre-replace or an embedded method. These five imputation techniques are used to impute artificially created missing data from different data sets of varying sizes. The performance of these techniques are compared based on the classification accuracy and the results are presented.},
   author = {Tahani Aljuaid and Sreela Sasi},
   doi = {10.1109/ICDSE.2016.7823957},
   isbn = {9781509012800},
   booktitle = {Proceedings of the 2016 International Conference on Data Science and Engineering, ICDSE 2016},
   keywords = {C5.0,Data Pre-processing,Decision Tree classification,Expectation Maximization,HotDeck,K-Nearest Neighbor},
   month = {1},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Proper imputation techniques for missing values in data sets},
   year = {2017}
}
@misc{,
   title = {Mixed Models: Theory and Applications with R, 2nd Edition | Wiley},
   url = {https://www.wiley.com/en-us/Mixed+Models%3A+Theory+and+Applications+with+R%2C+2nd+Edition-p-9781118091579}
}
@book{Demidenko1987,
   abstract = {Praise for the First Edition "This book will serve to greatly complement the growing number of texts dealing with mixed models, and I highly recommend including it in one's personal library." -Journal of the American Statistical Association Mixed modeling is a crucial area of statistics, enabling the analysis of clustered and longitudinal data. Mixed Models: Theory and Applications with R, Second Edition fills a gap in existing literature between mathematical and applied statistical books by presenting a powerful examination of mixed model theory and application with special attention given to the implementation in R. The new edition provides in-depth mathematical coverage of mixed models' statistical properties and numerical algorithms, as well as nontraditional applications, such as regrowth curves, shapes, and images. The book features the latest topics in statistics including modeling of complex clustered or longitudinal data, modeling data with multiple sources of variation, modeling biological variety and heterogeneity, Healthy Akaike Information Criterion (HAIC), parameter multidimensionality, and statistics of image processing. Mixed Models: Theory and Applications with R, Second Edition features unique applications of mixed model methodology, as well as: Comprehensive theoretical discussions illustrated by examples and figures Over 300 exercises, end-of-section problems, updated data sets, and R subroutines Problems and extended projects requiring simulations in R intended to reinforce material Summaries of major results and general points of discussion at the end of each chapter Open problems in mixed modeling methodology, which can be used as the basis for research or PhD dissertations Ideal for graduate-level courses in mixed statistical modeling, the book is also an excellent reference for professionals in a range of fields, including cancer research, computer science, and engineering.},
   author = {Eugene Demidenko},
   city = {Hoboken, NJ, USA},
   doi = {10.1002/9781118651537},
   isbn = {9781118651537},
   journal = {Mixed Models: Theory and Applications with R: Second Edition},
   month = {1},
   pages = {1-717},
   publisher = {John Wiley \& Sons, Inc.},
   title = {Mixed Models},
   url = {http://doi.wiley.com/10.1002/9781118651537},
   year = {1987}
}
@article{Mitre-Hernandez2022,
   abstract = {<p>Knowing the difficulty of a given task is crucial for improving the learning outcomes. This paper studies the difficulty level classification of memorization tasks from pupillary response data. Developing a difficulty level classifier from pupil size features is challenging because of the inter-subject variability of pupil responses. Eye-tracking data used in this study was collected while students solved different memorization tasks divided as low-, medium-, and high-level. Statistical analysis shows that values of pupillometric features (as peak dilation, pupil diameter change, and suchlike) differ significantly for different difficulty levels. We used a wrapper method to select the pupillometric features that work the best for the most common classifiers; Support Vector Machine (SVM), Decision Tree (DT), Linear Discriminant Analysis (LDA), and Random Forest (RF). Despite the statistical difference, experiments showed that a random forest classifier trained with five features obtained the best F1-score (82%). This result is essential because it describes a method to evaluate the cognitive load of a subject performing a task using only pupil size features.</p>},
   author = {Hugo Mitre-Hernandez and Jorge Sanchez-Rodriguez and Sergio Nava-Muñoz and Carlos Lara-Alvarez},
   doi = {10.7717/PEERJ.12864},
   issn = {2167-8359},
   journal = {PeerJ},
   keywords = {Classifiers,Cognitive load,Pupil size,Science and Medical Education,Subjects Neuroscience,Working memory},
   month = {3},
   pages = {e12864},
   publisher = {PeerJ Inc.},
   title = {Classifying the difficulty levels of working memory tasks by using pupillary response},
   volume = {10},
   url = {https://peerj.com/articles/12864},
   year = {2022}
}
@article{,
   abstract = {We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.},
   author = {Marek´smieja Marek´ Marek´smieja and Łukasz Struski and Jacek Tabor and Bartosz Zielí and Przemysław Spurek},
   title = {Processing of missing data by neural networks}
}
@article{,
   abstract = {Two techniques have emerged from the recent literature as candidate solutions to the problem of missing data imputation, and these are the Expectation Maximisation (EM) Algorithm and the auto-associative Neural Networks and Genetic Algorithms combination. Both these techniques have been discussed individually and their merits discussed at length in the available literature, but up to this point, they have not been compared with each other. This paper provides this comparison, using data sets of an industrial power plant, an industrial winding process and HIV sero-prevalence survey data. Results show that Expectation Maximization is suitable and performs better in cases where there is little or no interdependency between the input variables, whereas the auto-associative neural network and genetic algorithm combination is suitable when there is some inherent non-linear relationships between some of the given variables. 2},
   author = {Fulufhelo V Nelwamondo and Shakir Mohamed and Tshilidzi Marwala},
   title = {Missing Data: A Comparison of Neural Network and Expectation Maximisation Techniques}
}
@article{Smieja2018,
   abstract = {We propose a general, theoretically justified mechanism for processing
missing data by neural networks. Our idea is to replace typical neuron's
response in the first hidden layer by its expected value. This approach can be
applied for various types of networks at minimal cost in their modification.
Moreover, in contrast to recent approaches, it does not require complete data
for training. Experimental results performed on different types of
architectures show that our method gives better results than typical imputation
strategies and other methods dedicated for incomplete data.},
   author = {Marek Smieja and Łukasz Struski and Jacek Tabor and Bartosz Zielinski and Przemysław Spurek},
   doi = {10.48550/arxiv.1805.07405},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {5},
   pages = {2719-2729},
   publisher = {Neural information processing systems foundation},
   title = {Processing of missing data by neural networks},
   volume = {2018-December},
   url = {https://arxiv.org/abs/1805.07405v3},
   year = {2018}
}
@article{Sharpe1995,
   abstract = {Backpropagation neural networks have been applied to prediction and classification problems in many real world situations. However, a drawback of this type of neural network is that it requires a full set of input data, and real world data is seldom complete. We have investigated two ways of dealing with incomplete data — network reduction using multiple neural network classifiers, and value substitution using estimated values from predictor networks — and compared their performance with an induction method. On a thyroid disease database collected in a clinical situation, we found that the network reduction method was superior. We conclude that network reduction can be a useful method for dealing with missing values in diagnostic systems based on backpropagation neural networks.},
   author = {P. K. Sharpe and R. J. Solly},
   doi = {10.1007/BF01421959},
   issn = {1433-3058},
   issue = {2},
   journal = {Neural Computing \& Applications 1995 3:2},
   keywords = {Artificial Intelligence,Computational Biology/Bioinformatics,Computational Science and Engineering,Data Mining and Knowledge Discovery,Image Processing and Computer Vision,Probability and Statistics in Computer Science},
   month = {6},
   pages = {73-77},
   publisher = {Springer},
   title = {Dealing with missing values in neural network-based diagnostic systems},
   volume = {3},
   url = {https://link.springer.com/article/10.1007/BF01421959},
   year = {1995}
}
@article{Khan2022,
   abstract = {The presence of missing data is a challenging issue in processing real-world datasets. It is necessary to improve the data quality by imputing the missing values so that effective learning from data can be achieved. Recently, deep learning has become the most powerful type of machine learning techniques, which can be used for discovering the hidden knowledge that exists in a large dataset to make accurate predictions. In this paper, we propose an imputation method that involves using a convolutional neural network to impute the missing values. The missing value of each instance is imputed essentially by using a trained kernel. The weights of the kernel are determined by learning from the given data that are arranged spatially in the data matrix. The kernel carries out a weighted sum of neighboring elements in an array for imputing the missing values. In addition, in the absence of the true values with which the missing values are expected to be replaced, a loss function is designed without the need to know the true value. Our method is evaluated on UCI datasets in comparison with state-of-the-art methods. The experimental results show that the proposed approach performs closely to or better than other methods.},
   author = {Hufsa Khan and Xizhao Wang and Han Liu},
   doi = {10.1016/J.INS.2022.02.051},
   issn = {0020-0255},
   journal = {Information Sciences},
   keywords = {Convolutional neural network,Data imputation,Fuzzy clustering,Missing value},
   month = {5},
   pages = {278-293},
   publisher = {Elsevier},
   title = {Handling missing data through deep convolutional neural network},
   volume = {595},
   year = {2022}
}
@article{Stefnsson1997,
   abstract = {This note describes a simple technique, the Gamma (or Near Neighbour) test, which in many cases can be used to considerably simplify the design process of constructing a smooth data model such as a neural network. The Gamma test is a data analysis routine, that (in an optimal implementation) runs in time O(MlogM)as M→∞,where Mis the number of sample data points, and which aims to estimate the best Mean Squared Error (MSError) that can be achieved by any continuous or smooth (bounded first partial derivatives) data model constructed using the data.},
   author = {Aoalbjörn Stefánsson and N. Končar and Antonia J. Jones},
   doi = {10.1007/BF01413858},
   issn = {1433-3058},
   issue = {3},
   journal = {Neural Computing \& Applications 1997 5:3},
   keywords = {Artificial Intelligence,Computational Biology/Bioinformatics,Computational Science and Engineering,Data Mining and Knowledge Discovery,Image Processing and Computer Vision,Probability and Statistics in Computer Science},
   pages = {131-133},
   publisher = {Springer},
   title = {A note on the Gamma test},
   volume = {5},
   url = {https://link.springer.com/article/10.1007/BF01413858},
   year = {1997}
}
@article{Evans2002,
   abstract = {From a dataset of inputoutput vectors, the Gamma test estimates the variance of the noise on an output modulo any smooth model with bounded partial derivatives. We present a proof of the Gamma test...},
   author = {Dafydd Evans and Antonia J. Jones},
   doi = {10.1098/RSPA.2002.1010},
   issn = {13645021},
   issue = {2027},
   journal = {Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
   keywords = {Gamma test,nearneighbour distributions,nonlinear modelling,point processes},
   month = {11},
   pages = {2759-2799},
   publisher = {
				The Royal Society
			},
   title = {A proof of the Gamma test},
   volume = {458},
   url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2002.1010},
   year = {2002}
}
@article{Dietterich1998,
   abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 × 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
   author = {Thomas G. Dietterich},
   doi = {10.1162/089976698300017197},
   issn = {0899-7667},
   issue = {7},
   journal = {Neural Computation},
   month = {10},
   pages = {1895-1923},
   pmid = {9744903},
   publisher = {MIT Press},
   title = {Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms},
   volume = {10},
   url = {https://direct.mit.edu/neco/article/10/7/1895/6224/Approximate-Statistical-Tests-for-Comparing},
   year = {1998}
}
@article{Garcia2008,
   abstract = {In a recently published paper in JMLR, Demša (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. © 2008 Salvador García and Francisco Herrera.},
   author = {Salvador García and Francisco Herrera},
   issn = {15324435},
   journal = {Journal of Machine Learning Research},
   keywords = {Adjusted p-values,Logically related hypotheses,Multiple comparisons tests,Non-parametric test,Statistical methods},
   pages = {2677-2694},
   title = {An extension on "statistical comparisons of classifiers over multiple data sets" for all pairwise comparisons},
   volume = {9},
   year = {2008}
}
@article{Patil2021,
   abstract = {Graphical displays can reveal problems in a statistical model that might not be apparent from purely numerical summaries. Such visualizations can also be helpful for the reader to evaluate the validity of a model if it is reported in a scholarly publication or report. But, given the onerous costs involved, researchers often avoid preparing information-rich graphics and exploring several statistical approaches or tests available. The ggstatsplot package in the R programming language (R Core Team, 2021) provides a one-line syntax to enrich ggplot2-based visualizations with the results from statistical analysis embedded in the visualization itself. In doing so, the package helps researchers adopt a rigorous, reliable, and robust data exploratory and reporting workflow.},
   author = {Indrajeet Patil},
   doi = {10.21105/JOSS.03167},
   issn = {2475-9066},
   issue = {61},
   journal = {Journal of Open Source Software},
   month = {5},
   pages = {3167},
   publisher = {The Open Journal},
   title = {Visualizations with statistical details: The 'ggstatsplot' approach},
   volume = {6},
   url = {https://joss.theoj.org/papers/10.21105/joss.03167},
   year = {2021}
}
@article{Patil2021,
   abstract = {The `statsExpressions` package is designed to facilitate producing dataframes with rich statistical details for the most common types of statistical approaches and tests: parametric,
nonparametric, robust, and Bayesian t-test, one-way ANOVA, correlation analyses, contingency table analyses, and meta-analyses. The functions are pipe-friendly and provide a consistent syntax to work with tidy data. These dataframes additionally contain expressions with statistical details, and can be used in graphing packages to display these details.},
   author = {Indrajeet Patil},
   doi = {10.21105/JOSS.03236},
   issn = {2475-9066},
   issue = {61},
   journal = {Journal of Open Source Software},
   month = {5},
   pages = {3236},
   publisher = {The Open Journal},
   title = {statsExpressions: R Package for Tidy Dataframes and Expressions with Statistical Details},
   volume = {6},
   url = {https://joss.theoj.org/papers/10.21105/joss.03236},
   year = {2021}
}
@article{Cohen1960,
   abstract = {A coefficient of interjudge agreement for nominal scales, formula-omitted, is presented. It is directly interpretable as the pro-portion of joint judgments in which there is agreement, after chance agreement is excluded. Its upper limit is +1.00, and its lower limit falls between zero and -1.00, depending on the distribution of judgments by the two judges. The maximum value which x can take for any given problem is given, and the implications of this value to the question of agreement discussed. An interesting characteristic of x is its identity with 0 in the dichotomous case when the judges give the same marginal distributions. Finally, its standard error and techniques for estimation and hypothesis testing are presented. © 1960, Sage Publications. All rights reserved.},
   author = {Jacob Cohen},
   doi = {10.1177/001316446002000104},
   issn = {15523888},
   issue = {1},
   journal = {Educational and Psychological Measurement},
   pages = {37-46},
   title = {A Coefficient of Agreement for Nominal Scales},
   volume = {20},
   year = {1960}
}
@article{Leijnen2020,
   abstract = {An overview of neural network architectures is presented. Some of these architectures have been created in recent years, whereas others originate from many decades ago. Apart from providing a practical tool for comparing deep learning models, the Neural Network Zoo also uncovers a taxonomy of network architectures, their chronology, and traces back lineages and inspirations for these neural information processing systems.},
   author = {Stefan Leijnen and Fjodor van Veen},
   doi = {10.3390/proceedings47010009},
   issue = {1},
   journal = {Proceedings},
   keywords = {artificial intelligence,connectionism,deep learning,neural information processing,neural network architectures,neural networks},
   pages = {9},
   title = {The Neural Network Zoo},
   volume = {47},
   year = {2020}
}
@article{Mitre-Hernandez2022,
   abstract = {Knowing the difficulty of a given task is crucial for improving the learning outcomes. This paper studies the difficulty level classification of memorization tasks from pupillary response data. Developing a difficulty level classifier from pupil size features is challenging because of the inter-subject variability of pupil responses. Eye-tracking data used in this study was collected while students solved different memorization tasks divided as low-, medium-, and high-level. Statistical analysis shows that values of pupillometric features (as peak dilation, pupil diameter change, and suchlike) differ significantly for different difficulty levels. We used a wrapper method to select the pupillometric features that work the best for the most common classifiers; Support Vector Machine (SVM), Decision Tree (DT), Linear Discriminant Analysis (LDA), and Random Forest (RF). Despite the statistical difference, experiments showed that a random forest classifier trained with five features obtained the best F1-score (82%). This result is essential because it describes a method to evaluate the cognitive load of a subject performing a task using only pupil size features.},
   author = {Hugo Mitre-Hernandez and Jorge Sanchez-Rodriguez and Sergio Nava-Muñoz and Carlos Lara-Alvarez},
   doi = {10.7717/PEERJ.12864/TABLE-4},
   issn = {21678359},
   journal = {PeerJ},
   keywords = {Classifiers,Cognitive load,Pupil size,Working memory},
   month = {3},
   pages = {e12864},
   publisher = {PeerJ Inc.},
   title = {Classifying the difficulty levels of working memory tasks by using pupillary response},
   volume = {10},
   url = {https://peerj.com/articles/12864},
   year = {2022}
}
@article{Rodrguez-Gonzlez2020,
   abstract = {This issue depicts the first results of research in car making companies in the State of Aguascalientes, México.The sort of research carried was qualitative and exploratory. The aim of it is to propose a methodology or a model of technological innovation which integrates the best practices that can comply the engineering changes in the supplying chain. Changes of engineering happen due to changes of model in vehicles. The sample subject to studies is not probabilistic or led by the expert sort. The measuring instrument designed for this specific studies is a questionnaire. This questionnaire takes reference from the elements that form a change a change of engineering gathered in 8 core topics which include information about the activities done in the engineering, finance, planning, industrial engineering, suppliers development and production departments; with a total t 130 activities or items that are the results of the research. Two categories or levels are evaluated. The first one measures the Likert to a level of fulfillment of each of the activities developed during a change of engineering. Then, the second one measures the scale of Likert to a level of importance considered by the expert. The results shown in this issue are unique, given the fact that the topic has not been profoundly studied. The data gotten from the studies belong to the level of fulfillment of the activities that are executed in a change of engineering in 24 car making companies in the State of Aguascalientes. The car making companies involved in this research were assembly companies, direct and indirect suppliers known as Level One or Tier One Suppliers and Level Two or Tier Two Suppliers. These companies perform activities that are mostly described in the questionnaire in a range of evaluated scale, which allows this research to be continued and opens room to other researchers to continue exploring about this topic. These activities are definitely important because big part of the planning, execution and communication among the departments involved depend on it, given the fact that those automotive companies which are willing to adopt this methodology will have more possiblity to accomplish their deliveries on time and assure the quality of their products to their costumers. At the same time, having more earnings and making more business forthese companies as well as it will benefit the State of Aguascalientes and its industry.},
   author = {Marcela Rodríguez-González and Jonás Velasco-Alvarez and Efrain Flores-Figueroa and Ma. Antonieta Zuloaga-Garmendia and Sergio Martín Nava-Muñoz and Ana María Velázquez-González},
   doi = {10.29057/ICBI.V7I14.4989},
   issn = {2007-6363},
   issue = {14},
   journal = {Pädi Boletín Científico de Ciencias Básicas e Ingenierías del ICBI},
   keywords = {Aguascalientes,Arquitectura,Biología,Cambios de ingeniería,Ciencias Computacionales,Física y Tecnología Avanzada,Ingeniería Civil,Ingeniería Industrial,Ingeniería Minero Metalúrgica,Ingeniería en Ciencia de Materiales,Ingeniería en Electrónica,Ingeniería en Geología Ambiental,Ingeniería en Telecomunicaciones,Matemáticas Aplicadas,Química,Química en Alimentos,cadena de suministro,industria automotriz,metodología},
   month = {1},
   pages = {98-106},
   publisher = {Universidad Autonoma del Estado de Hidalgo},
   title = {Metodología para realizar  cambios de ingeniería en la cadena de suministro de la industria automotriz en el estado de Aguascalientes, México},
   volume = {7},
   url = {https://repository.uaeh.edu.mx/revistas/index.php/icbi/article/view/4989},
   year = {2020}
}
@article{Plaza-del-Arco2021,
   abstract = {This paper is an overview of MeOﬀendES 2021, organized at IberLEF 2021 and co-located with the 37th International Conference of the Spanish Society for Natural Language Processing (SEPLN 2021). The main purpose of MeOﬀendEs is to promote research on the detection of oﬀensive language in Spanish variants. The shared task involve four subtasks, the ﬁrst two correspond to the identiﬁcation of oﬀensive language categories in generic Spanish texts from diﬀerent social media platforms, while subtasks 3 and 4 are related to the identiﬁcation of oﬀensive langua-ge targeting the Mexican variant of Spanish. Two annotated datasets on oﬀensive language have been released to the Natural Language Processing community. MeOf-fendes attracted a large number of participants: a total of 69 signed up to participate in the task, 12 submitted oﬃcial runs on the test data, and 10 submitted system description papers. Corpora and results are available at the shared task website at https://competitions.codalab.org/competitions/28679.},
   author = {Flor Miriam Plaza-del-Arco and Marco Casavantes and Hugo Jair Escalante and M. Teresa Martín-Valdivia and Arturo Montejo-Ráez and Manuel Montes-y-Gómez and Horacio Jarquín-Vásquez and Luis Villaseñor-Pineda},
   doi = {10.26342/2021-67-16},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {Palabras clave: MeOffendEs,natural language pro-cessing,offensive language detection,text classification},
   month = {9},
   pages = {183-194},
   title = {Overview of MeOffendEs at IberLEF 2021: Offensive Language Detection in Spanish Variants},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6388},
   year = {2021}
}
@book{Efron1994,
   abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
   author = {Bradley Efron and R.J. Tibshirani},
   doi = {10.1201/9780429246593/INTRODUCTION-BOOTSTRAP-BRADLEY-EFRON-TIBSHIRANI},
   journal = {An Introduction to the Bootstrap},
   month = {5},
   publisher = {Chapman and Hall/CRC},
   title = {An Introduction to the Bootstrap},
   year = {1994}
}
@book{Chernick2011,
   abstract = {"This book provides both an elementary and a modern introduction to the bootstrap for students who do not have an extensive background in advanced mathematics. It offers reliable, hands-on coverage of the bootstrap's considerable advantages -- as well as its drawbacks. The book outpaces the competition by skillfully presenting results on improved confidence set estimation, estimation of error rates in discriminant analysis, and applications to a wide variety of hypothesis testing and estimation problems. To alert readers to the limitations of the method, the book exhibits counterexamples to the consistency of bootstrap methods. The authors take great care to draw connections between the more traditional resampling methods and the bootstrap, oftentimes displaying helpful computer routines in R. Emphasis throughout the book is on the use of the bootstrap as an exploratory tool including its value in variable selection and other modeling environments"--},
   author = {Michael R. Chernick and Robert A. LaBudde},
   isbn = {978-0-470-46704-6},
   publisher = {Wiley},
   title = {An introduction to bootstrap methods with applications to R},
   url = {https://www.wiley.com/en-us/An+Introduction+to+Bootstrap+Methods+with+Applications+to+R-p-9780470467046},
   year = {2011}
}
@article{Efron1979,
   abstract = {We discuss the following problem: given a random sample $\mathbf\{X\} = (X_1, X_2, \cdots, X_n)$ from an unknown probability distribution $F$, estimate the sampling distribution of some prespecified random variable $R(\mathbf\{X\}, F)$, on the basis of the observed data $\mathbf\{x\}$. (Standard jackknife theory gives an approximate mean and variance in the case $R(\mathbf\{X\}, F) = \theta(\hat\{F\}) - \theta(F), \theta$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
   author = {B. Efron},
   doi = {10.1214/AOS/1176344552},
   issn = {0090-5364},
   issue = {1},
   journal = {https://doi.org/10.1214/aos/1176344552},
   keywords = {62G05,62G15,62H30,62J05,Nonlinear regression,Resampling,bootstrap,discriminant analysis,error rate estimation,jackknife,nonparametric variance estimation,subsample values},
   month = {1},
   pages = {1-26},
   publisher = {Institute of Mathematical Statistics},
   title = {Bootstrap Methods: Another Look at the Jackknife},
   volume = {7},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at},
   year = {1979}
}
@article{Raschka2018,
   abstract = {The correct use of model evaluation, model selection, and algorithm selection
techniques is vital in academic machine learning research as well as in many
industrial settings. This article reviews different techniques that can be used
for each of these three subtasks and discusses the main advantages and
disadvantages of each technique with references to theoretical and empirical
studies. Further, recommendations are given to encourage best yet feasible
practices in research and applications of machine learning. Common methods such
as the holdout method for model evaluation and selection are covered, which are
not recommended when working with small datasets. Different flavors of the
bootstrap technique are introduced for estimating the uncertainty of
performance estimates, as an alternative to confidence intervals via normal
approximation if bootstrapping is computationally feasible. Common
cross-validation techniques such as leave-one-out cross-validation and k-fold
cross-validation are reviewed, the bias-variance trade-off for choosing k is
discussed, and practical tips for the optimal choice of k are given based on
empirical evidence. Different statistical tests for algorithm comparisons are
presented, and strategies for dealing with multiple comparisons such as omnibus
tests and multiple-comparison corrections are discussed. Finally, alternative
methods for algorithm selection, such as the combined F-test 5x2
cross-validation and nested cross-validation, are recommended for comparing
machine learning algorithms when datasets are small.},
   author = {Sebastian Raschka},
   doi = {10.48550/arxiv.1811.12808},
   month = {11},
   title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
   url = {https://arxiv.org/abs/1811.12808v3},
   year = {2018}
}
@inproceedings{Koehn2004,
   abstract = {If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.},
   author = {Philipp Koehn},
   booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004 - A meeting of SIGDAT, a Special Interest Group of the ACL held in conjunction with ACL 2004},
   pages = {388-395},
   title = {Statistical Significance Tests for Machine Translation Evaluation},
   url = {https://aclanthology.org/W04-3250},
   year = {2004}
}
@inproceedings{Zhang2004,
   author = {Ying Zhang and Stephan Vogel and Alex Waibel},
   booktitle = {Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC 2004},
   pages = {2051-2054},
   title = {Interpreting BLEU/NIST Scores: How Much Improvement do We Need to Have a Better System?},
   url = {http://www.lrec-conf.org/proceedings/lrec2004/pdf/755.pdf},
   year = {2004}
}
@article{Gillick1989,
   abstract = {The authors present two simple tests for deciding whether the difference in error rates between two algorithms tested on the same data set is statistically significant. The first (McNemar's test) requires the errors made by an algorithm to be independent events and is found to be most appropriate for isolated-word algorithms. The second (a matched-pairs test) can be used even when errors are not independent events and is more appropriate for connected speech.},
   author = {L. Gillick and S. J. Cox},
   doi = {10.1109/ICASSP.1989.266481},
   issn = {07367791},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   pages = {532-535},
   publisher = {Publ by IEEE},
   title = {Some statistical issues in the comparison of speech recognition algorithms},
   volume = {1},
   url = {https://www.researchgate.net/publication/3548274_Some_statistical_issues_in_the_comparison_of_speech_recognition_algorithms},
   year = {1989}
}
@article{Yeh2000,
   abstract = {Statistical significance testing of differences in values of metrics like recall, precision and balanced F-score is a necessary part of empirical natural language processing. Unfortunately, we find in a set of experiments that many commonly used tests often underestimate the significance and so are less likely to detect differences that exist between different techniques. This underestimation comes from an independence assumption that is often violated. We point out some useful tests that do not make this assumption, including computationally-intensive randomization tests.},
   author = {Alexander Yeh},
   doi = {10.3115/992730.992783},
   pages = {947},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {More accurate tests for the statistical significance of result differences},
   url = {https://dl.acm.org/doi/10.3115/992730.992783},
   year = {2000}
}
@article{Bisani2004,
   abstract = {The field of speech recognition has clearly benefited from precisely defined testing conditions and objective performance measures such as word error rate. In the development and evaluations of new methods, the question arises whether the empirically observed difference in performance is due to a genuine advantage of one system over the other, or just an effect of chance. However still many publications do not concern themselves with the statistical significance of the results reported. In this paper we present a bootstrap method for significance analysis which is at the same time intuitive, precise and and easy to use. Unlike some methods, we make no (possibly ill-founded) approximations and the results are immediately interpretable in terms of word error rate.},
   author = {M. Bisani and H. Ney},
   doi = {10.1109/ICASSP.2004.1326009},
   issn = {15206149},
   journal = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
   title = {Bootstrap estimates for confidence intervals in ASR performance evaluation},
   volume = {1},
   year = {2004}
}
@misc{Riezler2005,
   abstract = {We investigate some pitfalls regarding the discriminatory power of MT evaluation metrics and the accuracy of statistical significance tests. In a discriminative rerank-ing experiment for phrase-based SMT we show that the NIST metric is more sensitive than BLEU or F-score despite their incorporation of aspects of fluency or meaning adequacy into MT evaluation. In an experimental comparison of two statistical significance tests we show that p-values are estimated more conservatively by approximate randomization than by boot-strap tests, thus increasing the likelihood of type-I error for the latter. We point out a pitfall of randomly assessing significance in multiple pairwise comparisons, and conclude with a recommendation to combine NIST with approximate random-ization, at more stringent rejection levels than is currently standard.},
   author = {Stefan Riezler and John T. Maxwell III},
   pages = {57-64},
   title = {On Some Pitfalls in Automatic Evaluation and Significance Testing for MT},
   url = {https://aclanthology.org/W05-0908},
   year = {2005}
}
@article{Dang2008,
   author = {H. Dang and Karolina Owczarzak},
   journal = {Theory and Applications of Categories},
   title = {Overview of the TAC 2008 Update Summarization Task},
   year = {2008}
}
@inproceedings{Berg-Kirkpatrick2012,
   abstract = {We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. © 2012 Association for Computational Linguistics.},
   author = {Taylor Berg-Kirkpatrick and David Burkett and Dan Klein},
   booktitle = {EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference},
   title = {An empirical investigation of statistical significance in NLP},
   year = {2012}
}
@inproceedings{Och2003,
   abstract = {Often, the training procedure for statisti- cal machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation qual- ity. These training criteria make use of re- cently proposed automatic evaluation met- rics. We describe a new algorithm for effi- cient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evalua- tion criterion is taken directly into account as part of the training procedure.},
   author = {Franz Josef Och},
   doi = {10.3115/1075096.1075117},
   title = {Minimum error rate training in statistical machine translation},
   year = {2003}
}
@article{Labatut2012,
   abstract = {The selection of the best classification algorithm for a given dataset is a
very widespread problem. It is also a complex one, in the sense it requires to
make several important methodological choices. Among them, in this work we
focus on the measure used to assess the classification performance and rank the
algorithms. We present the most popular measures and discuss their properties.
Despite the numerous measures proposed over the years, many of them turn out to
be equivalent in this specific case, to have interpretation problems, or to be
unsuitable for our purpose. Consequently, classic overall success rate or
marginal rates should be preferred for this specific task.},
   author = {Vincent Labatut and Hocine Cherifi},
   doi = {10.48550/arxiv.1207.3790},
   keywords = {Accuracy Measure,Classification,Classifier Comparison},
   month = {7},
   title = {Accuracy Measures for the Comparison of Classifiers},
   url = {https://arxiv.org/abs/1207.3790v1},
   year = {2012}
}
@article{Pavao2022,
   abstract = {CodaLab Competitions is an open source web platform designed to help data scientists and research teams to crowd-source the resolution of machine learning problems through the organization of competitions, also called challenges or contests. CodaLab Competitions provides useful features such as multiple phases, results and code submissions, multi-score leaderboards, and jobs running inside Docker containers. The platform is very flexible and can handle large scale experiments, by allowing organizers to upload large datasets and provide their own CPU or GPU compute workers.},
   author = {Adrien Pavao and Isabelle Guyon and Anne-Catherine Letournel and Xavier Baró and Hugo Escalante and Sergio Escalera and Tyler Thomas and Zhen Xu},
   doi = {10.13039/501100001665},
   month = {4},
   publisher = {Université Paris-Saclay, FRA.},
   title = {CodaLab Competitions: An open source platform to organize scientific challenges},
   url = {https://hal.inria.fr/hal-03629462 https://hal.inria.fr/hal-03629462/document},
   year = {2022}
}
@article{Søgaard2014,
   abstract = {In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rank- or randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cut-off at ~0.0025 to reduce the risk of false positive results to <5%. Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.},
   author = {Anders Søgaard and Anders Johannsen and Barbara Plank and Dirk Hovy and Hector Martinez},
   doi = {10.3115/V1/W14-1601},
   isbn = {9781941643020},
   journal = {CoNLL 2014 - 18th Conference on Computational Natural Language Learning, Proceedings},
   pages = {1-10},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {What’s in a p-value in NLP?},
   url = {https://aclanthology.org/W14-1601},
   year = {2014}
}
@article{Dror2018,
   abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1},
   author = {Rotem Dror and Gili Baumer and Segev Shlomov and Roi Reichart},
   doi = {10.18653/V1/P18-1128},
   isbn = {9781948087322},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {1383-1392},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing},
   volume = {1},
   url = {https://aclanthology.org/P18-1128},
   year = {2018}
}
@article{Lones2021,
   abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
   author = {Michael A. Lones},
   pages = {1-25},
   title = {How to avoid machine learning pitfalls: a guide for academic researchers},
   url = {http://arxiv.org/abs/2108.02497},
   year = {2021}
}
@article{Holm1979,
   abstract = {This paper presents a simple and widely ap-plicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a tine until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
   author = {S Holm},
   issue = {2},
   journal = {Scandinavian journal of statistics},
   keywords = {exact definition,multiple test,simultaneous test,the methodological motivation and},
   pages = {65-70},
   title = {A simple sequentially rejective multiple test procedure},
   volume = {6},
   url = {http://www.jstor.org/stable/4615733},
   year = {1979}
}
@article{YoavBenjamini1995,
   abstract = {The commonapproachto themultiplicityproblemcalls forcontrollingthefamilywise errorrate(FWER). Thisapproach,though,hasfaults,andwepointouta few.A different approachto problemsofmultiplesignificancetestingis presented.Itcallsforcontrolling theexpectedproportionoffalselyrejectedhypotheses -the falsediscoveryrate.Thiserror rateisequivalenttotheFWER whenallhypotheses aretruebutissmallerotherwise.Therefore,in problemswherethecontrolof thefalsediscoveryrateratherthanthatof the FWER is desired,thereis potentialfora gaininpower.A simplesequentialBonferronitypeprocedureisprovedto controlthefalsediscoveryrateforindependentteststatistics, and a simulationstudyshowsthatthegainin poweris substantial.The use of thenew procedureand theappropriatenessofthecriterionareillustratedwithexamples},
   author = {Yoav Benjamini and Yosef Hochberg},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
   keywords = {icle},
   pages = {289-300},
   title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
   volume = {57},
   year = {1995}
}
@article{Benjamini2001,
   abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate $t$. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
   author = {Yoav Benjamini and Daniel Yekutieli},
   doi = {10.1214/AOS/1013699998},
   issn = {0090-5364},
   issue = {4},
   journal = {https://doi.org/10.1214/aos/1013699998},
   keywords = {47N30,62G30,62J15,FDR,Hochberg’s procedure,MTP2 densities,Multiple comparisons procedures,Simes’equality,comparisons with control,discrete test statistics,multiple endpoints many-to-one comparisons,positive regression dependency,unidimensional latent variables},
   month = {8},
   pages = {1165-1188},
   publisher = {Institute of Mathematical Statistics},
   title = {The control of the false discovery rate in multiple testing under dependency},
   volume = {29},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-4/The-control-of-the-false-discovery-rate-in-multiple-testing/10.1214/aos/1013699998.full https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-4/The-control-of-the-false-discovery-rate-in-multiple-testing/10.1214/aos/1013699998.short},
   year = {2001}
}
@article{Cox1994,
   abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
   author = {DR Cox and DV Hinkley and N Reid and DB Rubin and BW Silverman},
   doi = {10.1201/9780429246593},
   isbn = {9780429246593},
   issue = {2},
   journal = {An Introduction to the Bootstrap},
   month = {5},
   pages = {32},
   publisher = {Chapman and Hall/CRC},
   title = {An Introduction to the Bootstrap},
   volume = {21},
   url = {https://www.taylorfrancis.com/books/mono/10.1201/9780429246593/introduction-bootstrap-bradley-efron-tibshirani},
   year = {1994}
}
@article{Card2020,
   abstract = {Despite its importance to experimental design, statistical power (the
probability that, given a real effect, an experiment will reject the null
hypothesis) has largely been ignored by the NLP community. Underpowered
experiments make it more difficult to discern the difference between
statistical noise and meaningful model improvements, and increase the chances
of exaggerated findings. By meta-analyzing a set of existing NLP papers and
datasets, we characterize typical power for a variety of settings and conclude
that underpowered experiments are common in the NLP literature. In particular,
for several tasks in the popular GLUE benchmark, small test sets mean that most
attempted comparisons to state of the art models will not be adequately
powered. Similarly, based on reasonable assumptions, we find that the most
typical experimental design for human rating studies will be underpowered to
detect small model differences, of the sort that are frequently studied. For
machine translation, we find that typical test sets of 2000 sentences have
approximately 75% power to detect differences of 1 BLEU point. To improve the
situation going forward, we give an overview of best practices for power
analysis in NLP and release a series of notebooks to assist with future power
analyses.},
   author = {Dallas Card and Peter Henderson and Urvashi Khandelwal and Robin Jia and Kyle Mahowald and Dan Jurafsky},
   doi = {10.18653/v1/2020.emnlp-main.745},
   isbn = {9781952148606},
   journal = {EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
   month = {10},
   pages = {9263-9274},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {With Little Power Comes Great Responsibility},
   url = {https://arxiv.org/abs/2010.06595v1},
   year = {2020}
}
@article{Nava-Munoz2023,
   abstract = {In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific...},
   author = {Sergio Nava-Muñoz and Mario Graff Guerrero and Hugo Jair Escalante},
   doi = {10.1007/978-3-031-33783-3_9},
   keywords = {Bootstrap,Challenges,Performance},
   pages = {89-98},
   publisher = {Springer, Cham},
   title = {Comparison of Classifiers in Challenge Scheme},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-33783-3_9},
   year = {2023}
}
@article{Grandini2020,
   abstract = {Classification tasks in machine learning involving more than two classes are
known by the name of "multi-class classification". Performance indicators are
very useful when the aim is to evaluate and compare different classification
models or machine learning techniques. Many metrics come in handy to test the
ability of a multi-class classifier. Those metrics turn out to be useful at
different stage of the development process, e.g. comparing the performance of
two different models or analysing the behaviour of the same model by tuning
different parameters. In this white paper we review a list of the most
promising multi-class metrics, we highlight their advantages and disadvantages
and show their possible usages during the development of a classification
model.},
   author = {Margherita Grandini and Enrico Bagli and Giorgio Visani},
   month = {8},
   publisher = {AN OVERVIEW},
   title = {Metrics for Multi-Class Classification: an Overview},
   url = {https://arxiv.org/abs/2008.05756v1},
   year = {2020}
}
@article{Agerri2021,
   abstract = {This paper describes the VaxxStance task at IberLEF 2021. The task proposes to detect stance in Tweets referring to vaccines, a relevant and controversial topic in the current pandemia. The task is proposed in a multilingual setting, providing data for Basque and Spanish languages. The objective is to explore crosslingual approaches which also complement textual information with contextual features obtained from the social network. The results demonstrate that contextual information is crucial to obtain competitive results, especially across languages.},
   author = {Rodrigo Agerri and Roberto Centeno and María Espinosa and Joseba Fernández de Landa and Álvaro Rodrigo},
   doi = {10.26342/2021-67-15},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {Ciencias Sociales Com-putacionales,Computational Social Science,Extracción de Información,In-formation Extraction Palabras clave: Detección,Multilingualism,Multilingüismo,Stance Detection},
   month = {9},
   pages = {173-181},
   title = {VaxxStance@IberLEF 2021: Overview of the Task on Going Beyond Text in Cross-Lingual Stance Detection},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6387},
   year = {2021}
}
@article{Rodriguez-Sanchez2021,
   abstract = {The paper describes the organization, goals, and results of the sEXism Identiﬁcation in Social neTworks (EXIST) challenge, a shared task proposed for the ﬁrst time at IberLEF 2021. EXIST 2021 proposes two challenges: sexism identiﬁ-cation and sexism categorization of tweets and gabs, both in Spanish and English. We have received a total of 70 runs for the sexism identiﬁcation task and 61 for the sexism categorization challenge, submitted by 31 diﬀerent teams from 11 countries. We present the dataset, the evaluation methodology, an overview of the proposed systems, and the results obtained. The ﬁnal dataset consists of more than 11,000 annotated texts from two social networks (Twitter and Gab) and its development has been supervised and monitored by experts in gender issues.},
   author = {Francisco Rodríguez-Sánchez and Jorge Carrillo-de-Albornoz and Laura Plaza and Julio Gonzalo and Paolo Rosso and Miriam Comet and Trinidad Donoso},
   doi = {10.26342/2021-67-17},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {English Palabras clave: Detección de Sexismo,Español,Gab,Inglés,Sexism Detection,Spanish,Twitter},
   month = {9},
   pages = {195-207},
   title = {Overview of EXIST 2021: sEXism Identification in Social neTworks},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6389},
   year = {2021}
}
@article{Taule2021,
   abstract = {In this paper we present the DETOXIS task, DEtection of TOxicity in comments In Spanish, which took place as part of the IberLEF 2021 Workshop on Iberian Languages Evaluation Forum at the SEPLN 2021 Conference. We describe the NewsCom-TOX dataset used for training and testing the systems, the metrics applied for their evaluation and the results obtained by the submitted approaches. We also provide an error analysis of the results of these systems.},
   author = {Mariona Taulé and Alejandro Ariza and Montserrat Nofre and Enrique Amigó and Paolo Rosso},
   doi = {10.26342/2021-67-18},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {Closeness Evalu-ation Measure,Closeness Evaluation Mea-sure,NewsCom-TOX corpus,NewsCom-TOX corpus Palabras clave: Detección de to,Rank Biased Precision,Toxicity detection},
   month = {9},
   pages = {209-221},
   title = {Overview of DETOXIS at IberLEF 2021: DEtection of TOXicity in comments In Spanish},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6390},
   year = {2021}
}
@article{Bel-Enguix2022,
   abstract = {Paraphrase detection is an important unresolved task in natural language processing; especially in the Spanish language. In order to address this issue, and contribute to the creation of high-performance paraphrase detection automated systems, we propose a shared task called PAR-MEX. For this task, we created a corpus, in Spanish, with topics in the domain of Mexican gastronomy. Afterwards, the participants in this task submitted their classification results on our corpus. In this paper we explain the steps followed for the creation of the corpus, we summarize the results obtained by the various participants, and propose some conclusions regarding the paraphrase-detection task in Spanish.},
   author = {Gemma Bel-Enguix and Gerardo Sierra and Helena Gómez-Adorno and Juan-Manuel Torres-Moreno and Jesus-German Ortiz-Barajas and Juan Vásquez},
   doi = {10.26342/2022-69-22},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {Iberlef,Iberlef Palabras clave: PAR-MEX,PAR-MEX,detección paráfrasis,paraphrase detection},
   month = {9},
   pages = {255-263},
   title = {Overview of PAR-MEX at Iberlef 2022: Paraphrase Detection in Spanish Shared Task},
   volume = {69},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6445},
   year = {2022}
}
@article{Alvarez-Carmona2021,
   abstract = {This paper presents the framework and results from the Rest-Mex track at IberLEF 2021. This track considered two tasks: Recommendation System and Sentiment Analysis, using texts from Mexican touristic places. The Recommendation System task consists in predicting the degree of satisfaction that a tourist may have when recommending a destination of Nayarit, Mexico, based on places visited by the tourists and their opinions. On the other hand, the Sentiment Analysis task predicts the polarity of an opinion issued by a tourist who traveled to the most representative places in Guanajuato, Mexico. For both tasks, we have built new corpora considering Spanish opinions from the TripAdvisor website. This paper compares and discusses the results of the participants for both tasks.},
   author = {Miguel Á. Álvarez-Carmona and Ramón Aranda and Samuel Arce-Cardenas and Daniel Fajardo-Delgado and Rafael Guerrero-Rodríguez and A. Pastor López-Monroy and Juan Martínez-Miranda and Humberto Pérez-Espinosa and Ansel Y. Rodríguez-González},
   doi = {10.26342/2021-67-14},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {Análisis de sentimien-tos,Mexi-can Tourist Text Palabras clave: Rest-Mex 202,Recommendation System,Rest-Mex 2021,Sentiment Analysis,Sistemas de recomendación,Textos Turísticos Mexicanos},
   month = {9},
   pages = {163-172},
   title = {Overview of Rest-Mex at IberLEF 2021: Recommendation System for Text Mexican Tourism},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6386},
   year = {2021}
}
@article{Aragon2019,
   abstract = {This paper presents the framework and results from the MEX-A3T track at IberLEF 2019. This track considers two tasks, author profiling and aggressiveness detection, both of them using Mexican Spanish tweets. The author profiling task consists on determining the gender, occupation and place of residence of users from their tweets. As a novelty in this year’s edition, it considers the use of text and images as information sources, with the aim of studying the relevance and complementarity of multimodal data for profiling social media users. On the other hand, the aggressiveness detection task follows the same design than the previous edition; it aims to discriminate between aggressive and non-aggressive tweets. For both tasks, we have built new corpora considering tweets from Mexican Twitter users. This paper compares and discusses the results of the participants.},
   author = {Mario Ezra Aragón and Miguel Álvarez-Carmona and Manuel Montes-Y-Gómez and Hugo Jair Escalante and Luis Villaseñor-Pineda and Daniela Moctezuma},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   pages = {478-494},
   title = {Overview of MEX-A3T at IberLEF 2019: Authorship and aggressiveness analysis in Mexican Spanish tweets},
   volume = {2421},
   year = {2019}
}
@article{Garcia-Vegaa2020,
   abstract = {The Task on Semantic Analysis at SEPLN (TASS task within IberLEF 2020 workshop) took place on September 22, reaching its ninth edition. Due to the COVID-19 pandemic, the number of participants is lower compared to past campaigns. Also, the organizers decided to held it remotely. In this edition, the classical polarity classification subtask was, again, organized. As a novelty, a second subtask was proposed to foster research in emotion detection of Spanish texts on a new dataset. This paper summarizes the different approaches of the teams who participated, the key insights of their systems and the results obtained for all the proposed solutions.},
   author = {Manuel García-Vega and Manuel Carlos Díaz-Galiano and Miguel García-Cumbreras and Flor Miriam Plaza Del Arco and Arturo Montejo-Ráez and Salud Mariá Jiménez-Zafra and Eugenio Martínez Cámara and César Antonio Aguilar and Marco Antonio Sobrevilla Cabezudo and Luis Chiruzzo and Daniela Moctezuma},
   isbn = {0000000318679},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Opinion mining,Sentiment analysis,Social media},
   pages = {163-170},
   title = {Overview of TASS 2020: Introducing Emotion Detection},
   volume = {2664},
   year = {2020}
}
@article{Aragon2020,
   abstract = {This paper presents the overview of MEX-A3T 2020, the third edition of this lab under the IberLEF conference. The main purpose of MEX-A3T is to explore different methodologies and strategies related to the analysis of social media content in Mexican Spanish. This year edition focuses in the identification of fake news and the detection of aggressive tweets. For this purpose, we provided different news from verified web sources and a corpus of tweets from Mexican users.v.},
   author = {Mario Ezra Aragón and Horacio Jarquín-Vasqueza and Manuel Montes-Y-Gómez and Hugo Jair Escalante and Luis Villasenõr-Pineda and Helena Gómez-Adorno and Juan Pablo Posadas-Durán and Gemma Bel-Enguix},
   issn = {16130073},
   issue = {August},
   journal = {CEUR Workshop Proceedings},
   keywords = {Aggressiveness detection,Fake news detection,Iberlef,Mex-a3t},
   pages = {222-235},
   title = {Overview of mex-a3t at iberlef 2020: Fake news and aggressiveness analysis in mexican Spanish},
   volume = {2664},
   year = {2020}
}
@article{Plaza-Del-Arco2021a,
   abstract = {This paper presents the EmoEvalEs shared task, organized at IberLEF 2021, as part of the 37th International Conference of the Spanish Society for Natural Language Processing (SEPLN 2021). The aim of this task is to promote the Emotion detection and Evaluation for Spanish. It consists of a fine-grained emotion classification of tweets from the EmoEvalEs corpus in one of these seven classes: anger, disgust, fear, joy, sadness, surprise, or others. In this edition, 70 teams registered, 15 submitted results and 11 presented papers describing their systems. Most teams experimented with neural networks, being Transformers the most widely used model. It should be noted that few of them also considered the features of offensiveness and event that were provided in the corpus apart from the tweet texts.},
   author = {Flor Miriam Plaza-Del-Arco and Salud María Jiménez-Zafra and Arturo Montejo-Ráez and M. Dolores Molina-González and L. Alfonso Ureña-López and M. Teresa Martín-Valdivia},
   doi = {10.26342/2021-67-13},
   issn = {19897553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   keywords = {EmoEvalEs,emotion detection,natural language processing},
   month = {9},
   pages = {155-161},
   title = {Overview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021},
   volume = {67},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6385},
   year = {2021}
}
@article{Alvarez-Carmona2022,
   abstract = {This paper presents the framework and results from the Rest-Mex task at IberLEF 2022. This task considered three tracks: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction, using texts from Mexican touristic places. The Recommendation System task consists in predicting the degree of satisfaction that a tourist may have when recommending a destination of Nayarit, Mexico, based on places visited by the tourists and their opinions. On the other hand, the Sentiment Analysis task predicts the polarity of an opinion issued and the attraction by a tourist who traveled to the most representative places in Mexico. We have built corpora for both tasks considering Spanish opinions from the TripAdvisor website. As a novelty, the Covid Semaphore Prediction task aims to predict the color of the Mexican Semaphore for each state, according to the Covid news in the state, using data from the Mexican Ministry of Health. This paper compares and discusses the participants’ results for all three tacks.},
   author = {Miguel Á. Álvarez-Carmona and Ángel Díaz-Pacheco and Ramón Aranda and Ansel Y. Rodríguez-González and Daniel Fajardo-Delgado and Rafael Guerrero-Rodríguez and Lázaro Bustio-Martínez},
   doi = {10.26342/2022-69-26},
   issn = {1989-7553},
   issue = {0},
   journal = {Procesamiento del Lenguaje Natural},
   month = {9},
   pages = {289-299},
   title = {Overview of Rest-Mex at IberLEF 2022: Recommendation System, Sentiment Analysis and Covid Semaphore Prediction for Mexican Tourist Texts},
   volume = {69},
   url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6449},
   year = {2022}
}
@article{Jafari2019,
   abstract = {Currently, numerous papers are published reporting analysis of biological data at different omics levels by making statistical inferences. Of note, many studies, as those published in this Journal, report association of gene(s) at the genomic and transcriptomic levels by undertaking appropriate statistical tests. For instance, genotype, allele or haplotype frequencies at the genomic level or normalized expression levels at the transcriptomic level are compared between the case and control groups using the Chi-square/Fisher’s exact test or independent (i.e. two-sampled) t-test respectively, with this culminating into a single numeric, namely the P value (or the degree of the false positive rate), which is used to make or break the outcome of the association test. This approach has flaws but nevertheless remains a standard and convenient approach in association studies. However, what becomes a critical issue is that the same cut-off is used when ‘multiple’ tests are undertaken on the same case-control (or any pairwise) comparison. Here, in brevity, we present what the P value represents, and why and when it should be adjusted. We also show, with worked examples, how to adjust P values for multiple testing in the R environment for statistical computing (http://www.R-project.org).},
   author = {Mohieddin Jafari and Naser Ansari-Pour},
   doi = {10.22074/CELLJ.2019.5992},
   issn = {22285814},
   issue = {4},
   journal = {Cell Journal (Yakhteh)},
   keywords = {Bias,Gene expression profiling,Genetic variation,Research design,Statistical data analyses},
   pages = {604},
   pmid = {30124010},
   publisher = {Royan Institute},
   title = {Why, When and How to Adjust Your P Values?},
   volume = {20},
   url = {/pmc/articles/PMC6099145/ /pmc/articles/PMC6099145/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6099145/},
   year = {2019}
}
@article{Amig2020,
   abstract = {In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as positive, neutral, negative in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory. Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.},
   author = {Enrique Amigó and Julio Gonzalo and Stefano Mizzaro and Jorge Carrillo-De-Albornoz},
   doi = {10.18653/V1/2020.ACL-MAIN.363},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {3938-3949},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results},
   url = {https://aclanthology.org/2020.acl-main.363},
   year = {2020}
}
@article{Bates2023,
   abstract = {1. When deploying a predictive model, it is important to understand its prediction accuracy on future test points, so both good point estimates and accurate confidence intervals for prediction erro...},
   author = {Stephen Bates and Trevor Hastie and Robert Tibshirani},
   doi = {10.1080/01621459.2023.2197686},
   issn = {1537274X},
   journal = {Journal of the American Statistical Association},
   keywords = {Bootstrap/resampling,Computationally intensive methods,Cross-validation,Goodness-of-fit methods},
   publisher = {Taylor \& Francis},
   title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2023.2197686},
   year = {2023}
}
@article{Breiman2001,
   abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
   author = {Leo Breiman},
   doi = {10.1214/SS/1009213726},
   issn = {0883-4237},
   issue = {3},
   journal = {https://doi.org/10.1214/ss/1009213726},
   month = {8},
   pages = {199-231},
   publisher = {Institute of Mathematical Statistics},
   title = {Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)},
   volume = {16},
   url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--T},
   year = {2001}
}
@article{Mullainathan2017,
   author = {Sendhil Mullainathan and Jann Spiess},
   doi = {10.1257/JEP.31.2.87},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Econometric Modeling: General,Econometric Software},
   month = {3},
   pages = {87-106},
   publisher = {American Economic Association},
   title = {Machine Learning: An Applied Econometric Approach},
   volume = {31},
   url = {https://doi.org/10.1257/jep.31.2.87},
   year = {2017}
}
@article{Demsar2006,
   abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
   author = {Janez Demšar},
   doi = {10.1016/j.jecp.2010.03.005},
   isbn = {9781424450404},
   issn = {1532-4435},
   journal = {Journal of Machine Learning Research},
   keywords = {Friedman test,Wilcoxon signed ranks test,comparative studies,multiple comparisons tests,statistical methods},
   pages = {1-30},
   pmid = {20451214},
   title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
   volume = {7},
   year = {2006}
}
@article{Nwulu2017,
   abstract = {In this work, we present a performance comparison of the Multi Layer Perceptron (MLP), Support Vector Machines (SVM) and Voted Perceptron (VP) when applied to a social signal processing task. The signal processing task is in the field of computational politics where the aim is to predict the political parties of American congress members based on their response to certain questions. Using this dataset which is publicly available, we investigate the use of four methods to impute or approximate missing values. The four imputed datasets are used to train MLP, SVM and VP classifiers to associate the congress members' responses to their political party affiliation and we compare the results from the three classifiers. The aim is to design a practical system or model to be able to predict another person's political affiliations based on their responses to similar questions. The obtained experimental results suggest that machine learning classifiers can be used to accurately predict an individual's political leaning.},
   author = {Nnamdi I. Nwulu},
   doi = {10.1109/IDAP.2017.8090315},
   isbn = {9781538618806},
   journal = {IDAP 2017 - International Artificial Intelligence and Data Processing Symposium},
   keywords = {Missing data imputation,Multi layer perceptron,Support vector machines,Voted perceptron},
   month = {10},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Evaluation of machine learning classification algorithms \& missing data imputation techniques},
   year = {2017}
}
@article{Raj2023,
   abstract = {In the era of advanced machinery including sensor technology, massive amount of data than ever before is produced every day in a variety of different fields. In the process of handling huge data, some data values are missed due to various controllable or uncontrollable factors. The missing data problem is a real-life problem which found everywhere and that is why it has received huge attention in research in the past decades. It is a major hurdle in data analysis and research. Therefore, for better analysis of data there are various imputation methods to handle missing data varying from directly deleting the data which contains missing values or using traditional methods like mean, median and mode to fill the missing value to using complex deep learning and machine learning techniques to predict missing value. This paper first discusses different kinds of missing data and then analyzing various imputation techniques from traditional to modern approach for dealing with missing data. It also analyzes advantages and disadvantages of different imputation techniques to get a better perspective of these techniques.},
   author = {Ananya Raj and Aditya Dubey and Akhtar Rasool and Rajesh Wadwani},
   doi = {10.1109/ICCCNT56998.2023.10307783},
   isbn = {9798350335095},
   journal = {2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023},
   keywords = {Deep Learning Imputation,Machine Learning Imputation,Statistical Imputation},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Machine Learning based Missing Data Imputation Techniques},
   year = {2023}
}
@article{Nava-Muoz2024,
   abstract = {Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms’ performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems’ performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regression problems. The proposed methodology offers several advantages, including off-the-shell comparisons with correction mechanisms and the inclusion of confidence intervals. Furthermore, we introduce metrics that allow organizers to assess the difficulty of competitions. Our analysis shows the potential usefulness of our methodology for effectively evaluating competition results.},
   author = {Sergio Nava-Muñoz and Mario Graff and Hugo Jair Escalante},
   doi = {10.1016/J.PATREC.2024.03.010},
   issn = {0167-8655},
   journal = {Pattern Recognition Letters},
   keywords = {Bootstrap,Challenges,Performance},
   month = {3},
   publisher = {North-Holland},
   title = {Analysis of systems’ performance in natural language processing competitions},
   year = {2024}
}
@article{Garca2008,
   abstract = {In a recently published paper in JMLR, Demša (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. © 2008 Salvador García and Francisco Herrera.},
   author = {Salvador García and Francisco Herrera},
   issn = {15324435},
   journal = {Journal of Machine Learning Research},
   keywords = {Adjusted p-values,Logically related hypotheses,Multiple comparisons tests,Non-parametric test,Statistical methods},
   pages = {2677-2694},
   title = {An extension on ``statistical comparisons of classifiers over multiple data sets'' for all pairwise comparisons},
   volume = {9},
   year = {2008}
}
@book{Poli2008,
   author = {Riccardo Poli and William B Langdon and Nicholas Freitag N F McPhee},
   isbn = {978-1-4092-0073-4},
   keywords = {GPU,artificial intelligence,automatic programming,cartesian genetic programming,evolutionary computation,genetic algorithms,genetic programming,machine learning},
   pages = {8},
   publisher = {Published via lulu.com and freely available at www.gp-field-guide.org.uk},
   title = {A field guide to genetic programming},
   year = {2008}
}
@article{Aragn2019,
   abstract = {This paper presents the framework and results from the MEX-A3T track at IberLEF 2019. This track considers two tasks, author profiling and aggressiveness detection, both of them using Mexican Spanish tweets. The author profiling task consists on determining the gender, occupation and place of residence of users from their tweets. As a novelty in this year's edition, it considers the use of text and images as information sources, with the aim of studying the relevance and complementarity of multimodal data for profiling social media users. On the other hand, the aggressiveness detection task follows the same design than the previous edition; it aims to discriminate between aggressive and non-aggressive tweets. For both tasks, we have built new corpora considering tweets from Mexican Twitter users. This paper compares and discusses the results of the participants.},
   author = {Mario Ezra Aragón and Miguel Álvarez-Carmona and Manuel Montes-Y-Gómez and Hugo Jair Escalante and Luis Villaseñor-Pineda and Daniela Moctezuma},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   pages = {478-494},
   title = {Overview of MEX-A3T at IberLEF 2019: Authorship and aggressiveness analysis in Mexican Spanish tweets},
   volume = {2421},
   year = {2019}
}
@inproceedings{Nava-MuozSergioandGraffGuerrero2023,
   abstract = {In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific question, the dataset (with adequate quality, quantity, diversity, and complexity), performance metrics, as well as a way to authenticate the participants' results (Gold Standard). This paper addresses the problem of evaluating the performance of different competitors (algorithms) under the restrictions imposed by the challenge scheme, such as the comparison of multiple competitors with a unique dataset (with fixed size), a minimal number of submissions and, a set of metrics chosen to assess performance. The algorithms are sorted according to the performance metric. Still, it is common to observe performance differences among competitors as small as hundredths or even thousandths, so the question is whether the differences are significant. This paper analyzes the results of the MeOffendEs@IberLEF 2021 competition and proposes to make inference through resampling techniques (bootstrap) to support Challenge organizers' decision-making.},
   author = {Mario
and Escalante Hugo Jair Nava-Muñoz Sergio
and Graff Guerrero},
   city = {Cham},
   editor = {Humberto
and Martínez-Trinidad José Francisco
and Carrasco-Ochoa Jesús Ariel
and Olvera-López José Arturo Rodríguez-González Ansel Yoan
and Pérez-Espinosa},
   isbn = {978-3-031-33783-3},
   booktitle = {Pattern Recognition},
   pages = {89-98},
   publisher = {Springer Nature Switzerland},
   title = {Comparison of Classifiers in Challenge Scheme},
   year = {2023}
}
@article{Aragn2020,
   abstract = {This paper presents the overview of MEX-A3T 2020, the third edition of this lab under the IberLEF conference. The main purpose of MEX-A3T is to explore different methodologies and strategies related to the analysis of social media content in Mexican Spanish. This year edition focuses in the identification of fake news and the detection of aggressive tweets. For this purpose, we provided different news from verified web sources and a corpus of tweets from Mexican users.v.},
   author = {Mario Ezra Aragón and Horacio Jarquín-Vasqueza and Manuel Montes-Y-Gómez and Hugo Jair Escalante and Luis Villasen\~\{o\}r-Pineda and Helena Gómez-Adorno and Juan Pablo Posadas-Durán and Gemma Bel-Enguix},
   issn = {16130073},
   issue = {August},
   journal = {CEUR Workshop Proceedings},
   keywords = {Aggressiveness detection,Fake news detection,Iberlef,Mex-a3t},
   pages = {222-235},
   title = {Overview of mex-a3t at iberlef 2020: Fake news and aggressiveness analysis in mexican Spanish},
   volume = {2664},
   year = {2020}
}
@article{Garca-Vega2020,
   abstract = {The Task on Semantic Analysis at SEPLN (TASS task within IberLEF 2020 workshop) took place on September 22, reaching its ninth edition. Due to the COVID-19 pandemic, the number of participants is lower compared to past campaigns. Also, the organizers decided to held it remotely. In this edition, the classical polarity classification subtask was, again, organized. As a novelty, a second subtask was proposed to foster research in emotion detection of Spanish texts on a new dataset. This paper summarizes the different approaches of the teams who participated, the key insights of their systems and the results obtained for all the proposed solutions.},
   author = {Manuel García-Vega and Manuel Carlos Díaz-Galiano and Miguel García-Cumbreras and Flor Miriam Plaza Del Arco and Arturo Montejo-Ráez and Salud María Jiménez-Zafra and Eugenio Martínez Cámara and César Antonio Aguilar and Marco Antonio Sobrevilla Cabezudo and Luis Chiruzzo and Daniela Moctezuma},
   isbn = {0000000318679},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Opinion mining,Sentiment analysis,Social media},
   pages = {163-170},
   title = {Overview of TASS 2020: Introducing Emotion Detection},
   volume = {2664},
   year = {2020}
}
@article{Howe2006,
   author = {Jeff Howe},
   issue = {6},
   journal = {Wired magazine},
   pages = {1-4},
   title = {The rise of crowdsourcing},
   volume = {14},
   year = {2006}
}
@book{Brabham2013,
   author = {Daren C Brabham},
   publisher = {MIT Press},
   title = {Crowdsourcing},
   year = {2013}
}
@misc{,
   note = {Accedido: 2024-05-10},
   title = {Kaggle: Your Home for Data Science},
   url = {https://www.kaggle.com},
   year = {2010}
}
@article{Meier2012,
   author = {Patrick Meier},
   issue = {2},
   journal = {Journal of Map \& Geography Libraries},
   pages = {89-100},
   publisher = {Taylor \& Francis},
   title = {Crisis mapping in action: How open source software and global volunteer networks are changing the world, one map at a time},
   volume = {8},
   year = {2012}
}
@book{Surowiecki2005,
   author = {James Surowiecki},
   publisher = {Anchor Books},
   title = {The wisdom of crowds},
   year = {2005}
}
@article{Geiger2011,
   author = {Daniel Geiger and Stefan Seedorf and Thimo Schulze and Robert C Nickerson and Martin Schader},
   journal = {Proceedings of the Seventeenth Americas Conference on Information Systems},
   pages = {1-11},
   title = {Managing the crowd: Towards a taxonomy of crowdsourcing processes},
   volume = {2011},
   year = {2011}
}
@article{Krizhevsky2012,
   author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E Hinton},
   issue = {6},
   journal = {Communications of the ACM},
   pages = {84-90},
   publisher = {ACM},
   title = {ImageNet classification with deep convolutional neural networks},
   volume = {60},
   year = {2012}
}
@article{Guyon2004,
   author = {Isabelle Guyon and Steve Gunn and Masoud Nikravesh and Lotfi A Zadeh},
   journal = {Advances in Neural Information Processing Systems},
   pages = {545-552},
   publisher = {MIT Press},
   title = {Result analysis of the NIPS 2003 feature selection challenge},
   volume = {17},
   year = {2004}
}
@misc{Escalante2023,
   author = {Hugo Jair Escalante and Aleksandra Kruchinina},
   title = {Academic competitions},
   year = {2023}
}
@article{Nadeau2007,
   author = {David Nadeau and Satoshi Sekine},
   issue = {1},
   journal = {Lingvisticae Investigationes},
   pages = {3-26},
   title = {A survey of named entity recognition and classification},
   volume = {30},
   year = {2007}
}
@article{Russakovsky2015,
   abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
   author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
   doi = {10.1007/S11263-015-0816-Y/FIGURES/16},
   issn = {15731405},
   issue = {3},
   journal = {International Journal of Computer Vision},
   keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
   month = {12},
   pages = {211-252},
   publisher = {Springer New York LLC},
   title = {ImageNet Large Scale Visual Recognition Challenge},
   volume = {115},
   url = {https://link.springer.com/article/10.1007/s11263-015-0816-y},
   year = {2015}
}
@article{Lintott2008,
   abstract = {In order to understand the formation and subsequent evolution of galaxies one must first distinguish between the two main morphological classes of massive systems: spirals and early-type systems. This paper introduces a project, Galaxy Zoo, which provides visual morphological classifications for nearly one million galaxies, extracted from the Sloan Digital Sky Survey (SDSS). This achievement was made possible by inviting the general public to visually inspect and classify these galaxies via the internet. The project has obtained more than 4 × 107 individual classifications made by ∼10 5 participants. We discuss the motivation and strategy for this project, and detail how the classifications were performed and processed. We find that Galaxy Zoo results are consistent with those for subsets of SDSS galaxies classified by professional astronomers, thus demonstrating that our data provide a robust morphological catalogue. Obtaining morphologies by direct visual inspection avoids introducing biases associated with proxies for morphology such as colour, concentration or structural parameters. In addition, this catalogue can be used to directly compare SDSS morphologies with older data sets. The colour-magnitude diagrams for each morphological class are shown, and we illustrate how these distributions differ from those inferred using colour alone as a proxy for morphology. © 2008 RAS.},
   author = {Chris J. Lintott and Kevin Schawinski and Anže Slosar and Kate Land and Steven Bamford and Daniel Thomas and M. Jordan Raddick and Robert C. Nichol and Alex Szalay and Dan Andreescu and Phil Murray and Jan Vandenberg},
   doi = {10.1111/J.1365-2966.2008.13689.X/2/MNRAS0389-1179-F12.JPEG},
   issn = {13652966},
   issue = {3},
   journal = {Monthly Notices of the Royal Astronomical Society},
   keywords = {Galaxies: elliptical and lenticular, cD,Galaxies: general,Galaxies: spiral,Methods: data analysis},
   month = {9},
   pages = {1179-1189},
   publisher = {Oxford University Press},
   title = {Galaxy Zoo: Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey},
   volume = {389},
   url = {https://dx.doi.org/10.1111/j.1365-2966.2008.13689.x},
   year = {2008}
}
@article{Estells-Arolas2012,
   abstract = {‘Crowdsourcing’ is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of...},
   author = {Enrique Estellés-Arolas and Fernando González-Ladrón-De-Guevara},
   doi = {10.1177/0165551512437638},
   issn = {01655515},
   issue = {2},
   journal = {http://dx.doi.org/10.1177/0165551512437638},
   keywords = {crowdsourcing,definition,innovation},
   month = {3},
   pages = {189-200},
   publisher = {SAGE PublicationsSage UK: London, England},
   title = {Towards an integrated crowdsourcing definition},
   volume = {38},
   url = {https://journals.sagepub.com/doi/full/10.1177/0165551512437638?journalCode=jisb},
   year = {2012}
}
@article{Escalante2024,
   abstract = {Challenges can be seen as a type of game that motivates participants to solve serious tasks. As a result, competition organizers must develop effective game rules. However, these rules have multiple objectives beyond making the game enjoyable for participants. These objectives may include solving real-world problems, advancing scientific or technical areas, making scientific discoveries, and educating the public. In many ways, creating a challenge is similar to launching a product. It requires the same level of excitement and rigorous testing, and the goal is to attract ''customers'' in the form of participants. The process begins with a solid plan, such as a competition proposal that will eventually be submitted to an international conference and subjected to peer review. Although peer review does not guarantee quality, it does force organizers to consider the impact of their challenge, identify potential oversights, and generally improve its quality. This chapter provides guidelines for creating a strong plan for a challenge. The material draws on the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and Tailor 3 , as well as the NeurIPS proposal template, which some of the authors contributed to.},
   author = {Hugo-Jair Escalante and Isabelle Guyon and Addison Howard and Walter Reade and Sébastien Treguer and Adrien Pavao and Evelyne Viegas},
   keywords = {challenge design,challenge proposal INRIA,organizer guidelines},
   month = {1},
   title = {Challenge design roadmap},
   url = {https://arxiv.org/abs/2401.13693v1},
   year = {2024}
}
@article{Hesterberg2015,
   abstract = {1. Resampling methods, including permutation tests and the bootstrap, have enormous potential in statistics education and practice. They are beginning to make inroads in education. Cobb (2007) was ...},
   author = {Tim C. Hesterberg},
   doi = {10.1080/00031305.2015.1089789},
   issn = {15372731},
   issue = {4},
   journal = {The American Statistician},
   keywords = {Bias,Confidence intervals,Sampling distribution,Standard error,Statistical concepts,Teaching.},
   month = {10},
   pages = {371-386},
   publisher = {Taylor \& Francis},
   title = {What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum},
   volume = {69},
   url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.2015.1089789},
   year = {2015}
}
@article{Escalante2024,
   abstract = {Challenges can be seen as a type of game that motivates participants to solve serious tasks. As a result, competition organizers must develop effective game rules. However, these rules have multiple objectives beyond making the game enjoyable for participants. These objectives may include solving real-world problems, advancing scientific or technical areas, making scientific discoveries, and educating the public. In many ways, creating a challenge is similar to launching a product. It requires the same level of excitement and rigorous testing, and the goal is to attract ''customers'' in the form of participants. The process begins with a solid plan, such as a competition proposal that will eventually be submitted to an international conference and subjected to peer review. Although peer review does not guarantee quality, it does force organizers to consider the impact of their challenge, identify potential oversights, and generally improve its quality. This chapter provides guidelines for creating a strong plan for a challenge. The material draws on the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and Tailor 3 , as well as the NeurIPS proposal template, which some of the authors contributed to.},
   author = {Hugo-Jair Escalante and Isabelle Guyon and Addison Howard and Walter Reade and Sébastien Treguer and Adrien Pavao and Evelyne Viegas},
   keywords = {challenge design,challenge proposal INRIA,organizer guidelines},
   month = {1},
   title = {Challenge design roadmap},
   url = {https://arxiv.org/abs/2401.13693v1},
   year = {2024}
}
@article{Benjamini1995,
   abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses — the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
   author = {Yoav Benjamini and Yosef Hochberg},
   doi = {10.1111/J.2517-6161.1995.TB02031.X},
   issn = {2517-6161},
   issue = {1},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   keywords = {bonferroni,comparison procedures,familywise error rate,multiple,p,type procedures,values},
   month = {1},
   pages = {289-300},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
   volume = {57},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1995.tb02031.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1995.tb02031.x},
   year = {1995}
}
@article{Labatut2012,
   abstract = {The selection of the best classification algorithm for a given dataset is a very widespread problem. It is also a complex one, in the sense it requires to make several important methodological choices. Among them, in this work we focus on the measure used to assess the classification performance and rank the algorithms. We present the most popular measures and discuss their properties. Despite the numerous measures proposed over the years, many of them turn out to be equivalent in this specific case, to have interpretation problems, or to be unsuitable for our purpose. Consequently, classic overall success rate or marginal rates should be preferred for this specific task.},
   author = {Vincent Labatut and Hocine Cherifi},
   keywords = {Accuracy Measure,Classification,Classifier Comparison},
   month = {7},
   title = {Accuracy Measures for the Comparison of Classifiers},
   url = {https://arxiv.org/abs/1207.3790v1},
   year = {2012}
}
@misc{Berg-Kirkpatrick2012,
   abstract = {We investigate two aspects of the empirical behavior of paired significance tests for NLP systems. First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies significance? We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems. Next, once significance levels are computed, how well does the standard i.i.d. notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing.},
   author = {Taylor Berg-Kirkpatrick and David Burkett and Dan Klein},
   pages = {995-1005},
   publisher = {Association for Computational Linguistics},
   title = {An Empirical Investigation of Statistical Significance in NLP},
   url = {https://aclanthology.org/D12-1091},
   year = {2012}
}
@article{Chai2014,
   abstract = {Both the root mean square error (RMSE) and the mean absolute error (MAE) are regularly employed in model evaluation studies. Willmott and Matsuura (2005) have suggested that the RMSE is not a good indicator of average model performance and might be a misleading indicator of average error, and thus the MAE would be a better metric for that purpose. While some concerns over using RMSE raised by Willmott and Matsuura (2005) and Willmott et al. (2009) are valid, the proposed avoidance of RMSE in favor of MAE is not the solution. Citing the aforementioned papers, many researchers chose MAE over RMSE to present their model evaluation statistics when presenting or adding the RMSE measures could be more beneficial. In this technical note, we demonstrate that the RMSE is not ambiguous in its meaning, contrary to what was claimed by Willmott et al. (2009). The RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian. In addition, we show that the RMSE satisfies the triangle inequality requirement for a distance metric, whereas Willmott et al. (2009) indicated that the sums-of-squares-based statistics do not satisfy this rule. In the end, we discussed some circumstances where using the RMSE will be more beneficial. However, we do not contend that the RMSE is superior over the MAE. Instead, a combination of metrics, including but certainly not limited to RMSEs and MAEs, are often required to assess model performance. © Author(s) 2014. CC Attribution 3.0 License.},
   author = {T. Chai and R. R. Draxler},
   doi = {10.5194/GMD-7-1247-2014},
   issn = {19919603},
   issue = {3},
   journal = {Geoscientific Model Development},
   month = {6},
   pages = {1247-1250},
   publisher = {Copernicus GmbH},
   title = {Root mean square error (RMSE) or mean absolute error (MAE)? -Arguments against avoiding RMSE in the literature},
   volume = {7},
   year = {2014}
}
@article{Fawcett2006,
   author = {Tom Fawcett},
   issue = {8},
   journal = {Pattern recognition letters},
   pages = {861-874},
   publisher = {Elsevier},
   title = {An introduction to ROC analysis},
   volume = {27},
   year = {2006}
}
@article{Powers2011,
   author = {David M W Powers},
   journal = {arXiv preprint arXiv:2010.16061},
   title = {Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
   year = {2011}
}
@book{Murphy2012,
   author = {Kevin P Murphy},
   publisher = {MIT press},
   title = {Machine learning: a probabilistic perspective},
   year = {2012}
}
@article{Willmott2005,
   abstract = {ABSTRACT:The relative abilities of 2, dimensioned statistics—the root-mean-square error (RMSE) and the mean absolute error (MAE)—to describe average model-performance error are examined. The RMSE is of special interest because it is widely reported in the climatic and environmental literature; nevertheless, it is an inappropriate and misinterpreted measure of average error. RMSE is inappropriate because it is a function of 3 characteristics of a set of errors, rather than of one (the average error). RMSE varies with the variability within the distribution of error magnitudes and with the square root of the number of errors (n1/2), as well as with the average-error magnitude (MAE). Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous. Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE.},
   author = {Cort J Willmott and Kenji Matsuura},
   issn = {0936577X, 16161572},
   issue = {1},
   journal = {Climate Research},
   pages = {79-82},
   publisher = {Inter-Research Science Center},
   title = {Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},
   volume = {30},
   url = {http://www.jstor.org/stable/24869236},
   year = {2005}
}
@article{deMyttenaere2016,
   abstract = {We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We prove the existence of an optimal MAPE model and we show the universal consistency of Empirical Risk Minimization based on the MAPE. We also show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression, and we apply this weighting strategy to kernel regression. The behavior of the MAPE kernel regression is illustrated on simulated data.},
   author = {Arnaud de Myttenaere and Boris Golden and Bénédicte Le Grand and Fabrice Rossi},
   doi = {10.1016/J.NEUCOM.2015.12.114},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Consistency,Empirical Risk Minimization,Kernel regression,Mean Absolute Percentage Error,Optimization},
   month = {6},
   pages = {38-48},
   publisher = {Elsevier},
   title = {Mean Absolute Percentage Error for regression models},
   volume = {192},
   year = {2016}
}
@article{Liu2012,
   author = {Bing Liu},
   city = {Cham},
   doi = {10.1007/978-3-031-02145-9},
   isbn = {978-3-031-01017-0},
   publisher = {Springer International Publishing},
   title = {Sentiment Analysis and Opinion Mining},
   url = {https://link.springer.com/10.1007/978-3-031-02145-9},
   year = {2012}
}
@article{Nakatsu2021,
   abstract = {This article investigates resampling methods used to evaluate the performance of machine learning classification algorithms. It compares four key resampling methods: 1) Monte Carlo resampling, 2) the Bootstrap Method, 3) k-fold Cross Validation, and 4) Repeated k-fold Cross Validation. Two classification algorithms, Support Vector Machines and Random Forests, applied to three datasets, are used in this article. Nine variations of the four resampling methods are used to tune parameters on the two classification algorithms on each of the three datasets. Performance is defined by how well the resampling method chooses a parameter value that fits the data well. A main finding is that Repeated k-fold Cross Validation, overall, outperforms the other resampling methods in selecting the best-fit parameter value across the three different datasets.},
   author = {Robbie T. Nakatsu},
   doi = {10.1109/MIS.2020.2978066},
   issn = {19411294},
   issue = {3},
   journal = {IEEE Intelligent Systems},
   keywords = {classification,hootstrap,hyperparameter tuning,k-fold cross validation,machine learning,modeling and prediction,resampling,simulation,validation},
   month = {5},
   pages = {51-57},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Evaluation of Four Resampling Methods Used in Machine Learning Classification},
   volume = {36},
   year = {2021}
}
@article{Good2006,
   abstract = {"The author has packaged an excellent and modern set of topics around the development and use of quantitative models. If you need to learn about resampling, this book would be a good place to start." -Technometrics (Review of the Second Edition) This thoroughly revised and expanded third edition is a practical guide to data analysis using the bootstrap, cross-validation, and permutation tests. Only requiring minimal mathematics beyond algebra, the book provides a table-free introduction to data analysis utilizing numerous exercises, practical data sets, and freely available statistical shareware. Topics and Features. Practical presentation covers both the bootstrap and permutations along with the program code necessary to put them to work. Includes a systematic guide to selecting the correct procedure for a particular application. Detailed coverage of classification, estimation, experimental design, hypothesis testing, and modeling. Suitable for both classroom use and individual self-study. New to the Third Edition. Procedures are grouped by application; a prefatory chapter guides readers to the appropriate reading matter. Program listings and screen shots now accompany each resampling procedure: Whether one programs in C++, CART, Blossom, Box Sampler (an Excel add-in), EViews, MATLAB, R, Resampling Stats, SAS macros, S-PLUS, Stata, or StatXact, readers will find the program listings and screen shots needed to put each resampling procedure into practice. To simplify programming, code for readers to download and apply is posted at http://www.springeronline. com/0-8176-4386-9. Notation has been simplified and, where possible, eliminated. A glossary and answers to selected exercises are included. With its accessible style and intuitive topic development, the book is an excellent basic resource for the power, simplicity, and versatility of resampling methods. It is an essential resource for statisticians, biostatisticians, statistical consultants, students, and research professionals in the biological, physical, and social sciences, engineering, and technology. © 1999 Birkhäuser Boston, 1st edition. All rights reserved.},
   author = {Phillip I. Good},
   doi = {10.1007/0-8176-4444-X/COVER},
   isbn = {0817643869},
   journal = {Resampling Methods: A Practical Guide to Data Analysis},
   pages = {1-218},
   publisher = {Birkhauser Boston},
   title = {Resampling methods: A practical guide to data analysis},
   year = {2006}
}
@article{Meinshausen2010,
   abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data. © 2010 Royal Statistical Society.},
   author = {Nicolai Meinshausen and Peter Bühlmann},
   doi = {10.1111/J.1467-9868.2010.00740.X},
   issn = {1467-9868},
   issue = {4},
   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
   keywords = {High dimensional data,Resampling,Stability selection,Structure estimation},
   month = {9},
   pages = {417-473},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Stability selection},
   volume = {72},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2010.00740.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00740.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2010.00740.x},
   year = {2010}
}
@book{Hastie2009,
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   publisher = {Springer},
   title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
   year = {2009}
}
@book{Good2013,
   author = {Phillip Good},
   publisher = {Springer Science \& Business Media},
   title = {Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses},
   year = {2013}
}
@article{Ojala2010,
   author = {Markus Ojala and Gemma C Garriga},
   journal = {Journal of Machine Learning Research},
   pages = {1833-1863},
   title = {Permutation tests for studying classifier performance},
   volume = {11},
   year = {2010}
}
@article{Bergmeir2018,
   author = {Christoph Bergmeir and Jose M Benitez},
   journal = {Computational Statistics \& Data Analysis},
   pages = {70-83},
   title = {A note on the validity of cross-validation for evaluating time series prediction},
   volume = {120},
   year = {2018}
}
@article{Sokolova2009,
   author = {Marina Sokolova and Guy Lapalme},
   issue = {4},
   journal = {Information Processing \& Management},
   pages = {427-437},
   publisher = {Elsevier},
   title = {A survey of performance evaluation measures for classification systems},
   volume = {45},
   year = {2009}
}
@article{Fawcett2006,
   author = {Tom Fawcett},
   issue = {8},
   journal = {Pattern Recognition Letters},
   pages = {861-874},
   publisher = {Elsevier},
   title = {An introduction to ROC analysis},
   volume = {27},
   year = {2006}
}
@book{Japkowicz2011,
   author = {Nathalie Japkowicz and Mohak Shah},
   publisher = {Cambridge University Press},
   title = {Evaluating Learning Algorithms: A Classification Perspective},
   year = {2011}
}
@article{Bergmeir2018,
   abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often replaced by practitioners in favour of an out-of-sample (OOS) evaluation. It is shown that for purely autoregressive models, the use of standard K-fold CV is possible provided the models considered have uncorrelated errors. Such a setup occurs, for example, when the models nest a more appropriate model. This is very common when Machine Learning methods are used for prediction, and where CV can control for overfitting the data. Theoretical insights supporting these arguments are presented, along with a simulation study and a real-world example. It is shown empirically that K-fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
   author = {Christoph Bergmeir and Rob J. Hyndman and Bonsoo Koo},
   doi = {10.1016/J.CSDA.2017.11.003},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Autoregression,Cross-validation,Time series},
   month = {4},
   pages = {70-83},
   publisher = {Elsevier B.V.},
   title = {A note on the validity of cross-validation for evaluating autoregressive time series prediction},
   volume = {120},
   year = {2018}
}
@book{Good2004,
   abstract = {Review From the reviews of the third edition: "All told, Permutation, Parametric, and Bootstrap Tests of Hypotheses garners high marks for its scope and clarity. Graduate students will appreciate its rigorous treatment of diverse topics and ample exercises to reinforce ideas...this text deserves a place in any scientific library." Journal of the American Statistical Association, December 2005 "The book provides a good overview of hypotheses testing and decision theory. a The book is well-written, concise and clearly organized. Many examples and figures illustrate the text. Each chapter is concluded by numerous exercises a to make the fundamental concepts more comprehensive. a My overall impression of the book is very positive. a the book is a valuable supplement to the existing literature and can be recommended to both practitioners and researchers in statistics." (Bernd Droge, Metrika, Vol. 64, 2006) "This is the third edition of a well known and respected book by Good. a provides a very good overview on decision theory and hypothesis testing. It is well written and does cover permutation, parametric and bootstrap techniques very effectively. I would recommend this book for the statisticians as well as biostatisticians to practice the methodologies provided in this book. This book will be of interest to graduate students in statistics and biostatistics. In addition, this book would be a valuable asset for the library." (B. M. Golam Kibria, Statistical Papers, Vol. 47, 2006) "Although this third edition has only 45 more pages a the change in title suggests a change in focus from mainly dealing with permutation tests to put equal weight on parametric and bootstraptests of hypotheses. a It would be excellent as a supplemental text on testing hypotheses and decision theory a . it is excellent for those who want to learn about or to apply permutation methods a . It also has a good general overview of hypotheses testing and decision theory." (Andreas Karlsson, Journal of the Royal Statistical Society, Vol. 169 (1), 2006) "This is the third edition of a well known and highly praised book. a It a also includes material on parametric and bootstrap tests but permutation tests take the centerstage. This revised edition has been enlarged by about 25 pages. a The number of exercises has been greatly increased. More interestingly, some of the essential results have now been given in the form of exercises." (Arup Bose, Sankhya, Vol. 67 (1), 2005) "From the start of the journey into testing hypotheses a the book refers to the authora (TM)s personal experience. a This is supposed to benefit the students, instructors and autodidacts a . the book is intended for a two-semester graduate course on hypotheses testing and decision theory. a to the best of judgment, it is a very interesting, profound, modern and useful book." (Gaj Vidmar, ISCB News, Vol. 144 (2), 2007) "In this book the author explores the use of computational methods for hypothesis testing, and he describes the great advantages that make these methods the most powerful tools among statistical procedures. a The book is clear, readable and very well focused a . This is a book for graduate students and scientists. Practitioners can also take great advantage of it a . In my view, this book should be present in all statistics departments and university libraries." (AnaF. Militino, Journal of Applied Statistics, Vol. 34 (10), 2007) Product Description Explains the necessary background in testing hypothesis and decision theory to enable innumerable practical applications of statistics. This book includes many real-world illustrations from biology, business, clinical trials, economics, geology, law, medicine, social science and engineering along with twice the number of exercises.},
   author = {Phillip Good},
   doi = {10.1007/B138696},
   journal = {Permutation, Parametric and Bootstrap Tests of Hypotheses},
   publisher = {Springer-Verlag},
   title = {Permutation, Parametric and Bootstrap Tests of Hypotheses},
   year = {2004}
}
@article{Arlot2010,
   abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
   author = {Sylvain Arlot and Alain Celisse},
   doi = {10.1214/09-SS054},
   issn = {1935-7516},
   issue = {none},
   journal = {https://doi.org/10.1214/09-SS054},
   keywords = {62G05,62G08,62G09,Model selection,cross-validation,leave-one-out},
   month = {1},
   pages = {40-79},
   publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
   title = {A survey of cross-validation procedures for model selection},
   volume = {4},
   url = {https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.short},
   year = {2010}
}
@article{Plevris2022,
   abstract = {Performance metrics (Evaluation metrics or error metrics) are crucial components of regression analysis and machine learning-based prediction models. A performance metric can be defined as a logical and mathematical construct designed to measure how close the predicted outcome is to the actual result. A variety of performance metrics have been described and proposed in the literature. Knowledge about the metrics’ properties needs to be systematized to simplify their design and use. In this work, we examine various regression related metrics (14 in total) for continuous variables, including the most widely used ones, such as the (root) mean squared error, the mean absolute error, the Pearson correlation coefficient, and the coefficient of determination, among many others. We provide their mathematical formulations, as well as a discussion on their use, their characteristics, advantages, disadvantages, and limitations, through theoretical analysis and a detailed numerical example. The 10 unitless metrics are further investigated through a numerical analysis with Monte Carlo Simulation based on (i) random guessing and (ii) the addition of random noise with various noise ratios to the predicted values. Some of the metrics show a poor or inconsistent performance, while others exhibit good performance as evaluation measures of the “goodness of fit”. We highlight the importance of the usage of the right metrics to obtain good predictions in machine learning and regression models in general.},
   author = {Vagelis Plevris and German Solorzano and Nikolaos P. Bakas and Mohamed El Amine Ben Seghier},
   doi = {10.23967/ECCOMAS.2022.155},
   issn = {26966999},
   journal = {ECCOMAS Congress 2022 - 8th European Congress on Computational Methods in Applied Sciences and Engineering},
   keywords = {Error metric,Evaluation,Machine learning,Neural network,Performance metric,Prediction model,Regression model},
   month = {11},
   publisher = {Scipedia S.L.},
   title = {Investigation of performance metrics in regression analysis and machine learning-based prediction models},
   url = {https://www.scipedia.com/public/Plevris_et_al_2022a},
   year = {2022}
}
@article{Sedgwick2012,
   author = {Philip Sedgwick},
   doi = {10.1136/BMJ.E509},
   issn = {09598146},
   issue = {7841},
   journal = {BMJ (Online)},
   month = {1},
   title = {Multiple significance tests: The Bonferroni correction},
   volume = {344},
   year = {2012}
}
@article{Kraus2022,
   abstract = {Review articles or literature reviews are a critical part of scientific research. While numerous guides on literature reviews exist, these are often limited to the philosophy of review procedures, protocols, and nomenclatures, triggering non-parsimonious reporting and confusion due to overlapping similarities. To address the aforementioned limitations, we adopt a pragmatic approach to demystify and shape the academic practice of conducting literature reviews. We concentrate on the types, focuses, considerations, methods, and contributions of literature reviews as independent, standalone studies. As such, our article serves as an overview that scholars can rely upon to navigate the fundamental elements of literature reviews as standalone and independent studies, without getting entangled in the complexities of review procedures, protocols, and nomenclatures.},
   author = {Sascha Kraus and Matthias Breier and Weng Marc Lim and Marina Dabić and Satish Kumar and Dominik Kanbach and Debmalya Mukherjee and Vincenzo Corvello and Juan Piñeiro-Chousa and Eric Liguori and Daniel Palacios-Marqués and Francesco Schiavone and Alberto Ferraris and Cristina Fernandes and João J. Ferreira},
   doi = {10.1007/S11846-022-00588-8/FIGURES/3},
   issn = {18636691},
   issue = {8},
   journal = {Review of Managerial Science},
   keywords = {Bibliometrics,Contributions,Literature reviews,Meta Analysis,Methods},
   month = {11},
   pages = {2577-2595},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Literature reviews as independent studies: guidelines for academic practice},
   volume = {16},
   url = {https://link.springer.com/article/10.1007/s11846-022-00588-8},
   year = {2022}
}
@article{Raschka2018,
   abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
   author = {Sebastian Raschka},
   month = {11},
   title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
   url = {https://arxiv.org/abs/1811.12808v3},
   year = {2018}
}
@article{Wainer2022,
   abstract = {This paper proposes a Bayesian model to compare multiple algorithms on multiple data sets, on any metric. The model is based on the Bradley-Terry model, that counts the number of times one algorithm performs better than another on different data sets. Because of its Bayesian foundations, the Bayesian Bradley Terry model (BBT) has different characteristics than frequentist approaches to comparing multiple algorithms on multiple data sets, such as Demsar (2006) tests on mean rank, and Benavoli et al. (2016) multiple pairwise Wilcoxon tests with p-adjustment procedures. In particular, a Bayesian approach allows for more nuanced statements regarding the algorithms beyond claiming that the difference is or it is not statistically significant. Bayesian approaches also allow to define when two algorithms are equivalent for practical purposes, or the region of practical equivalence (ROPE). Different than a Bayesian signed rank comparison procedure proposed by Benavoli et al. (2017), our approach can define a ROPE for any metric, since it is based on probability statements, and not on differences of that metric. This paper also proposes a local ROPE concept, that evaluates whether a positive difference between a mean measure across some cross validation to the mean of some other algorithms is should be really seen as the first algorithm being better than the second, based on effect sizes. This local ROPE proposal is independent of a Bayesian use, and can be used in frequentist approaches based on ranks. A R package and a Python program that implements the BBT is available.},
   author = {Jacques Wainer},
   keywords = {Bayesian,Bradley-Terry model,Comparison of classifiers,Comparison of regressors,Multiple algorithms,Multiple data sets},
   month = {8},
   title = {A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets},
   url = {https://arxiv.org/abs/2208.04935v2},
   year = {2022}
}
@article{Olson2017,
   abstract = {The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.},
   author = {Randal S. Olson and William La Cava and Patryk Orzechowski and Ryan J. Urbanowicz and Jason H. Moore},
   doi = {10.1186/s13040-017-0154-4},
   issn = {17560381},
   issue = {1},
   journal = {BioData Mining},
   keywords = {Benchmarking,Data repository,Machine learning,Model evaluation},
   month = {3},
   publisher = {BioMed Central Ltd.},
   title = {PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison},
   volume = {10},
   url = {https://arxiv.org/abs/1703.00512v1},
   year = {2017}
}
@article{Lavesson2007,
   abstract = {We analyse 18 evaluation methods for learning algorithms and classifiers, and show how to categorise these methods with the help of an evaluation method taxonomy based on several criteria. We also define a formal framework that make it possible to describe all methods using the same terminology, and apply it in a review of the state-of-the-art in learning algorithm and classifier evaluation. The framework enables comparison and deeper understanding of evaluation methods from different fields of research. Moreover, we argue that the framework and taxonomy support the process of finding candidate evaluation methods for a particular problem. © 2007 Inderscience Enterprises Ltd.},
   author = {Niklas Lavesson and Paul Davidsson},
   doi = {10.1504/IJIIDS.2007.013284},
   issn = {17515866},
   issue = {1},
   journal = {International Journal of Intelligent Information and Database Systems},
   keywords = {classification,evaluation,supervised learning},
   pages = {37-52},
   title = {Evaluating learning algorithms and classifiers},
   volume = {1},
   year = {2007}
}
@techReport{Lacoste2012,
   abstract = {We propose a new method for comparing learning algorithms on multiple tasks which is based on a novel non-parametric test that we call the Poisson binomial test. The key aspect of this work is that we provide a formal definition for what is meant to have an algorithm that is better than another. Also, we are able to take into account the dependencies induced when evaluating classifiers on the same test set. Finally we make optimal use (in the Bayesian sense) of all the testing data we have. We demonstrate empirically that our approach is more reliable than the sign test and the Wilcoxon signed rank test, the current state of the art for algorithm comparisons.},
   author = {Alexandre Lacoste and François Laviolette and Mario Marchand},
   title = {Bayesian Comparison of Machine Learning Algorithms on Single and Multiple Datasets},
   url = {http://code.google.com/p/mleval/.},
   year = {2012}
}
@article{Egele2024,
   abstract = {Machine learning is now used in many applications thanks to its ability to predict, generate, or discover patterns from large quantities of data. However, the process of collecting and transforming data for practical use is intricate. Even in today's digital era, where substantial data is generated daily, it is uncommon for it to be readily usable; most often, it necessitates meticulous manual data preparation. The haste in developing new models can frequently result in various shortcomings, potentially posing risks when deployed in real-world scenarios (eg social discrimination, critical failures), leading to the failure or substantial escalation of costs in AI-based projects. This chapter provides a comprehensive overview of established methodological tools, enriched by our practical experience, in the development of datasets for machine learning. Initially, we develop the tasks involved in dataset development and offer insights into their effective management (including requirements, design, implementation, evaluation, distribution, and maintenance). Then, we provide more details about the implementation process which includes data collection, transformation, and quality evaluation. Finally, we address practical considerations regarding dataset distribution and maintenance.},
   author = {Romain Egele and Julio C S Jacques Junior and Jan N van Rijn and University Paris-Saclay and Usa Xavier Baró and Albert Clapés and Prasanna Balaprakash and Sergio Escalera and Thomas Moeslund and Jun Wan},
   keywords = {Data-centric machine learning,data preparation,dataset development},
   month = {4},
   title = {AI Competitions and Benchmarks: Dataset Development},
   url = {https://arxiv.org/abs/2404.09703v1},
   year = {2024}
}
@article{Hor2010,
   abstract = {In this paper, we analyse two well-known objective image quality metrics, the peak-signal-to-noise ratio (PSNR) as well as the structural similarity index measure (SSIM), and we derive a simple mathematical relationship between them which works for various kinds of image degradations such as Gaussian blur, additive Gaussian white noise, jpeg and jpeg2000 compression. A series of tests realized on images extracted from the Kodak database gives a better understanding of the similarity and difference between the SSIM and the PSNR. © 2010 IEEE.},
   author = {Alain Horé and Djemel Ziou},
   doi = {10.1109/ICPR.2010.579},
   isbn = {9780769541099},
   issn = {10514651},
   journal = {Proceedings - International Conference on Pattern Recognition},
   keywords = {Image quality metrics,PSNR,SSIM},
   pages = {2366-2369},
   title = {Image quality metrics: PSNR vs. SSIM},
   year = {2010}
}
@article{Wang2004,
   abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
   author = {Zhou Wang and Alan Conrad Bovik and Hamid Rahim Sheikh and Eero P. Simoncelli},
   doi = {10.1109/TIP.2003.819861},
   issn = {10577149},
   issue = {4},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
   month = {4},
   pages = {600-612},
   pmid = {15376593},
   title = {Image quality assessment: From error visibility to structural similarity},
   volume = {13},
   year = {2004}
}
@techReport{,
   author = {C E Shannon},
   journal = {The Bell System Technical Journal},
   pages = {623-656},
   title = {A Mathematical Theory of Communication},
   volume = {27}
}
@article{Ranjan2016,
   abstract = {We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.},
   author = {Rajeev Ranjan and Vishal M. Patel and Rama Chellappa},
   doi = {10.1109/TPAMI.2017.2781233},
   issn = {19393539},
   issue = {1},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Face detection,deep convolutional neural networks,gender recognition,head pose estimation,landmarks localization,multi-task learning},
   month = {3},
   pages = {121-135},
   pmid = {29990235},
   publisher = {IEEE Computer Society},
   title = {HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition},
   volume = {41},
   url = {https://arxiv.org/abs/1603.01249v3},
   year = {2016}
}
@article{Willmott2005,
   abstract = {The relative abilities of 2, dimensioned statistics - the root-mean-square error (RMSE) and the mean absolute error (MAE) - to describe average model-performance error are examined. The RMSE is of special interest because it is widely reported in the climatic and environmental literature; nevertheless, it is an inappropriate and misinterpreted measure of average error. RMSE is inappropriate because it is a function of 3 characteristics of a set of errors, rather than of one (the average error). RMSE varies with the variability within the distribution of error magnitudes and with the square root of the number of errors (n1/2), as well as with the average-error magnitude (MAE). Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous. Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE. © Inter-Research 2005.},
   author = {Cort J. Willmott and Kenji Matsuura},
   doi = {10.3354/CR030079},
   issn = {0936577X},
   issue = {1},
   journal = {Climate Research},
   keywords = {Mean absolute error,Model-performance measures,Root-mean-square error},
   month = {12},
   pages = {79-82},
   publisher = {Inter-Research},
   title = {Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},
   volume = {30},
   year = {2005}
}
@article{Langford2005,
   abstract = {We discuss basic prediction theory and its impact on classification success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The first is that common practices for reporting results in classification should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms.},
   author = {John Langford and Jl@hunch Net},
   issn = {1533-7928},
   issue = {10},
   journal = {Journal of Machine Learning Research},
   keywords = {classification,quantitative bounds,sample complexity bounds},
   pages = {273-306},
   title = {Tutorial on Practical Prediction Theory for Classification},
   volume = {6},
   url = {http://jmlr.org/papers/v6/langford05a.html},
   year = {2005}
}
@techReport{Verdinelli2024,
   abstract = {Because of the widespread use of black box prediction methods such as random forests and neural nets, there is renewed interest in developing methods for quantifying variable importance as part of the broader goal of interpretable prediction. A popular approach is to define a variable importance parameter-known as LOCO (Leave Out COvariates)-based on dropping covariates from a regression model. This is essentially a nonparametric version of R 2. This parameter is very general and can be estimated nonparametrically, but it can be hard to interpret because it is affected by correlation between covariates. We propose a method for mitigating the effect of correlation by defining a modified version of LOCO. This new parameter is difficult to estimate nonparametrically, but we show how to estimate it using semiparametric models.},
   author = {Isabella Verdinelli and Larry Wasserman},
   journal = {Journal of Machine Learning Research},
   keywords = {Correlation,Nonparametric Estimators,Prediction,Variable Importance},
   pages = {1-27},
   title = {Decorrelated Variable Importance},
   volume = {25},
   url = {http://jmlr.org/papers/v25/22-0801.html.},
   year = {2024}
}
@article{Lpez2023,
   abstract = {Various studies have focused on estimating the price of cryptocurrencies using time series models and static variables. This study focuses on Bitcoin price prediction, using a model that combines multiple linear regression and neural networks. This approach makes it possible to identify the factors that influence Bitcoin volatility and, through a dynamic selection of variables, to constantly detect the most relevant set of characteristics for prediction. Likewise, the amount of data is optimized to improve precision and avoid overuse of historical information. The combination of these techniques captures underlying patterns and trends, increasing the reliability of predictions, with an accuracy of 88%. However, it is crucial to consider the need for continuous evaluations to adapt to changing market conditions. This approach provides a more accurate tool for making informed decisions in a highly volatile market.},
   author = {Manuel Humberto Díaz López and Andrea King-Domínguez and Luis Améstica-Rivas},
   doi = {10.31095/podium.2023.44.8},
   issn = {1390-5473},
   issue = {44},
   journal = {PODIUM},
   keywords = {Bitcoin,modelo de regresión lineal múltiple,precio del petróleo,redes neuronales,volatility index,índice de sentimiento del mercado,índice de volatilidad,índices bursátiles},
   month = {12},
   pages = {119-132},
   publisher = {Universidad Espiritu Santo},
   title = {Estimación de Precios de Bitcoin mediante Regresión Lineal Múltiple y Redes Neuronales},
   url = {https://revistas.uees.edu.ec/index.php/Podium/article/view/1108 https://revistas.uees.edu.ec/index.php/Podium/article/view/1108},
   year = {2023}
}
@article{Balcilar2017,
   abstract = {Prior studies on the price formation in the Bitcoin market consider the role of Bitcoin transactions at the conditional mean of the returns distribution. This study employs in contrast a non-parametric causality-in-quantiles test to analyse the causal relation between trading volume and Bitcoin returns and volatility, over the whole of their respective conditional distributions. The nonparametric characteristics of our test control for misspecification due to nonlinearity and structural breaks, two features of our data that cover 19th December 2011 to 25th April 2016. The causality-in-quantiles test reveals that volume can predict returns – except in Bitcoin bear and bull market regimes. This result highlights the importance of modelling nonlinearity and accounting for the tail behaviour when analysing causal relationships between Bitcoin returns and trading volume. We show, however, that volume cannot help predict the volatility of Bitcoin returns at any point of the conditional distribution.},
   author = {Mehmet Balcilar and Elie Bouri and Rangan Gupta and David Roubaud},
   doi = {10.1016/J.ECONMOD.2017.03.019},
   issn = {0264-9993},
   journal = {Economic Modelling},
   keywords = {Bitcoin,Nonparametric quantile causality,Returns,Volatility,Volume},
   month = {8},
   pages = {74-81},
   publisher = {North-Holland},
   title = {Can volume predict Bitcoin returns and volatility? A quantiles-based approach},
   volume = {64},
   year = {2017}
}
@article{Dey2021,
   abstract = {Investors in the stock market have always been in search of novel and unique techniques so that they can successfully predict stock price movement and make a big profit. However, investors continue to look for improved and new techniques to beat the market instead of old and traditional ones. Therefore, researchers are continuously working to build novel techniques to supply the demand of investors. Different types of recurrent neural networks (RNN) are used in time series analyses, especially in stock price prediction. However, since not all stocks’ prices follow the same trend, a single model cannot be used to predict the movement of all types of stock’s price. Therefore, in this research we conducted a comparative analysis of three commonly used RNNs—simple RNN, Long Short Term Memory (LSTM), and Gated Recurrent Unit (GRU)—and analyzed their efficiency for stocks having different stock trends and various price ranges and for different time frequencies. We considered three companies’ datasets from 30 June 2000 to 21 July 2020. The stocks follow different trends of price movements, with price ranges of $30, $50, and $290 during this period. We also analyzed the performance for one-day, three-day, and five-day time intervals. We compared the performance of RNN, LSTM, and GRU in terms of R2 value, MAE, MAPE, and RMSE metrics. The results show that simple RNN is outperformed by LSTM and GRU because RNN is susceptible to vanishing gradient problems, while the other two models are not. Moreover, GRU produces lesser errors comparing to LSTM. It is also evident from the results that as the time intervals get smaller, the models produce lower errors and higher reliability.},
   author = {Polash Dey and Emam Hossain and Md Ishtiaque Hossain and Mohammed Armanuzzaman Chowdhury and Md Shariful Alam and Mohammad Shahadat Hossain and Karl Andersson},
   doi = {10.3390/A14080251},
   issn = {1999-4893},
   issue = {8},
   journal = {Algorithms 2021, Vol. 14, Page 251},
   keywords = {recurrent neural networks,stock price forecasting,stock price movement,stock price prediction,time series analysis},
   month = {8},
   pages = {251},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Comparative Analysis of Recurrent Neural Networks in Stock Price Prediction for Different Frequency Domains},
   volume = {14},
   url = {https://www.mdpi.com/1999-4893/14/8/251/htm https://www.mdpi.com/1999-4893/14/8/251},
   year = {2021}
}
@article{Ivanovski2023,
   abstract = {The impact of cryptocurrency on other assets has become a subject of intense research, given the rise of digital currency over the last decade. However, unlike traditional assets, cryptocurrency has been subject to extreme movements in price and volatility. As a result, it has become important for investors and risk managers to model and forecast volatility and correlation between digital currency and other assets. This paper utilises a multivariate generalised autoregressive score (GAS) model to study the time-varying dependence between stock prices (S&P500, NASDAQ, Dow Jones Industrial) and cryptocurrencies (Bitcoin and Ethereum). The results show that the GAS framework outperforms the traditional DCC-GARCH model, capturing the volatility persistence and non-linearity between stock and cryptocurrency. Regarding the correlations, while we identify a time-varying relationship, the strength of this relationship is in the low-to-moderate range. In addition, our forecasting exercise shows that the GAS specification has superior forecasting ability beyond certain horizon days compared to the DCC-GARCH model.},
   author = {Kris Ivanovski and Abebe Hailemariam},
   doi = {10.1016/J.IREF.2023.03.008},
   issn = {1059-0560},
   journal = {International Review of Economics \& Finance},
   keywords = {Bitcoin,Correlation,Cryptocurrency,Ethereum,Forecasting,Stock price},
   month = {7},
   pages = {97-111},
   publisher = {JAI},
   title = {Forecasting the stock-cryptocurrency relationship: Evidence from a dynamic GAS model},
   volume = {86},
   year = {2023}
}
@techReport{,
   author = {José Ignacio Martínez Riquelme and Luis Amestica and Antonino Alej Parisi Aparisi and César Gurrola Ríos},
   title = {Alternative predictor of the price of financial assets. The case of silver and gold},
   url = {https://www.researchgate.net/publication/359605876}
}
@article{Mai2015,
   abstract = {As the world's first completely decentralized digital payment system, Bitcoin represents a revolutionary phenomenon in financial markets. This study examines predictive relationships between social media and bitcoin returns by considering the relative effect of different social media platforms (Internet forum vs. microblogging) and the dynamics of the resulting relationships using vector autoregressive and vector error correction models. The results suggest that more bullish forums have a positive, statistically significant relationship with future bitcoin returns at a daily level. Internet forum predictive metrics outperform microblogging ones at a daily frequency, but their effects are opposite at an hourly frequency. The user-generated content contributed by the vocal minority and the silent majority exhibit distinct relationships with bitcoin performance, in terms of both transaction volumes and returns. The implications of these results for research and practice are notable with regard to the transformative power of social media analytics in networked business environments subject to the dynamics of bitcoin performance.},
   author = {Feng Mai and Qing Bai and Zhe Shan and Xin (Shane) Wang and Roger H.L. Chiang},
   doi = {10.2139/SSRN.2545957},
   journal = {SSRN Electronic Journal},
   month = {1},
   publisher = {Elsevier BV},
   title = {From Bitcoin to Big Coin: The Impacts of Social Media on Bitcoin Performance},
   year = {2015}
}
@article{Sun2020,
   abstract = {Forecasting cryptocurrency prices is crucial for investors. In this paper, we adopt a novel Gradient Boosting Decision Tree (GBDT) algorithm, Light Gradient Boosting Machine (LightGBM), to forecast the price trend (falling, or not falling) of cryptocurrency market. In order to utilize market information, we combine the daily data of 42 kinds of primary cryptocurrencies with key economic indicators. Results show that the robustness of the LightGBM model is better than the other methods, and the comprehensive strength of the cryptocurrencies impacts the forecasting performance. This can effectively guide investors in constructing an appropriate cryptocurrency portfolio and mitigate risks.},
   author = {Xiaolei Sun and Mingxi Liu and Zeqian Sima},
   doi = {10.1016/J.FRL.2018.12.032},
   issn = {15446123},
   journal = {Finance Research Letters},
   keywords = {Cryptocurrency,Forecasting performance,LightGBM,Trend forecasting},
   month = {1},
   publisher = {Elsevier Ltd},
   title = {A novel cryptocurrency price trend forecasting model based on LightGBM},
   volume = {32},
   year = {2020}
}
@article{Yao2022,
   abstract = {This study aims to establish the model of the cryptocurrency price trend based on a financial theory using the Long Short-Term Memory (LSTM) networks model with multiple combinations between the window length and the predicting horizons. The Random Walk model is also applied with different parameter settings. The object of this study is the cryptocurrency and medical issues, primarily the Bitcoin and Ethereum and the COVID-19. Quantitative analysis is adopted as the method of this dissertation. The research tool is Python programming language, and the TensorFlow package is employed to model and analyze research topics. The results of this study show the limitations of the LSTM and Random Walk model for price prediction while demonstrating the different characteristics of both models with different parameter settings, providing a balance between the model's accuracy and the model's practicality.},
   author = {Yifan Yao and Xinxin Li and Qing Li},
   doi = {10.1155/2022/4383245},
   issn = {16875273},
   journal = {Computational Intelligence and Neuroscience},
   pmid = {36052038},
   publisher = {Hindawi Limited},
   title = {A Comparison on LSTM Deep Learning Method and Random Walk Model Used on Financial and Medical Applications: An Example in COVID-19 Development Prediction},
   volume = {2022},
   year = {2022}
}
@techReport{,
   abstract = {Resumen Casas, Marta y Cepeda, Edilberto. "Modelos ARCH, GARCH y EGARCH: aplicaciones a series financieras", Cuadernos de Economía, v. XXVII, n. 48, Bogotá, 2008, páginas 287-319. En este artículo se incluye una descripción de los modelos ARCH, GARCH y EGARCH, y de los procesos de estimación de sus parámetros usando máxima verosimilitud. Se propone un modelo alternativo para el análisis de series financieras y se estu-dian las series de precios y de retornos de las acciones de Gillette. La selección de modelos usando los criterios AIC y BIC permite concluir que, de los modelos considerados el GARCH(1,2) es el que mejor explica el comportamiento de los precios de las acciones y el EGARCH(2,1) es el que mejor explica la serie de los retornos. Edilberto Cepeda es Doctor y Magíster en Matemáticas; actualmente se desempeña como profesor asociado del Este artículo fue recibido el 2 de octubre de 2007 y su publicación aprobada el 6 de febrero de 2008.},
   author = {Marta Casas Monsegny and Edilberto Cepeda Cuervo},
   title = {Modelos ARCH, GARCH Y EGARCH: Aplicaciones a series financieras}
}
@article{Goutte2023,
   abstract = {A large number of modern practices in financial forecasting rely on technical analysis, which involves several heuristics techniques of price charts visual pattern recognition as well as other technical indicators. In this study, we aim to investigate the potential use of those technical information (candlestick information as well as technical indicators) as inputs for machine learning models, especially the state-of-the-art deep learning algorithms, to generate trading signals. To properly address this problem, empirical research is conducted which applies several machine learning methods to 5 years of Bitcoin hourly data from 2017 to 2022. From the result of our study, we confirm the potential of trading strategies using machine learning approaches. We also find that among several machine learning models, deep learning models, specifically the recurrent neural networks, tend to outperform the others in time-series prediction.},
   author = {Stéphane Goutte and Hoang Viet Le and Fei Liu and Hans Jörg von Mettenheim},
   doi = {10.1016/J.FRL.2023.103809},
   issn = {1544-6123},
   journal = {Finance Research Letters},
   keywords = {Bitcoin,Convolutional neural networks,Deep learning,Machine learning,Recurrent neural network,Technical analysis},
   month = {6},
   pages = {103809},
   publisher = {Elsevier},
   title = {Deep learning and technical analysis in cryptocurrency market},
   volume = {54},
   year = {2023}
}
@misc{,
   title = {IPC MEXICO (^MXX) Stock Historical Prices \& Data - Yahoo Finance},
   url = {https://finance.yahoo.com/quote/%5EMXX/history/}
}
@misc{MartnAbadi2015,
   author = {Martín~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Mané and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Viégas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
   note = {Software available from tensorflow.org},
   title = { TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
   url = {https://www.tensorflow.org/},
   year = {2015}
}
@misc{Chollet2015,
   author = {Francois Chollet and others},
   publisher = {GitHub},
   title = {Keras},
   url = {https://github.com/fchollet/keras},
   year = {2015}
}
@article{Coronel-Brizio2010,
   abstract = {Maximum likelihood estimation and a test of fit based on the Anderson-Darling statistic are presented for the case of the power-law distribution when the parameters are estimated from a left-censored sample. Expressions for the maximum likelihood estimators and tables of asymptotic percentage points for the A2 statistic are given. The technique is illustrated for data from the Dow Jones Industrial Average index, an example of high theoretical and practical importance in Econophysics, Finance, Physics, Biology and, in general, in other related sciences such as Complexity Sciences. © 2010 Elsevier B.V. All rights reserved.},
   author = {H. F. Coronel-Brizio and A. R. Hernández-Montoya},
   doi = {10.1016/J.PHYSA.2010.03.041},
   issn = {0378-4371},
   issue = {17},
   journal = {Physica A: Statistical Mechanics and its Applications},
   keywords = {Anderson-Darling,Asymptotic distributions,Complexity,EDF-based tests,Goodness of fit,Maximum likelihood estimation,Power-law distribution,Type II censoring},
   month = {9},
   pages = {3508-3515},
   publisher = {North-Holland},
   title = {The Anderson–Darling test of fit for the power-law distribution from left-censored samples},
   volume = {389},
   year = {2010}
}
@book{VanRossum1995,
   author = {Guido Van Rossum and Fred L Drake Jr},
   publisher = {Centrum voor Wiskunde en Informatica Amsterdam},
   title = {Python reference manual},
   year = {1995}
}
@techReport{Lorenzo2014,
   author = {Genoveva Lorenzo and Sergio Fco Juárez Cerrillo and Héctor Fco. Coronel Brizio},
   institution = {XXVIII Foro Internacional de Estadística},
   journal = {XXVII Foro Nacional de Estadística},
   title = {Valor en Riesgo del índice de Precios y Cotizaciones 1991-2011},
   year = {2014}
}
@techReport{Lorenzo2015,
   author = {Genoveva Lorenzo and Sergio Fco Juárez Cerrillo and J Martín and Cadena Barajas},
   institution = {XXIX Foro Internacional de Estad´ ıstica},
   title = {Modelación de los Retornos Negativos del IPC con las Distribuciones Pareto y Pareto Generalizada, 1991-2014},
   year = {2015}
}
@techReport{Chernysh2021,
   author = {Anton Chernysh},
   institution = {Universidad de Alicante},
   title = {Predicción de criptomonedas con técnicas de Deep Learning},
   year = {2021}
}
@article{Shaffer1995,
   abstract = {Reviews the literature on multiple testing, emphasizing conceptual issues and general approaches. Organizing concepts are discussed, including primary hypotheses, closure, hierarchical sets, minimal hypotheses, families, Type 1 error control, power, p-values, adjusted p-values, and closed test procedures. Two method types are discussed in detail: (1) methods based on ordered p-values and (2) comparisons among normally distributed means. Also discussed are tests vs confidence intervals, directional vs nondirectional inference, and robustness. The major challenge in multiple hypothesis testing is to devise methods that incorporate some kind of overall control of Type 1 error while retaining reasonable power for tests of the individual hypothesis. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
   author = {Juliet Popper Shaffer},
   city = {US},
   doi = {10.1146/annurev.ps.46.020195.003021},
   issn = {1545-2085(Electronic),0066-4308(Print)},
   journal = {Annual Review of Psychology},
   keywords = {Hypothesis Testing},
   pages = {561-584},
   publisher = {Annual Reviews},
   title = {Multiple hypothesis testing.},
   volume = {46},
   year = {1995}
}
@inproceedings{Landa2024,
   author = {Genoveva Lorenzo Landa and Sergio Nava-Muñoz and Daniel A. Cervantes Cabrera},
   doi = {10.1109/CONISOFT63288.2024.00043},
   isbn = {979-8-3315-3211-6},
   booktitle = {2024 12th International Conference in Software Engineering Research and Innovation (CONISOFT)},
   month = {10},
   pages = {273-278},
   publisher = {IEEE},
   title = {Price and Quote Index Prediction with Deep Learning},
   url = {https://ieeexplore.ieee.org/document/10795526/},
   year = {2024}
}
@article{Mohammad2018,
   abstract = {We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.},
   author = {Saif M. Mohammad and Felipe Bravo-Marquez and Mohammad Salameh and Svetlana Kiritchenko},
   doi = {10.18653/V1/S18-1001},
   isbn = {9781948087209},
   journal = {NAACL HLT 2018 - International Workshop on Semantic Evaluation, SemEval 2018 - Proceedings of the 12th Workshop},
   pages = {1-17},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {SemEval-2018 Task 1: Affect in Tweets},
   url = {https://aclanthology.org/S18-1001/},
   year = {2018}
}
@article{Barbieri2016,
   abstract = {English. The SENTIment POLarity Classification Task 2016 (SENTIPOLC), is a rerun of the shared task on sentiment classification at the message level on Italian tweets proposed for the first time in 2014 for the Evalita evaluation campaign. It includes three subtasks: subjectivity classification , polarity classification, and irony detection. In 2016 SENTIPOLC has been again the most participated EVALITA task with a total of 57 submitted runs from 13 different teams. We present the datasets – which includes an enriched annotation scheme for dealing with the impact on polarity of a figurative use of language – the evaluation methodology, and discuss results and participating systems.},
   author = {Francesco Barbieri and Valerio Basile and Danilo Croce and Malvina Nissim and Nicole Novielli and Viviana Patti},
   keywords = {Natural language processing and web,Sentiment analysis,Social media analysis},
   month = {12},
   title = {Overview of the Evalita 2016 SENTIment POLarity Classification Task},
   url = {https://inria.hal.science/hal-01414731 https://inria.hal.science/hal-01414731/document},
   year = {2016}
}
@article{Kiranyaz2021,
   abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.},
   author = {Serkan Kiranyaz and Onur Avci and Osama Abdeljaber and Turker Ince and Moncef Gabbouj and Daniel J. Inman},
   doi = {10.1016/j.ymssp.2020.107398},
   issn = {10961216},
   journal = {Mechanical Systems and Signal Processing},
   keywords = {Arrhythmia detection and identification,Artificial Neural Networks,Condition monitoring,Convolutional neural networks,Deep learning,Fault detection,Machine learning,Structural damage detection,Structural health monitoring},
   month = {4},
   publisher = {Academic Press},
   title = {1D convolutional neural networks and applications: A survey},
   volume = {151},
   year = {2021}
}
@article{Wibawa2022,
   abstract = {CNN originates from image processing and is not commonly known as a forecasting technique in time-series analysis which depends on the quality of input data. One of the methods to improve the quality is by smoothing the data. This study introduces a novel hybrid exponential smoothing using CNN called Smoothed-CNN (S-CNN). The method of combining tactics outperforms the majority of individual solutions in forecasting. The S-CNN was compared with the original CNN method and other forecasting methods such as Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM). The dataset is a year time-series of daily website visitors. Since there are no special rules for using the number of hidden layers, the Lucas number was used. The results show that S-CNN is better than MLP and LSTM, with the best MSE of 0.012147693 using 76 hidden layers at 80%:20% data composition.},
   author = {Aji Prasetya Wibawa and Agung Bella Putra Utama and Hakkun Elmunsyah and Utomo Pujianto and Felix Andika Dwiyanto and Leonel Hernandez},
   doi = {10.1186/s40537-022-00599-y},
   issn = {21961115},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {CNN,Exponential smoothing,Optimum smoothing factor,Time-series},
   month = {12},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Time-series analysis with smoothed Convolutional Neural Network},
   volume = {9},
   year = {2022}
}
@techReport{,
   author = {López de Prado},
   title = {Praise for Advances in Financial Machine Learning}
}
@article{Mikolov2013,
   abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
   author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
   journal = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
   month = {1},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Efficient Estimation of Word Representations in Vector Space},
   url = {https://arxiv.org/pdf/1301.3781},
   year = {2013}
}
@article{Sutskever2014,
   abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
   author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
   issn = {10495258},
   issue = {January},
   journal = {Advances in Neural Information Processing Systems},
   month = {9},
   pages = {3104-3112},
   publisher = {Neural information processing systems foundation},
   title = {Sequence to Sequence Learning with Neural Networks},
   volume = {4},
   url = {https://arxiv.org/pdf/1409.3215},
   year = {2014}
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyung Hyun Cho and Yoshua Bengio},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   month = {9},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {https://arxiv.org/pdf/1409.0473},
   year = {2014}
}
@article{Elharrouss2025,
   abstract = {Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.},
   author = {Omar Elharrouss and Yasir Mahmood and Yassine Bechqito and Mohamed Adel Serhani and Elarbi Badidi and Jamal Riffi and Hamid Tairi},
   month = {4},
   title = {Loss Functions in Deep Learning: A Comprehensive Review},
   url = {http://arxiv.org/abs/2504.04242},
   year = {2025}
}
@article{Mojtahedi2025,
   abstract = {This paper presents a detailed review of existing and emerging deep learning algorithms for time series forecasting in geotechnics and geoscience applications. Deep learning has shown promising results in addressing complex prediction problems involving large datasets and multiple interacting variables without requiring extensive feature extraction. This study provides an in-depth description of prominent deep learning methods, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), generative adversarial network, deep belief network, reinforcement learning, attention and transformer algorithms as well as hybrid networks using a combination of these architectures. In addition, this paper summarizes the applications of these models in various fields, including mining and tunnelling, railway and road construction, seismology, slope stability, earth retaining and stabilizing structures, remote sensing, as well as scour and erosion. This review reveals that RNN-based models, particularly Long Short-Term Memory networks, are the most commonly used models for time series forecasting. The advantages of deep learning models over traditional machine learning, including their superior ability to handle complex patterns and process large-scale data more effectively, are discussed. Furthermore, in time series forecasting within the fields of geotechnics and geosciences, studies frequently reveal that deep learning methods tend to surpass traditional machine learning techniques in effectiveness.},
   author = {F. Fazel Mojtahedi and N. Yousefpour and S. H. Chow and M. Cassidy},
   doi = {10.1007/S11831-025-10244-5},
   issn = {1886-1784},
   journal = {Archives of Computational Methods in Engineering 2025},
   keywords = {Mathematical and Computational Engineering},
   month = {2},
   pages = {1-31},
   publisher = {Springer},
   title = {Deep Learning for Time Series Forecasting: Review and Applications in Geotechnics and Geosciences},
   url = {https://link.springer.com/article/10.1007/s11831-025-10244-5},
   year = {2025}
}
@article{Terven2023,
   abstract = {One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.},
   author = {Juan R Terven and Diana M Cordova-Esparza and Alfonzo Ramirez-Pedraza and Edgar A Chavez-Urbiola Cicata-Qro and Instituto Politecnico and Nacional Mexico},
   title = {LOSS FUNCTIONS AND METRICS IN DEEP LEARNING. A REVIEW UNDER REVIEW IN COMPUTER SCIENCE REVIEW},
   year = {2023}
}
@article{Benidis2022,
   abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the ...},
   author = {Konstantinos Benidis and Syama Sundar Rangapuram and Valentin Flunkert and Yuyang Wang and Danielle Maddix and Caner Turkmen and Jan Gasthaus and Michael Bohlke-Schneider and David Salinas and Lorenzo Stella and François Xavier Aubet and Laurent Callot and Tim Januschowski},
   doi = {10.1145/3533382},
   issn = {15577341},
   issue = {6},
   journal = {ACM Computing Surveys},
   keywords = {Time series,forecasting,neural networks},
   month = {12},
   publisher = {ACMPUB27New York, NY},
   title = {Deep Learning for Time Series Forecasting: Tutorial and Literature Survey},
   volume = {55},
   url = {/doi/pdf/10.1145/3533382?download=true},
   year = {2022}
}
